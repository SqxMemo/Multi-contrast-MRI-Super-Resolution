{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"n5d5n8bqc8SB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":132},"executionInfo":{"status":"ok","timestamp":1597583770753,"user_tz":-120,"elapsed":36268,"user":{"displayName":"Alexandre Bodinier","photoUrl":"","userId":"11185344835719138768"}},"outputId":"86329017-a00d-4d5f-a486-50297a7b2d5c"},"source":["# imports\n","import os\n","import sys\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time\n","from math import log10\n","import pandas as pd\n","from torchvision.transforms import ToTensor\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import time\n","\n","\n","from google.colab import drive\n","drive.mount('/content/Gdrive', force_remount=True)\n","dir = \"/content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/\"\n","data_dir = \"/content/Gdrive/My Drive/Colab Notebooks/PRE/Data/\"\n","#dir = \"/Users/Alex2/Desktop/Multi Modalities/\"\n","#data_dir = dir\n","sys.path.append(dir)\n","\n","# Own packages\n","from data import get_training_set, get_test_set\n","from edsr import EDSR\n","from ssim import ssim"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/Gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"db-dzMJoc-bf","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597583770761,"user_tz":-120,"elapsed":36266,"user":{"displayName":"Alexandre Bodinier","photoUrl":"","userId":"11185344835719138768"}}},"source":["# Training settings:\n","batchSize = 32\n","testBatchSize = 32\n","nEpochs = 100\n","lr = 0.0001\n","threads = 8\n","seed = 123\n","torch.manual_seed(seed)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","cold_start = False\n","early_upsampling = False\n","\n","# Model :\n","upscale_factor = 3\n","input_channels = [\"flair\"]\n","target_channels = ['flair']\n","patch_size = [240, 240]\n","n_resblocks = 32\n","n_patch_feats = 128"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"M4WgB4KHy0lq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597583770763,"user_tz":-120,"elapsed":36259,"user":{"displayName":"Alexandre Bodinier","photoUrl":"","userId":"11185344835719138768"}}},"source":["# Paths :\n","training_path = data_dir + \"Data/train/\"\n","test_path = data_dir + \"Data/test/\"\n","#training_path = data_dir + \"Data/smallTrain/\"\n","#test_path = data_dir + \"Data/smallTest/\"\n","\n","modelName = \"flair->flair 32blocs\""],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"guAtiQCty0l-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597583771415,"user_tz":-120,"elapsed":36907,"user":{"displayName":"Alexandre Bodinier","photoUrl":"","userId":"11185344835719138768"}}},"source":["training_folder = dir + \"Trainings/\" + modelName\n","model_weights = training_folder + \"/Pre_trained_model/\"\n","stats_path = training_folder + \"/Stats/\"\n","images_path = training_folder + \"/Training_images/\"\n","summary_path = training_folder + \"/Summary/\"\n","\n","paths = [training_folder, model_weights, stats_path, images_path, summary_path]\n","for path in paths:\n","    try:\n","        os.stat(path)\n","    except:\n","        os.makedirs(path) "],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"qlMG5JQDdc_h","colab_type":"code","tags":[],"colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1597583793556,"user_tz":-120,"elapsed":59014,"user":{"displayName":"Alexandre Bodinier","photoUrl":"","userId":"11185344835719138768"}},"outputId":"7b5ade62-2ca0-4672-9f19-3355724c96d2"},"source":["print('===> Loading datasets')\n","train_set = get_training_set(upscale_factor, input_channels, target_channels, training_path, patch_size=patch_size, early_upsampling=early_upsampling)\n","test_set = get_test_set(upscale_factor, input_channels, target_channels, test_path, patch_size=patch_size, early_upsampling=early_upsampling)\n","\n","training_data_loader = DataLoader(dataset=train_set, num_workers=threads, batch_size=batchSize, shuffle=True)\n","testing_data_loader = DataLoader(dataset=test_set, num_workers=threads, batch_size=testBatchSize, shuffle=False)\n","\n","print(\"{} training images / {} testing images\".format(len(train_set), len(test_set)))\n","print(\"===> dataset loaded !\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["===> Loading datasets\n","3880 training images / 960 testing images\n","===> dataset loaded !\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"tags":[],"id":"BFU4Jsguy0mY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597583803804,"user_tz":-120,"elapsed":69236,"user":{"displayName":"Alexandre Bodinier","photoUrl":"","userId":"11185344835719138768"}},"outputId":"318d9a74-508f-4db7-a20b-b102308929d6"},"source":["print('===> Building model')\n","#model = Net(upscale_factor=upscale_factor, input_channels=input_channels).to(device)\n","model = EDSR(upscale_factor, input_channels, target_channels, n_resblocks=n_resblocks, n_feats=n_patch_feats, res_scale=.1, bn=None).to(device)\n","print(model.parameters())\n","\n","criterion = nn.MSELoss()\n","optimizer = optim.AdamW(model.parameters(), lr=lr)\n","    \n","print(model)\n","print(\"==> Model built\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["===> Building model\n","<generator object Module.parameters at 0x7fc35a5ce678>\n","EDSR(\n","  (act): ReLU(inplace=True)\n","  (head): Sequential(\n","    (0): Conv2d(1, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  )\n","  (body): Sequential(\n","    (0): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (1): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (2): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (3): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (4): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (5): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (6): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (7): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (8): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (9): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (10): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (11): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (12): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (13): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (14): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (15): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (16): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (17): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (18): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (19): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (20): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (21): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (22): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (23): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (24): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (25): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (26): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (27): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (28): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (29): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (30): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (31): ResBlock(\n","      (body): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (32): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  )\n","  (tail): Sequential(\n","    (0): Upsampler(\n","      (0): Conv2d(128, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): PixelShuffle(upscale_factor=3)\n","    )\n","    (1): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  )\n",")\n","==> Model built\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RNSeESPoy0mg","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597583803805,"user_tz":-120,"elapsed":69233,"user":{"displayName":"Alexandre Bodinier","photoUrl":"","userId":"11185344835719138768"}}},"source":["def calc_psnr(x):\n","    return 10 * log10(1/x)\n","\n","def calc_ssim(pred, target):\n","    ssim_score  = ssim(pred, target, window_size=11, size_average=True).item()\n","    return ssim_score\n","\n","def show(epoch, pred, target, title=\"\"):\n","    psnr = 10 * log10(1/criterion(pred, target))\n","\n","    pred = pred.detach().numpy()\n","    norm = 255./(pred.max()-pred.min())\n","    pred *= norm\n","    pred.clip(0, 255)\n","\n","    target = target.detach().numpy()\n","    norm = 255./(target.max()-target.min())\n","    target *= norm\n","    target.clip(0, 255)\n","\n","\n","    residual = np.abs(pred - target)\n","    residual -= residual.min()\n","    norm = 255./(residual.max()-residual.min())\n","    residual *= norm\n","    residual.clip(0, 255)\n","    \n","    pred = Image.fromarray(pred, mode='F').convert('L')\n","    pred.save(images_path + \"{}_epoch_{:.4f}_{}dB_{}\".format(modelName, epoch, psnr, title+\"_prediction.jpg\"))\n","    target = Image.fromarray(np.uint8(target), mode='L')\n","    target.save(images_path + \"{}_epoch_{}_{}\".format(modelName, epoch, title+\"_target.jpg\"))\n","    residual = Image.fromarray(np.uint8(residual), mode='L')\n","    residual.save(images_path + \"{}_epoch_{}_{}\".format(modelName, epoch, title+\"_residual.jpg\"))\n","\n","    fig, axs = plt.subplots(1, 3)\n","    fig.set_size_inches((15,10))\n","    axs[0].imshow(target, cmap='gray')\n","    axs[0].set_title(\"Target \" + title)\n","    axs[2].imshow(residual)\n","    axs[2].set_title(\"Residual \")\n","    axs[1].imshow(pred, cmap='gray')\n","    axs[1].set_title(\"Prediction \" + title + ' psnr= {:.4f} dB'.format(psnr))\n","    fig.savefig(summary_path + \"{}_epoch_{}_{}\".format(modelName, epoch, title+\"_summary.jpg\"))\n","    plt.close()"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"_LecvjQiy0ml","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597583803807,"user_tz":-120,"elapsed":69230,"user":{"displayName":"Alexandre Bodinier","photoUrl":"","userId":"11185344835719138768"}}},"source":["def train(epoch):\n","    epoch_loss = 0\n","    epoch_loss_indiv = [0 for x in range(len(target_channels))]\n","    epoch_ssim_indiv = [0 for x in range(len(target_channels))]\n","    for iteration, batch in enumerate(training_data_loader, 1):\n","        inp, target, mask = batch[0].to(device), batch[1].to(device), batch[2].to(device).to(device=device, dtype=torch.float32)\n","\n","        optimizer.zero_grad()\n","        prediction = model(inp)\n","\n","        loss = 0\n","        for x in range(len(target_channels)):\n","            loss_x = criterion(prediction[:, x, :, :], target[:, x, :, :])\n","            ssim_x = calc_ssim(prediction[:, x, :, :].view(len(batch[0]), 1, 240, 240), target[:, x, :, :].view(len(batch[0]), 1, 240, 240))\n","            epoch_loss_indiv[x] += loss_x.item()\n","            epoch_ssim_indiv[x] += ssim_x\n","            loss += loss_x\n","\n","        epoch_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","        print(\"===> Epoch[{}]({}/{}): Loss: {:.4f}\".format(epoch, iteration, len(training_data_loader), loss.item()))\n","\n","    epochLoss = epoch_loss / len(training_data_loader)\n","\n","    for x in range(len(target_channels)):\n","            epoch_loss_indiv[x] /= len(training_data_loader)\n","            epoch_ssim_indiv[x] /= len(training_data_loader)\n","\n","    psnr_indiv = list(map(calc_psnr, epoch_loss_indiv))\n","    psnr = calc_psnr(epochLoss)\n","    print(\"psnr_indiv= \", psnr_indiv, \"global_psnr= \", psnr)\n","\n","    print(\"===> Epoch {} Complete: Avg. Loss: {:.4f}\".format(epoch, epochLoss))\n","    pred = prediction.cpu()[0]\n","    target = target.cpu()[0]\n","\n","    for p, t, c_name in zip(pred, target, target_channels):\n","        show(epoch, p, t, \"train_\"+c_name)\n","    return psnr, psnr_indiv, epoch_ssim_indiv\n","\n","\n","def test(epoch):\n","    avg_psnr = 0\n","    epoch_loss_indiv = [0 for x in range(len(target_channels))]\n","    epoch_ssim_indiv = [0 for x in range(len(target_channels))]\n","    with torch.no_grad():\n","        for c, batch in enumerate(testing_data_loader):\n","            inp, target, mask = batch[0].to(device), batch[1].to(device), batch[2].to(device).to(device=device, dtype=torch.float32)\n","\n","            prediction = model(inp)\n","            mse = criterion(prediction, target)\n","\n","            for x in range(len(target_channels)):\n","                loss_x = criterion(prediction[:, x, :, :], target[:, x, :, :])\n","                ssim_x = calc_ssim(prediction[:, x, :, :].view(len(batch[0]), 1, 240, 240), target[:, x, :, :].view(len(batch[0]), 1, 240, 240))\n","                epoch_loss_indiv[x] += loss_x.item()\n","                epoch_ssim_indiv[x] += ssim_x\n","\n","            psnr = 10 * log10(1/mse.item())\n","            avg_psnr += psnr\n","            \n","        print(\"===> Avg. PSNR: {:.4f} dB\".format(avg_psnr / len(testing_data_loader)))\n","        pred = prediction.cpu()[0]\n","        target = target.cpu()[0]\n","\n","        for p, t, c_name in zip(pred, target, target_channels):\n","            show(epoch, p, t, \" \"+c_name)\n","\n","        for x in range(len(target_channels)):\n","            epoch_loss_indiv[x] /= len(testing_data_loader)\n","            epoch_ssim_indiv[x] /= len(testing_data_loader)\n","            \n","\n","        psnr_indiv = list(map(calc_psnr, epoch_loss_indiv))\n","    return avg_psnr / len(testing_data_loader), psnr_indiv, epoch_ssim_indiv\n","\n","\n","def save_checkpoint(ckp, stats, save_path):\n","    model_out_path = save_path + modelName\n","    torch.save(ckp, model_out_path + \".pth\") #save ckp\n","    torch.save(ckp['state_dict'], model_out_path + \"model_epoch_{}.pth\".format(epoch)) #save model\n","    print(\"Checkpoint saved to {}\".format(model_out_path))\n","\n","def load_checkpoint(checkpoint_fpath, model, optimizer):\n","    checkpoint = torch.load(checkpoint_fpath)\n","    model.load_state_dict(checkpoint['state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    return model, optimizer, checkpoint['epoch'], checkpoint['stats'], checkpoint['stats']['elapsed_time'][-1]"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"csb-NoIyy0mt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597596584567,"user_tz":-120,"elapsed":12849969,"user":{"displayName":"Alexandre Bodinier","photoUrl":"","userId":"11185344835719138768"}},"outputId":"185712a2-a512-4b61-b785-0d55f5fec770"},"source":["print(\"==> Starts to train\")\n","try:\n","    if cold_start:\n","        raise Exception\n","    model, optimizer, start_epoch, stats, start_time = load_checkpoint(model_weights+modelName+\".pth\", model, optimizer)\n","    print(\"Training resumed at epoch \", start_epoch)\n","except:\n","    print(\"training from scratch\")\n","    start_epoch=0\n","\n","    stats = {\n","    'model': modelName,\n","    'lr': lr,\n","    'epoch': [],\n","    'elapsed_time': [],\n","    'train_PSNR': [],\n","    'test_PSNR': [],\n","    'train_PSNR_t1w': [],\n","    'test_PSNR_t1w': [],\n","    'train_ssim_t1w': [],\n","    'test_ssim_t1w': [],\n","    'train_PSNR_t1gd': [],\n","    'test_PSNR_t1gd': [],\n","    'train_ssim_t1gd': [],\n","    'test_ssim_t1gd': [],\n","    'train_PSNR_t2w': [],\n","    'test_PSNR_t2w': [],\n","    'train_ssim_t2w': [],\n","    'test_ssim_t2w': [],\n","    'train_PSNR_flair': [],\n","    'test_PSNR_flair': [],\n","    'train_ssim_flair': [],\n","    'test_ssim_flair': []\n","    }\n","    start_time = time.time()\n","\n","\n","\n","for epoch in range(start_epoch, nEpochs + 1):\n","\n","    test_psnr, test_psnr_indiv, test_ssim = test(epoch)\n","    train_psnr, train_psnr_indiv, train_ssim = train(epoch)\n","\n","    test_per_modality = {target_channels[x]: test_psnr_indiv[x] for x in range(len(target_channels))}\n","    train_per_modality = {target_channels[x]: train_psnr_indiv[x] for x in range(len(target_channels))}\n","    test_ssim_per_modality = {target_channels[x]: test_ssim[x] for x in range(len(target_channels))}\n","    train_ssim_per_modality = {target_channels[x]: train_ssim[x] for x in range(len(target_channels))}\n","    print(\"test_per_mod\", test_per_modality)\n","    print(\"train_per_mod\", train_per_modality)\n","\n","    stop_time = time.time()\n","    elapsed_time = np.round(stop_time - start_time, 2)\n","    stats['elapsed_time'].append(elapsed_time)\n","    stats['epoch'].append(epoch)\n","    stats['train_PSNR'].append(train_psnr)\n","    stats['test_PSNR'].append(test_psnr)\n","    stats['train_PSNR_t1w'].append(0)\n","    stats['test_PSNR_t1w'].append(0)\n","    stats['train_ssim_t1w'].append(0)\n","    stats['test_ssim_t1w'].append(0)\n","    stats['train_PSNR_t1gd'].append(0)\n","    stats['test_PSNR_t1gd'].append(0)\n","    stats['train_ssim_t1gd'].append(0)\n","    stats['test_ssim_t1gd'].append(0)\n","    stats['train_PSNR_t2w'].append(0)\n","    stats['test_PSNR_t2w'].append(0)\n","    stats['train_ssim_t2w'].append(0)\n","    stats['test_ssim_t2w'].append(0)\n","    stats['train_PSNR_flair'].append(0)\n","    stats['test_PSNR_flair'].append(0)\n","    stats['train_ssim_flair'].append(0)\n","    stats['test_ssim_flair'].append(0)\n","    for x in target_channels:\n","        stats['test_PSNR_' + x][-1] = test_per_modality[x]\n","        stats['train_PSNR_' + x][-1] = train_per_modality[x]\n","        stats['test_ssim_' + x][-1] = test_ssim_per_modality[x]\n","        stats['train_ssim_' + x][-1] = train_ssim_per_modality[x]\n","\n","\n","    ckp = {\n","        'epoch': epoch + 1,\n","        'state_dict': model.state_dict(),\n","        'optimizer': optimizer.state_dict(), \n","        'stats': stats\n","    }\n","    save_checkpoint(ckp, stats, model_weights)\n","\n","    df = pd.DataFrame(\n","        data=stats, \n","        columns=['model', 'lr', 'epoch', 'elapsed_time', 'train_PSNR','test_PSNR', 'train_PSNR_t1w', 'test_PSNR_t1w', 'train_ssim_t1w', 'test_ssim_t1w','train_PSNR_t1gd', 'test_PSNR_t1gd', 'train_ssim_t1gd', 'test_ssim_t1gd','train_PSNR_t2w', 'test_PSNR_t2w', 'train_ssim_t2w', 'test_ssim_t2w','train_PSNR_flair', 'test_PSNR_flair''train_ssim_flair', 'test_ssim_flair']\n","        )\n","    df.to_csv(stats_path + modelName + \"_stats.csv\")\n","\n","print(\"===> Display results\")\n","df = pd.DataFrame(\n","        data=stats, \n","        columns=['model', 'lr', 'epoch', 'elapsed_time', 'train_PSNR','test_PSNR', 'train_PSNR_t1w', 'test_PSNR_t1w', 'train_ssim_t1w', 'test_ssim_t1w','train_PSNR_t1gd', 'test_PSNR_t1gd', 'train_ssim_t1gd', 'test_ssim_t1gd','train_PSNR_t2w', 'test_PSNR_t2w', 'train_ssim_t2w', 'test_ssim_t2w','train_PSNR_flair', 'test_PSNR_flair''train_ssim_flair', 'test_ssim_flair']\n","        )\n","df.to_csv(dir + modelName + \"_stats.csv\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mLe flux de sortie a été tronqué et ne contient que les 5000 dernières lignes.\u001b[0m\n","===> Epoch[61](122/122): Loss: 0.0002\n","psnr_indiv=  [36.841480190801676] global_psnr=  36.841480190801676\n","===> Epoch 61 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.73788067323568}\n","train_per_mod {'flair': 36.841480190801676}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.1559 dB\n","===> Epoch[62](1/122): Loss: 0.0002\n","===> Epoch[62](2/122): Loss: 0.0002\n","===> Epoch[62](3/122): Loss: 0.0002\n","===> Epoch[62](4/122): Loss: 0.0002\n","===> Epoch[62](5/122): Loss: 0.0003\n","===> Epoch[62](6/122): Loss: 0.0002\n","===> Epoch[62](7/122): Loss: 0.0003\n","===> Epoch[62](8/122): Loss: 0.0002\n","===> Epoch[62](9/122): Loss: 0.0002\n","===> Epoch[62](10/122): Loss: 0.0003\n","===> Epoch[62](11/122): Loss: 0.0002\n","===> Epoch[62](12/122): Loss: 0.0002\n","===> Epoch[62](13/122): Loss: 0.0002\n","===> Epoch[62](14/122): Loss: 0.0002\n","===> Epoch[62](15/122): Loss: 0.0002\n","===> Epoch[62](16/122): Loss: 0.0002\n","===> Epoch[62](17/122): Loss: 0.0002\n","===> Epoch[62](18/122): Loss: 0.0003\n","===> Epoch[62](19/122): Loss: 0.0002\n","===> Epoch[62](20/122): Loss: 0.0002\n","===> Epoch[62](21/122): Loss: 0.0002\n","===> Epoch[62](22/122): Loss: 0.0002\n","===> Epoch[62](23/122): Loss: 0.0002\n","===> Epoch[62](24/122): Loss: 0.0002\n","===> Epoch[62](25/122): Loss: 0.0002\n","===> Epoch[62](26/122): Loss: 0.0002\n","===> Epoch[62](27/122): Loss: 0.0002\n","===> Epoch[62](28/122): Loss: 0.0002\n","===> Epoch[62](29/122): Loss: 0.0002\n","===> Epoch[62](30/122): Loss: 0.0002\n","===> Epoch[62](31/122): Loss: 0.0003\n","===> Epoch[62](32/122): Loss: 0.0003\n","===> Epoch[62](33/122): Loss: 0.0003\n","===> Epoch[62](34/122): Loss: 0.0003\n","===> Epoch[62](35/122): Loss: 0.0002\n","===> Epoch[62](36/122): Loss: 0.0003\n","===> Epoch[62](37/122): Loss: 0.0003\n","===> Epoch[62](38/122): Loss: 0.0003\n","===> Epoch[62](39/122): Loss: 0.0002\n","===> Epoch[62](40/122): Loss: 0.0002\n","===> Epoch[62](41/122): Loss: 0.0003\n","===> Epoch[62](42/122): Loss: 0.0002\n","===> Epoch[62](43/122): Loss: 0.0002\n","===> Epoch[62](44/122): Loss: 0.0002\n","===> Epoch[62](45/122): Loss: 0.0002\n","===> Epoch[62](46/122): Loss: 0.0002\n","===> Epoch[62](47/122): Loss: 0.0002\n","===> Epoch[62](48/122): Loss: 0.0002\n","===> Epoch[62](49/122): Loss: 0.0003\n","===> Epoch[62](50/122): Loss: 0.0002\n","===> Epoch[62](51/122): Loss: 0.0002\n","===> Epoch[62](52/122): Loss: 0.0002\n","===> Epoch[62](53/122): Loss: 0.0002\n","===> Epoch[62](54/122): Loss: 0.0002\n","===> Epoch[62](55/122): Loss: 0.0002\n","===> Epoch[62](56/122): Loss: 0.0002\n","===> Epoch[62](57/122): Loss: 0.0002\n","===> Epoch[62](58/122): Loss: 0.0002\n","===> Epoch[62](59/122): Loss: 0.0002\n","===> Epoch[62](60/122): Loss: 0.0002\n","===> Epoch[62](61/122): Loss: 0.0003\n","===> Epoch[62](62/122): Loss: 0.0002\n","===> Epoch[62](63/122): Loss: 0.0002\n","===> Epoch[62](64/122): Loss: 0.0002\n","===> Epoch[62](65/122): Loss: 0.0002\n","===> Epoch[62](66/122): Loss: 0.0002\n","===> Epoch[62](67/122): Loss: 0.0002\n","===> Epoch[62](68/122): Loss: 0.0002\n","===> Epoch[62](69/122): Loss: 0.0002\n","===> Epoch[62](70/122): Loss: 0.0002\n","===> Epoch[62](71/122): Loss: 0.0002\n","===> Epoch[62](72/122): Loss: 0.0002\n","===> Epoch[62](73/122): Loss: 0.0002\n","===> Epoch[62](74/122): Loss: 0.0003\n","===> Epoch[62](75/122): Loss: 0.0003\n","===> Epoch[62](76/122): Loss: 0.0002\n","===> Epoch[62](77/122): Loss: 0.0002\n","===> Epoch[62](78/122): Loss: 0.0002\n","===> Epoch[62](79/122): Loss: 0.0002\n","===> Epoch[62](80/122): Loss: 0.0002\n","===> Epoch[62](81/122): Loss: 0.0002\n","===> Epoch[62](82/122): Loss: 0.0002\n","===> Epoch[62](83/122): Loss: 0.0002\n","===> Epoch[62](84/122): Loss: 0.0002\n","===> Epoch[62](85/122): Loss: 0.0002\n","===> Epoch[62](86/122): Loss: 0.0002\n","===> Epoch[62](87/122): Loss: 0.0002\n","===> Epoch[62](88/122): Loss: 0.0002\n","===> Epoch[62](89/122): Loss: 0.0002\n","===> Epoch[62](90/122): Loss: 0.0003\n","===> Epoch[62](91/122): Loss: 0.0002\n","===> Epoch[62](92/122): Loss: 0.0002\n","===> Epoch[62](93/122): Loss: 0.0002\n","===> Epoch[62](94/122): Loss: 0.0002\n","===> Epoch[62](95/122): Loss: 0.0003\n","===> Epoch[62](96/122): Loss: 0.0002\n","===> Epoch[62](97/122): Loss: 0.0002\n","===> Epoch[62](98/122): Loss: 0.0002\n","===> Epoch[62](99/122): Loss: 0.0002\n","===> Epoch[62](100/122): Loss: 0.0001\n","===> Epoch[62](101/122): Loss: 0.0002\n","===> Epoch[62](102/122): Loss: 0.0002\n","===> Epoch[62](103/122): Loss: 0.0002\n","===> Epoch[62](104/122): Loss: 0.0002\n","===> Epoch[62](105/122): Loss: 0.0002\n","===> Epoch[62](106/122): Loss: 0.0002\n","===> Epoch[62](107/122): Loss: 0.0003\n","===> Epoch[62](108/122): Loss: 0.0002\n","===> Epoch[62](109/122): Loss: 0.0002\n","===> Epoch[62](110/122): Loss: 0.0002\n","===> Epoch[62](111/122): Loss: 0.0002\n","===> Epoch[62](112/122): Loss: 0.0002\n","===> Epoch[62](113/122): Loss: 0.0002\n","===> Epoch[62](114/122): Loss: 0.0002\n","===> Epoch[62](115/122): Loss: 0.0002\n","===> Epoch[62](116/122): Loss: 0.0002\n","===> Epoch[62](117/122): Loss: 0.0002\n","===> Epoch[62](118/122): Loss: 0.0002\n","===> Epoch[62](119/122): Loss: 0.0002\n","===> Epoch[62](120/122): Loss: 0.0002\n","===> Epoch[62](121/122): Loss: 0.0003\n","===> Epoch[62](122/122): Loss: 0.0002\n","psnr_indiv=  [36.68216013943929] global_psnr=  36.68216013943929\n","===> Epoch 62 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.68744610186776}\n","train_per_mod {'flair': 36.68216013943929}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.2219 dB\n","===> Epoch[63](1/122): Loss: 0.0002\n","===> Epoch[63](2/122): Loss: 0.0002\n","===> Epoch[63](3/122): Loss: 0.0002\n","===> Epoch[63](4/122): Loss: 0.0002\n","===> Epoch[63](5/122): Loss: 0.0002\n","===> Epoch[63](6/122): Loss: 0.0002\n","===> Epoch[63](7/122): Loss: 0.0002\n","===> Epoch[63](8/122): Loss: 0.0002\n","===> Epoch[63](9/122): Loss: 0.0002\n","===> Epoch[63](10/122): Loss: 0.0002\n","===> Epoch[63](11/122): Loss: 0.0002\n","===> Epoch[63](12/122): Loss: 0.0002\n","===> Epoch[63](13/122): Loss: 0.0002\n","===> Epoch[63](14/122): Loss: 0.0002\n","===> Epoch[63](15/122): Loss: 0.0002\n","===> Epoch[63](16/122): Loss: 0.0002\n","===> Epoch[63](17/122): Loss: 0.0003\n","===> Epoch[63](18/122): Loss: 0.0002\n","===> Epoch[63](19/122): Loss: 0.0002\n","===> Epoch[63](20/122): Loss: 0.0002\n","===> Epoch[63](21/122): Loss: 0.0002\n","===> Epoch[63](22/122): Loss: 0.0002\n","===> Epoch[63](23/122): Loss: 0.0002\n","===> Epoch[63](24/122): Loss: 0.0002\n","===> Epoch[63](25/122): Loss: 0.0002\n","===> Epoch[63](26/122): Loss: 0.0002\n","===> Epoch[63](27/122): Loss: 0.0002\n","===> Epoch[63](28/122): Loss: 0.0002\n","===> Epoch[63](29/122): Loss: 0.0003\n","===> Epoch[63](30/122): Loss: 0.0002\n","===> Epoch[63](31/122): Loss: 0.0002\n","===> Epoch[63](32/122): Loss: 0.0002\n","===> Epoch[63](33/122): Loss: 0.0002\n","===> Epoch[63](34/122): Loss: 0.0002\n","===> Epoch[63](35/122): Loss: 0.0002\n","===> Epoch[63](36/122): Loss: 0.0002\n","===> Epoch[63](37/122): Loss: 0.0002\n","===> Epoch[63](38/122): Loss: 0.0002\n","===> Epoch[63](39/122): Loss: 0.0002\n","===> Epoch[63](40/122): Loss: 0.0003\n","===> Epoch[63](41/122): Loss: 0.0002\n","===> Epoch[63](42/122): Loss: 0.0002\n","===> Epoch[63](43/122): Loss: 0.0002\n","===> Epoch[63](44/122): Loss: 0.0002\n","===> Epoch[63](45/122): Loss: 0.0002\n","===> Epoch[63](46/122): Loss: 0.0002\n","===> Epoch[63](47/122): Loss: 0.0002\n","===> Epoch[63](48/122): Loss: 0.0002\n","===> Epoch[63](49/122): Loss: 0.0002\n","===> Epoch[63](50/122): Loss: 0.0003\n","===> Epoch[63](51/122): Loss: 0.0002\n","===> Epoch[63](52/122): Loss: 0.0002\n","===> Epoch[63](53/122): Loss: 0.0002\n","===> Epoch[63](54/122): Loss: 0.0002\n","===> Epoch[63](55/122): Loss: 0.0002\n","===> Epoch[63](56/122): Loss: 0.0001\n","===> Epoch[63](57/122): Loss: 0.0002\n","===> Epoch[63](58/122): Loss: 0.0002\n","===> Epoch[63](59/122): Loss: 0.0002\n","===> Epoch[63](60/122): Loss: 0.0002\n","===> Epoch[63](61/122): Loss: 0.0002\n","===> Epoch[63](62/122): Loss: 0.0002\n","===> Epoch[63](63/122): Loss: 0.0002\n","===> Epoch[63](64/122): Loss: 0.0001\n","===> Epoch[63](65/122): Loss: 0.0002\n","===> Epoch[63](66/122): Loss: 0.0002\n","===> Epoch[63](67/122): Loss: 0.0002\n","===> Epoch[63](68/122): Loss: 0.0002\n","===> Epoch[63](69/122): Loss: 0.0003\n","===> Epoch[63](70/122): Loss: 0.0002\n","===> Epoch[63](71/122): Loss: 0.0002\n","===> Epoch[63](72/122): Loss: 0.0002\n","===> Epoch[63](73/122): Loss: 0.0002\n","===> Epoch[63](74/122): Loss: 0.0002\n","===> Epoch[63](75/122): Loss: 0.0002\n","===> Epoch[63](76/122): Loss: 0.0002\n","===> Epoch[63](77/122): Loss: 0.0002\n","===> Epoch[63](78/122): Loss: 0.0002\n","===> Epoch[63](79/122): Loss: 0.0001\n","===> Epoch[63](80/122): Loss: 0.0002\n","===> Epoch[63](81/122): Loss: 0.0002\n","===> Epoch[63](82/122): Loss: 0.0003\n","===> Epoch[63](83/122): Loss: 0.0002\n","===> Epoch[63](84/122): Loss: 0.0002\n","===> Epoch[63](85/122): Loss: 0.0003\n","===> Epoch[63](86/122): Loss: 0.0002\n","===> Epoch[63](87/122): Loss: 0.0002\n","===> Epoch[63](88/122): Loss: 0.0002\n","===> Epoch[63](89/122): Loss: 0.0002\n","===> Epoch[63](90/122): Loss: 0.0002\n","===> Epoch[63](91/122): Loss: 0.0002\n","===> Epoch[63](92/122): Loss: 0.0002\n","===> Epoch[63](93/122): Loss: 0.0002\n","===> Epoch[63](94/122): Loss: 0.0002\n","===> Epoch[63](95/122): Loss: 0.0002\n","===> Epoch[63](96/122): Loss: 0.0002\n","===> Epoch[63](97/122): Loss: 0.0002\n","===> Epoch[63](98/122): Loss: 0.0002\n","===> Epoch[63](99/122): Loss: 0.0002\n","===> Epoch[63](100/122): Loss: 0.0002\n","===> Epoch[63](101/122): Loss: 0.0002\n","===> Epoch[63](102/122): Loss: 0.0002\n","===> Epoch[63](103/122): Loss: 0.0002\n","===> Epoch[63](104/122): Loss: 0.0002\n","===> Epoch[63](105/122): Loss: 0.0002\n","===> Epoch[63](106/122): Loss: 0.0002\n","===> Epoch[63](107/122): Loss: 0.0002\n","===> Epoch[63](108/122): Loss: 0.0003\n","===> Epoch[63](109/122): Loss: 0.0002\n","===> Epoch[63](110/122): Loss: 0.0002\n","===> Epoch[63](111/122): Loss: 0.0002\n","===> Epoch[63](112/122): Loss: 0.0002\n","===> Epoch[63](113/122): Loss: 0.0002\n","===> Epoch[63](114/122): Loss: 0.0002\n","===> Epoch[63](115/122): Loss: 0.0002\n","===> Epoch[63](116/122): Loss: 0.0002\n","===> Epoch[63](117/122): Loss: 0.0002\n","===> Epoch[63](118/122): Loss: 0.0002\n","===> Epoch[63](119/122): Loss: 0.0002\n","===> Epoch[63](120/122): Loss: 0.0002\n","===> Epoch[63](121/122): Loss: 0.0002\n","===> Epoch[63](122/122): Loss: 0.0003\n","psnr_indiv=  [36.86095506667351] global_psnr=  36.86095506667351\n","===> Epoch 63 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.748282099081734}\n","train_per_mod {'flair': 36.86095506667351}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.2262 dB\n","===> Epoch[64](1/122): Loss: 0.0002\n","===> Epoch[64](2/122): Loss: 0.0002\n","===> Epoch[64](3/122): Loss: 0.0002\n","===> Epoch[64](4/122): Loss: 0.0002\n","===> Epoch[64](5/122): Loss: 0.0002\n","===> Epoch[64](6/122): Loss: 0.0002\n","===> Epoch[64](7/122): Loss: 0.0002\n","===> Epoch[64](8/122): Loss: 0.0002\n","===> Epoch[64](9/122): Loss: 0.0002\n","===> Epoch[64](10/122): Loss: 0.0002\n","===> Epoch[64](11/122): Loss: 0.0002\n","===> Epoch[64](12/122): Loss: 0.0002\n","===> Epoch[64](13/122): Loss: 0.0003\n","===> Epoch[64](14/122): Loss: 0.0002\n","===> Epoch[64](15/122): Loss: 0.0002\n","===> Epoch[64](16/122): Loss: 0.0002\n","===> Epoch[64](17/122): Loss: 0.0002\n","===> Epoch[64](18/122): Loss: 0.0002\n","===> Epoch[64](19/122): Loss: 0.0002\n","===> Epoch[64](20/122): Loss: 0.0002\n","===> Epoch[64](21/122): Loss: 0.0002\n","===> Epoch[64](22/122): Loss: 0.0002\n","===> Epoch[64](23/122): Loss: 0.0002\n","===> Epoch[64](24/122): Loss: 0.0002\n","===> Epoch[64](25/122): Loss: 0.0002\n","===> Epoch[64](26/122): Loss: 0.0002\n","===> Epoch[64](27/122): Loss: 0.0002\n","===> Epoch[64](28/122): Loss: 0.0002\n","===> Epoch[64](29/122): Loss: 0.0002\n","===> Epoch[64](30/122): Loss: 0.0002\n","===> Epoch[64](31/122): Loss: 0.0002\n","===> Epoch[64](32/122): Loss: 0.0002\n","===> Epoch[64](33/122): Loss: 0.0002\n","===> Epoch[64](34/122): Loss: 0.0002\n","===> Epoch[64](35/122): Loss: 0.0002\n","===> Epoch[64](36/122): Loss: 0.0002\n","===> Epoch[64](37/122): Loss: 0.0002\n","===> Epoch[64](38/122): Loss: 0.0002\n","===> Epoch[64](39/122): Loss: 0.0002\n","===> Epoch[64](40/122): Loss: 0.0003\n","===> Epoch[64](41/122): Loss: 0.0002\n","===> Epoch[64](42/122): Loss: 0.0002\n","===> Epoch[64](43/122): Loss: 0.0002\n","===> Epoch[64](44/122): Loss: 0.0002\n","===> Epoch[64](45/122): Loss: 0.0003\n","===> Epoch[64](46/122): Loss: 0.0002\n","===> Epoch[64](47/122): Loss: 0.0002\n","===> Epoch[64](48/122): Loss: 0.0002\n","===> Epoch[64](49/122): Loss: 0.0002\n","===> Epoch[64](50/122): Loss: 0.0003\n","===> Epoch[64](51/122): Loss: 0.0002\n","===> Epoch[64](52/122): Loss: 0.0002\n","===> Epoch[64](53/122): Loss: 0.0002\n","===> Epoch[64](54/122): Loss: 0.0002\n","===> Epoch[64](55/122): Loss: 0.0002\n","===> Epoch[64](56/122): Loss: 0.0002\n","===> Epoch[64](57/122): Loss: 0.0002\n","===> Epoch[64](58/122): Loss: 0.0002\n","===> Epoch[64](59/122): Loss: 0.0002\n","===> Epoch[64](60/122): Loss: 0.0002\n","===> Epoch[64](61/122): Loss: 0.0002\n","===> Epoch[64](62/122): Loss: 0.0002\n","===> Epoch[64](63/122): Loss: 0.0002\n","===> Epoch[64](64/122): Loss: 0.0002\n","===> Epoch[64](65/122): Loss: 0.0002\n","===> Epoch[64](66/122): Loss: 0.0002\n","===> Epoch[64](67/122): Loss: 0.0002\n","===> Epoch[64](68/122): Loss: 0.0002\n","===> Epoch[64](69/122): Loss: 0.0002\n","===> Epoch[64](70/122): Loss: 0.0002\n","===> Epoch[64](71/122): Loss: 0.0002\n","===> Epoch[64](72/122): Loss: 0.0002\n","===> Epoch[64](73/122): Loss: 0.0002\n","===> Epoch[64](74/122): Loss: 0.0002\n","===> Epoch[64](75/122): Loss: 0.0003\n","===> Epoch[64](76/122): Loss: 0.0002\n","===> Epoch[64](77/122): Loss: 0.0003\n","===> Epoch[64](78/122): Loss: 0.0002\n","===> Epoch[64](79/122): Loss: 0.0002\n","===> Epoch[64](80/122): Loss: 0.0002\n","===> Epoch[64](81/122): Loss: 0.0002\n","===> Epoch[64](82/122): Loss: 0.0002\n","===> Epoch[64](83/122): Loss: 0.0002\n","===> Epoch[64](84/122): Loss: 0.0002\n","===> Epoch[64](85/122): Loss: 0.0002\n","===> Epoch[64](86/122): Loss: 0.0002\n","===> Epoch[64](87/122): Loss: 0.0002\n","===> Epoch[64](88/122): Loss: 0.0003\n","===> Epoch[64](89/122): Loss: 0.0002\n","===> Epoch[64](90/122): Loss: 0.0002\n","===> Epoch[64](91/122): Loss: 0.0002\n","===> Epoch[64](92/122): Loss: 0.0002\n","===> Epoch[64](93/122): Loss: 0.0002\n","===> Epoch[64](94/122): Loss: 0.0002\n","===> Epoch[64](95/122): Loss: 0.0002\n","===> Epoch[64](96/122): Loss: 0.0002\n","===> Epoch[64](97/122): Loss: 0.0002\n","===> Epoch[64](98/122): Loss: 0.0002\n","===> Epoch[64](99/122): Loss: 0.0002\n","===> Epoch[64](100/122): Loss: 0.0002\n","===> Epoch[64](101/122): Loss: 0.0002\n","===> Epoch[64](102/122): Loss: 0.0002\n","===> Epoch[64](103/122): Loss: 0.0002\n","===> Epoch[64](104/122): Loss: 0.0002\n","===> Epoch[64](105/122): Loss: 0.0002\n","===> Epoch[64](106/122): Loss: 0.0002\n","===> Epoch[64](107/122): Loss: 0.0002\n","===> Epoch[64](108/122): Loss: 0.0002\n","===> Epoch[64](109/122): Loss: 0.0002\n","===> Epoch[64](110/122): Loss: 0.0002\n","===> Epoch[64](111/122): Loss: 0.0002\n","===> Epoch[64](112/122): Loss: 0.0002\n","===> Epoch[64](113/122): Loss: 0.0002\n","===> Epoch[64](114/122): Loss: 0.0002\n","===> Epoch[64](115/122): Loss: 0.0002\n","===> Epoch[64](116/122): Loss: 0.0002\n","===> Epoch[64](117/122): Loss: 0.0002\n","===> Epoch[64](118/122): Loss: 0.0002\n","===> Epoch[64](119/122): Loss: 0.0002\n","===> Epoch[64](120/122): Loss: 0.0002\n","===> Epoch[64](121/122): Loss: 0.0002\n","===> Epoch[64](122/122): Loss: 0.0002\n","psnr_indiv=  [36.880778368082] global_psnr=  36.880778368082\n","===> Epoch 64 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.74992037879981}\n","train_per_mod {'flair': 36.880778368082}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.2204 dB\n","===> Epoch[65](1/122): Loss: 0.0002\n","===> Epoch[65](2/122): Loss: 0.0002\n","===> Epoch[65](3/122): Loss: 0.0002\n","===> Epoch[65](4/122): Loss: 0.0002\n","===> Epoch[65](5/122): Loss: 0.0002\n","===> Epoch[65](6/122): Loss: 0.0002\n","===> Epoch[65](7/122): Loss: 0.0002\n","===> Epoch[65](8/122): Loss: 0.0002\n","===> Epoch[65](9/122): Loss: 0.0002\n","===> Epoch[65](10/122): Loss: 0.0003\n","===> Epoch[65](11/122): Loss: 0.0002\n","===> Epoch[65](12/122): Loss: 0.0002\n","===> Epoch[65](13/122): Loss: 0.0002\n","===> Epoch[65](14/122): Loss: 0.0002\n","===> Epoch[65](15/122): Loss: 0.0002\n","===> Epoch[65](16/122): Loss: 0.0002\n","===> Epoch[65](17/122): Loss: 0.0002\n","===> Epoch[65](18/122): Loss: 0.0002\n","===> Epoch[65](19/122): Loss: 0.0002\n","===> Epoch[65](20/122): Loss: 0.0002\n","===> Epoch[65](21/122): Loss: 0.0002\n","===> Epoch[65](22/122): Loss: 0.0002\n","===> Epoch[65](23/122): Loss: 0.0002\n","===> Epoch[65](24/122): Loss: 0.0002\n","===> Epoch[65](25/122): Loss: 0.0002\n","===> Epoch[65](26/122): Loss: 0.0002\n","===> Epoch[65](27/122): Loss: 0.0003\n","===> Epoch[65](28/122): Loss: 0.0002\n","===> Epoch[65](29/122): Loss: 0.0002\n","===> Epoch[65](30/122): Loss: 0.0002\n","===> Epoch[65](31/122): Loss: 0.0002\n","===> Epoch[65](32/122): Loss: 0.0002\n","===> Epoch[65](33/122): Loss: 0.0002\n","===> Epoch[65](34/122): Loss: 0.0002\n","===> Epoch[65](35/122): Loss: 0.0002\n","===> Epoch[65](36/122): Loss: 0.0002\n","===> Epoch[65](37/122): Loss: 0.0002\n","===> Epoch[65](38/122): Loss: 0.0002\n","===> Epoch[65](39/122): Loss: 0.0002\n","===> Epoch[65](40/122): Loss: 0.0002\n","===> Epoch[65](41/122): Loss: 0.0002\n","===> Epoch[65](42/122): Loss: 0.0002\n","===> Epoch[65](43/122): Loss: 0.0002\n","===> Epoch[65](44/122): Loss: 0.0002\n","===> Epoch[65](45/122): Loss: 0.0002\n","===> Epoch[65](46/122): Loss: 0.0002\n","===> Epoch[65](47/122): Loss: 0.0002\n","===> Epoch[65](48/122): Loss: 0.0002\n","===> Epoch[65](49/122): Loss: 0.0002\n","===> Epoch[65](50/122): Loss: 0.0002\n","===> Epoch[65](51/122): Loss: 0.0002\n","===> Epoch[65](52/122): Loss: 0.0002\n","===> Epoch[65](53/122): Loss: 0.0002\n","===> Epoch[65](54/122): Loss: 0.0002\n","===> Epoch[65](55/122): Loss: 0.0002\n","===> Epoch[65](56/122): Loss: 0.0002\n","===> Epoch[65](57/122): Loss: 0.0002\n","===> Epoch[65](58/122): Loss: 0.0002\n","===> Epoch[65](59/122): Loss: 0.0001\n","===> Epoch[65](60/122): Loss: 0.0002\n","===> Epoch[65](61/122): Loss: 0.0002\n","===> Epoch[65](62/122): Loss: 0.0002\n","===> Epoch[65](63/122): Loss: 0.0002\n","===> Epoch[65](64/122): Loss: 0.0002\n","===> Epoch[65](65/122): Loss: 0.0002\n","===> Epoch[65](66/122): Loss: 0.0002\n","===> Epoch[65](67/122): Loss: 0.0002\n","===> Epoch[65](68/122): Loss: 0.0002\n","===> Epoch[65](69/122): Loss: 0.0002\n","===> Epoch[65](70/122): Loss: 0.0002\n","===> Epoch[65](71/122): Loss: 0.0002\n","===> Epoch[65](72/122): Loss: 0.0002\n","===> Epoch[65](73/122): Loss: 0.0002\n","===> Epoch[65](74/122): Loss: 0.0002\n","===> Epoch[65](75/122): Loss: 0.0002\n","===> Epoch[65](76/122): Loss: 0.0002\n","===> Epoch[65](77/122): Loss: 0.0002\n","===> Epoch[65](78/122): Loss: 0.0002\n","===> Epoch[65](79/122): Loss: 0.0002\n","===> Epoch[65](80/122): Loss: 0.0002\n","===> Epoch[65](81/122): Loss: 0.0002\n","===> Epoch[65](82/122): Loss: 0.0002\n","===> Epoch[65](83/122): Loss: 0.0002\n","===> Epoch[65](84/122): Loss: 0.0002\n","===> Epoch[65](85/122): Loss: 0.0002\n","===> Epoch[65](86/122): Loss: 0.0002\n","===> Epoch[65](87/122): Loss: 0.0002\n","===> Epoch[65](88/122): Loss: 0.0002\n","===> Epoch[65](89/122): Loss: 0.0002\n","===> Epoch[65](90/122): Loss: 0.0002\n","===> Epoch[65](91/122): Loss: 0.0002\n","===> Epoch[65](92/122): Loss: 0.0002\n","===> Epoch[65](93/122): Loss: 0.0002\n","===> Epoch[65](94/122): Loss: 0.0002\n","===> Epoch[65](95/122): Loss: 0.0002\n","===> Epoch[65](96/122): Loss: 0.0002\n","===> Epoch[65](97/122): Loss: 0.0002\n","===> Epoch[65](98/122): Loss: 0.0002\n","===> Epoch[65](99/122): Loss: 0.0003\n","===> Epoch[65](100/122): Loss: 0.0002\n","===> Epoch[65](101/122): Loss: 0.0002\n","===> Epoch[65](102/122): Loss: 0.0002\n","===> Epoch[65](103/122): Loss: 0.0002\n","===> Epoch[65](104/122): Loss: 0.0002\n","===> Epoch[65](105/122): Loss: 0.0002\n","===> Epoch[65](106/122): Loss: 0.0003\n","===> Epoch[65](107/122): Loss: 0.0002\n","===> Epoch[65](108/122): Loss: 0.0002\n","===> Epoch[65](109/122): Loss: 0.0002\n","===> Epoch[65](110/122): Loss: 0.0002\n","===> Epoch[65](111/122): Loss: 0.0002\n","===> Epoch[65](112/122): Loss: 0.0002\n","===> Epoch[65](113/122): Loss: 0.0002\n","===> Epoch[65](114/122): Loss: 0.0002\n","===> Epoch[65](115/122): Loss: 0.0002\n","===> Epoch[65](116/122): Loss: 0.0003\n","===> Epoch[65](117/122): Loss: 0.0002\n","===> Epoch[65](118/122): Loss: 0.0002\n","===> Epoch[65](119/122): Loss: 0.0002\n","===> Epoch[65](120/122): Loss: 0.0002\n","===> Epoch[65](121/122): Loss: 0.0002\n","===> Epoch[65](122/122): Loss: 0.0002\n","psnr_indiv=  [36.888787876587585] global_psnr=  36.888787876587585\n","===> Epoch 65 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.75150894702192}\n","train_per_mod {'flair': 36.888787876587585}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.2082 dB\n","===> Epoch[66](1/122): Loss: 0.0002\n","===> Epoch[66](2/122): Loss: 0.0002\n","===> Epoch[66](3/122): Loss: 0.0002\n","===> Epoch[66](4/122): Loss: 0.0002\n","===> Epoch[66](5/122): Loss: 0.0002\n","===> Epoch[66](6/122): Loss: 0.0003\n","===> Epoch[66](7/122): Loss: 0.0002\n","===> Epoch[66](8/122): Loss: 0.0002\n","===> Epoch[66](9/122): Loss: 0.0002\n","===> Epoch[66](10/122): Loss: 0.0002\n","===> Epoch[66](11/122): Loss: 0.0002\n","===> Epoch[66](12/122): Loss: 0.0002\n","===> Epoch[66](13/122): Loss: 0.0001\n","===> Epoch[66](14/122): Loss: 0.0002\n","===> Epoch[66](15/122): Loss: 0.0002\n","===> Epoch[66](16/122): Loss: 0.0002\n","===> Epoch[66](17/122): Loss: 0.0002\n","===> Epoch[66](18/122): Loss: 0.0002\n","===> Epoch[66](19/122): Loss: 0.0002\n","===> Epoch[66](20/122): Loss: 0.0002\n","===> Epoch[66](21/122): Loss: 0.0002\n","===> Epoch[66](22/122): Loss: 0.0002\n","===> Epoch[66](23/122): Loss: 0.0002\n","===> Epoch[66](24/122): Loss: 0.0002\n","===> Epoch[66](25/122): Loss: 0.0002\n","===> Epoch[66](26/122): Loss: 0.0002\n","===> Epoch[66](27/122): Loss: 0.0002\n","===> Epoch[66](28/122): Loss: 0.0002\n","===> Epoch[66](29/122): Loss: 0.0002\n","===> Epoch[66](30/122): Loss: 0.0002\n","===> Epoch[66](31/122): Loss: 0.0002\n","===> Epoch[66](32/122): Loss: 0.0002\n","===> Epoch[66](33/122): Loss: 0.0002\n","===> Epoch[66](34/122): Loss: 0.0002\n","===> Epoch[66](35/122): Loss: 0.0002\n","===> Epoch[66](36/122): Loss: 0.0002\n","===> Epoch[66](37/122): Loss: 0.0002\n","===> Epoch[66](38/122): Loss: 0.0002\n","===> Epoch[66](39/122): Loss: 0.0002\n","===> Epoch[66](40/122): Loss: 0.0002\n","===> Epoch[66](41/122): Loss: 0.0002\n","===> Epoch[66](42/122): Loss: 0.0002\n","===> Epoch[66](43/122): Loss: 0.0002\n","===> Epoch[66](44/122): Loss: 0.0002\n","===> Epoch[66](45/122): Loss: 0.0002\n","===> Epoch[66](46/122): Loss: 0.0002\n","===> Epoch[66](47/122): Loss: 0.0002\n","===> Epoch[66](48/122): Loss: 0.0002\n","===> Epoch[66](49/122): Loss: 0.0002\n","===> Epoch[66](50/122): Loss: 0.0003\n","===> Epoch[66](51/122): Loss: 0.0002\n","===> Epoch[66](52/122): Loss: 0.0002\n","===> Epoch[66](53/122): Loss: 0.0002\n","===> Epoch[66](54/122): Loss: 0.0002\n","===> Epoch[66](55/122): Loss: 0.0002\n","===> Epoch[66](56/122): Loss: 0.0002\n","===> Epoch[66](57/122): Loss: 0.0002\n","===> Epoch[66](58/122): Loss: 0.0002\n","===> Epoch[66](59/122): Loss: 0.0002\n","===> Epoch[66](60/122): Loss: 0.0002\n","===> Epoch[66](61/122): Loss: 0.0002\n","===> Epoch[66](62/122): Loss: 0.0002\n","===> Epoch[66](63/122): Loss: 0.0003\n","===> Epoch[66](64/122): Loss: 0.0002\n","===> Epoch[66](65/122): Loss: 0.0002\n","===> Epoch[66](66/122): Loss: 0.0002\n","===> Epoch[66](67/122): Loss: 0.0002\n","===> Epoch[66](68/122): Loss: 0.0003\n","===> Epoch[66](69/122): Loss: 0.0002\n","===> Epoch[66](70/122): Loss: 0.0002\n","===> Epoch[66](71/122): Loss: 0.0002\n","===> Epoch[66](72/122): Loss: 0.0002\n","===> Epoch[66](73/122): Loss: 0.0002\n","===> Epoch[66](74/122): Loss: 0.0002\n","===> Epoch[66](75/122): Loss: 0.0003\n","===> Epoch[66](76/122): Loss: 0.0002\n","===> Epoch[66](77/122): Loss: 0.0002\n","===> Epoch[66](78/122): Loss: 0.0002\n","===> Epoch[66](79/122): Loss: 0.0002\n","===> Epoch[66](80/122): Loss: 0.0002\n","===> Epoch[66](81/122): Loss: 0.0002\n","===> Epoch[66](82/122): Loss: 0.0002\n","===> Epoch[66](83/122): Loss: 0.0002\n","===> Epoch[66](84/122): Loss: 0.0002\n","===> Epoch[66](85/122): Loss: 0.0002\n","===> Epoch[66](86/122): Loss: 0.0002\n","===> Epoch[66](87/122): Loss: 0.0002\n","===> Epoch[66](88/122): Loss: 0.0002\n","===> Epoch[66](89/122): Loss: 0.0002\n","===> Epoch[66](90/122): Loss: 0.0002\n","===> Epoch[66](91/122): Loss: 0.0002\n","===> Epoch[66](92/122): Loss: 0.0002\n","===> Epoch[66](93/122): Loss: 0.0002\n","===> Epoch[66](94/122): Loss: 0.0002\n","===> Epoch[66](95/122): Loss: 0.0002\n","===> Epoch[66](96/122): Loss: 0.0002\n","===> Epoch[66](97/122): Loss: 0.0002\n","===> Epoch[66](98/122): Loss: 0.0002\n","===> Epoch[66](99/122): Loss: 0.0002\n","===> Epoch[66](100/122): Loss: 0.0002\n","===> Epoch[66](101/122): Loss: 0.0002\n","===> Epoch[66](102/122): Loss: 0.0002\n","===> Epoch[66](103/122): Loss: 0.0002\n","===> Epoch[66](104/122): Loss: 0.0002\n","===> Epoch[66](105/122): Loss: 0.0002\n","===> Epoch[66](106/122): Loss: 0.0002\n","===> Epoch[66](107/122): Loss: 0.0002\n","===> Epoch[66](108/122): Loss: 0.0002\n","===> Epoch[66](109/122): Loss: 0.0002\n","===> Epoch[66](110/122): Loss: 0.0002\n","===> Epoch[66](111/122): Loss: 0.0002\n","===> Epoch[66](112/122): Loss: 0.0002\n","===> Epoch[66](113/122): Loss: 0.0002\n","===> Epoch[66](114/122): Loss: 0.0002\n","===> Epoch[66](115/122): Loss: 0.0002\n","===> Epoch[66](116/122): Loss: 0.0002\n","===> Epoch[66](117/122): Loss: 0.0002\n","===> Epoch[66](118/122): Loss: 0.0002\n","===> Epoch[66](119/122): Loss: 0.0002\n","===> Epoch[66](120/122): Loss: 0.0002\n","===> Epoch[66](121/122): Loss: 0.0002\n","===> Epoch[66](122/122): Loss: 0.0003\n","psnr_indiv=  [36.867882678413736] global_psnr=  36.867882678413736\n","===> Epoch 66 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.73458313568702}\n","train_per_mod {'flair': 36.867882678413736}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.2551 dB\n","===> Epoch[67](1/122): Loss: 0.0002\n","===> Epoch[67](2/122): Loss: 0.0002\n","===> Epoch[67](3/122): Loss: 0.0002\n","===> Epoch[67](4/122): Loss: 0.0002\n","===> Epoch[67](5/122): Loss: 0.0002\n","===> Epoch[67](6/122): Loss: 0.0002\n","===> Epoch[67](7/122): Loss: 0.0002\n","===> Epoch[67](8/122): Loss: 0.0002\n","===> Epoch[67](9/122): Loss: 0.0003\n","===> Epoch[67](10/122): Loss: 0.0002\n","===> Epoch[67](11/122): Loss: 0.0002\n","===> Epoch[67](12/122): Loss: 0.0002\n","===> Epoch[67](13/122): Loss: 0.0002\n","===> Epoch[67](14/122): Loss: 0.0002\n","===> Epoch[67](15/122): Loss: 0.0002\n","===> Epoch[67](16/122): Loss: 0.0002\n","===> Epoch[67](17/122): Loss: 0.0002\n","===> Epoch[67](18/122): Loss: 0.0002\n","===> Epoch[67](19/122): Loss: 0.0002\n","===> Epoch[67](20/122): Loss: 0.0002\n","===> Epoch[67](21/122): Loss: 0.0002\n","===> Epoch[67](22/122): Loss: 0.0002\n","===> Epoch[67](23/122): Loss: 0.0003\n","===> Epoch[67](24/122): Loss: 0.0002\n","===> Epoch[67](25/122): Loss: 0.0002\n","===> Epoch[67](26/122): Loss: 0.0002\n","===> Epoch[67](27/122): Loss: 0.0002\n","===> Epoch[67](28/122): Loss: 0.0002\n","===> Epoch[67](29/122): Loss: 0.0002\n","===> Epoch[67](30/122): Loss: 0.0002\n","===> Epoch[67](31/122): Loss: 0.0002\n","===> Epoch[67](32/122): Loss: 0.0002\n","===> Epoch[67](33/122): Loss: 0.0002\n","===> Epoch[67](34/122): Loss: 0.0002\n","===> Epoch[67](35/122): Loss: 0.0002\n","===> Epoch[67](36/122): Loss: 0.0001\n","===> Epoch[67](37/122): Loss: 0.0002\n","===> Epoch[67](38/122): Loss: 0.0002\n","===> Epoch[67](39/122): Loss: 0.0002\n","===> Epoch[67](40/122): Loss: 0.0002\n","===> Epoch[67](41/122): Loss: 0.0002\n","===> Epoch[67](42/122): Loss: 0.0002\n","===> Epoch[67](43/122): Loss: 0.0002\n","===> Epoch[67](44/122): Loss: 0.0002\n","===> Epoch[67](45/122): Loss: 0.0002\n","===> Epoch[67](46/122): Loss: 0.0002\n","===> Epoch[67](47/122): Loss: 0.0002\n","===> Epoch[67](48/122): Loss: 0.0002\n","===> Epoch[67](49/122): Loss: 0.0002\n","===> Epoch[67](50/122): Loss: 0.0002\n","===> Epoch[67](51/122): Loss: 0.0002\n","===> Epoch[67](52/122): Loss: 0.0002\n","===> Epoch[67](53/122): Loss: 0.0002\n","===> Epoch[67](54/122): Loss: 0.0002\n","===> Epoch[67](55/122): Loss: 0.0002\n","===> Epoch[67](56/122): Loss: 0.0002\n","===> Epoch[67](57/122): Loss: 0.0002\n","===> Epoch[67](58/122): Loss: 0.0002\n","===> Epoch[67](59/122): Loss: 0.0002\n","===> Epoch[67](60/122): Loss: 0.0002\n","===> Epoch[67](61/122): Loss: 0.0003\n","===> Epoch[67](62/122): Loss: 0.0002\n","===> Epoch[67](63/122): Loss: 0.0002\n","===> Epoch[67](64/122): Loss: 0.0002\n","===> Epoch[67](65/122): Loss: 0.0003\n","===> Epoch[67](66/122): Loss: 0.0002\n","===> Epoch[67](67/122): Loss: 0.0002\n","===> Epoch[67](68/122): Loss: 0.0002\n","===> Epoch[67](69/122): Loss: 0.0002\n","===> Epoch[67](70/122): Loss: 0.0002\n","===> Epoch[67](71/122): Loss: 0.0002\n","===> Epoch[67](72/122): Loss: 0.0002\n","===> Epoch[67](73/122): Loss: 0.0002\n","===> Epoch[67](74/122): Loss: 0.0002\n","===> Epoch[67](75/122): Loss: 0.0003\n","===> Epoch[67](76/122): Loss: 0.0002\n","===> Epoch[67](77/122): Loss: 0.0002\n","===> Epoch[67](78/122): Loss: 0.0002\n","===> Epoch[67](79/122): Loss: 0.0002\n","===> Epoch[67](80/122): Loss: 0.0002\n","===> Epoch[67](81/122): Loss: 0.0002\n","===> Epoch[67](82/122): Loss: 0.0002\n","===> Epoch[67](83/122): Loss: 0.0002\n","===> Epoch[67](84/122): Loss: 0.0002\n","===> Epoch[67](85/122): Loss: 0.0003\n","===> Epoch[67](86/122): Loss: 0.0002\n","===> Epoch[67](87/122): Loss: 0.0002\n","===> Epoch[67](88/122): Loss: 0.0002\n","===> Epoch[67](89/122): Loss: 0.0002\n","===> Epoch[67](90/122): Loss: 0.0002\n","===> Epoch[67](91/122): Loss: 0.0002\n","===> Epoch[67](92/122): Loss: 0.0002\n","===> Epoch[67](93/122): Loss: 0.0002\n","===> Epoch[67](94/122): Loss: 0.0002\n","===> Epoch[67](95/122): Loss: 0.0002\n","===> Epoch[67](96/122): Loss: 0.0002\n","===> Epoch[67](97/122): Loss: 0.0002\n","===> Epoch[67](98/122): Loss: 0.0002\n","===> Epoch[67](99/122): Loss: 0.0002\n","===> Epoch[67](100/122): Loss: 0.0002\n","===> Epoch[67](101/122): Loss: 0.0002\n","===> Epoch[67](102/122): Loss: 0.0002\n","===> Epoch[67](103/122): Loss: 0.0002\n","===> Epoch[67](104/122): Loss: 0.0002\n","===> Epoch[67](105/122): Loss: 0.0002\n","===> Epoch[67](106/122): Loss: 0.0002\n","===> Epoch[67](107/122): Loss: 0.0002\n","===> Epoch[67](108/122): Loss: 0.0002\n","===> Epoch[67](109/122): Loss: 0.0002\n","===> Epoch[67](110/122): Loss: 0.0002\n","===> Epoch[67](111/122): Loss: 0.0002\n","===> Epoch[67](112/122): Loss: 0.0002\n","===> Epoch[67](113/122): Loss: 0.0001\n","===> Epoch[67](114/122): Loss: 0.0002\n","===> Epoch[67](115/122): Loss: 0.0001\n","===> Epoch[67](116/122): Loss: 0.0002\n","===> Epoch[67](117/122): Loss: 0.0002\n","===> Epoch[67](118/122): Loss: 0.0002\n","===> Epoch[67](119/122): Loss: 0.0002\n","===> Epoch[67](120/122): Loss: 0.0002\n","===> Epoch[67](121/122): Loss: 0.0002\n","===> Epoch[67](122/122): Loss: 0.0002\n","psnr_indiv=  [36.91083272634521] global_psnr=  36.91083272634521\n","===> Epoch 67 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.786250997043005}\n","train_per_mod {'flair': 36.91083272634521}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.2625 dB\n","===> Epoch[68](1/122): Loss: 0.0002\n","===> Epoch[68](2/122): Loss: 0.0002\n","===> Epoch[68](3/122): Loss: 0.0002\n","===> Epoch[68](4/122): Loss: 0.0002\n","===> Epoch[68](5/122): Loss: 0.0002\n","===> Epoch[68](6/122): Loss: 0.0002\n","===> Epoch[68](7/122): Loss: 0.0002\n","===> Epoch[68](8/122): Loss: 0.0002\n","===> Epoch[68](9/122): Loss: 0.0002\n","===> Epoch[68](10/122): Loss: 0.0002\n","===> Epoch[68](11/122): Loss: 0.0002\n","===> Epoch[68](12/122): Loss: 0.0002\n","===> Epoch[68](13/122): Loss: 0.0003\n","===> Epoch[68](14/122): Loss: 0.0002\n","===> Epoch[68](15/122): Loss: 0.0002\n","===> Epoch[68](16/122): Loss: 0.0002\n","===> Epoch[68](17/122): Loss: 0.0002\n","===> Epoch[68](18/122): Loss: 0.0002\n","===> Epoch[68](19/122): Loss: 0.0002\n","===> Epoch[68](20/122): Loss: 0.0002\n","===> Epoch[68](21/122): Loss: 0.0002\n","===> Epoch[68](22/122): Loss: 0.0002\n","===> Epoch[68](23/122): Loss: 0.0002\n","===> Epoch[68](24/122): Loss: 0.0002\n","===> Epoch[68](25/122): Loss: 0.0002\n","===> Epoch[68](26/122): Loss: 0.0002\n","===> Epoch[68](27/122): Loss: 0.0002\n","===> Epoch[68](28/122): Loss: 0.0002\n","===> Epoch[68](29/122): Loss: 0.0002\n","===> Epoch[68](30/122): Loss: 0.0001\n","===> Epoch[68](31/122): Loss: 0.0002\n","===> Epoch[68](32/122): Loss: 0.0002\n","===> Epoch[68](33/122): Loss: 0.0002\n","===> Epoch[68](34/122): Loss: 0.0002\n","===> Epoch[68](35/122): Loss: 0.0003\n","===> Epoch[68](36/122): Loss: 0.0002\n","===> Epoch[68](37/122): Loss: 0.0001\n","===> Epoch[68](38/122): Loss: 0.0002\n","===> Epoch[68](39/122): Loss: 0.0002\n","===> Epoch[68](40/122): Loss: 0.0002\n","===> Epoch[68](41/122): Loss: 0.0002\n","===> Epoch[68](42/122): Loss: 0.0002\n","===> Epoch[68](43/122): Loss: 0.0002\n","===> Epoch[68](44/122): Loss: 0.0002\n","===> Epoch[68](45/122): Loss: 0.0002\n","===> Epoch[68](46/122): Loss: 0.0002\n","===> Epoch[68](47/122): Loss: 0.0002\n","===> Epoch[68](48/122): Loss: 0.0002\n","===> Epoch[68](49/122): Loss: 0.0002\n","===> Epoch[68](50/122): Loss: 0.0002\n","===> Epoch[68](51/122): Loss: 0.0003\n","===> Epoch[68](52/122): Loss: 0.0002\n","===> Epoch[68](53/122): Loss: 0.0002\n","===> Epoch[68](54/122): Loss: 0.0002\n","===> Epoch[68](55/122): Loss: 0.0002\n","===> Epoch[68](56/122): Loss: 0.0002\n","===> Epoch[68](57/122): Loss: 0.0002\n","===> Epoch[68](58/122): Loss: 0.0002\n","===> Epoch[68](59/122): Loss: 0.0002\n","===> Epoch[68](60/122): Loss: 0.0002\n","===> Epoch[68](61/122): Loss: 0.0002\n","===> Epoch[68](62/122): Loss: 0.0002\n","===> Epoch[68](63/122): Loss: 0.0002\n","===> Epoch[68](64/122): Loss: 0.0002\n","===> Epoch[68](65/122): Loss: 0.0002\n","===> Epoch[68](66/122): Loss: 0.0002\n","===> Epoch[68](67/122): Loss: 0.0002\n","===> Epoch[68](68/122): Loss: 0.0002\n","===> Epoch[68](69/122): Loss: 0.0002\n","===> Epoch[68](70/122): Loss: 0.0002\n","===> Epoch[68](71/122): Loss: 0.0002\n","===> Epoch[68](72/122): Loss: 0.0002\n","===> Epoch[68](73/122): Loss: 0.0002\n","===> Epoch[68](74/122): Loss: 0.0002\n","===> Epoch[68](75/122): Loss: 0.0002\n","===> Epoch[68](76/122): Loss: 0.0003\n","===> Epoch[68](77/122): Loss: 0.0002\n","===> Epoch[68](78/122): Loss: 0.0002\n","===> Epoch[68](79/122): Loss: 0.0002\n","===> Epoch[68](80/122): Loss: 0.0002\n","===> Epoch[68](81/122): Loss: 0.0003\n","===> Epoch[68](82/122): Loss: 0.0002\n","===> Epoch[68](83/122): Loss: 0.0002\n","===> Epoch[68](84/122): Loss: 0.0002\n","===> Epoch[68](85/122): Loss: 0.0002\n","===> Epoch[68](86/122): Loss: 0.0002\n","===> Epoch[68](87/122): Loss: 0.0002\n","===> Epoch[68](88/122): Loss: 0.0002\n","===> Epoch[68](89/122): Loss: 0.0002\n","===> Epoch[68](90/122): Loss: 0.0002\n","===> Epoch[68](91/122): Loss: 0.0002\n","===> Epoch[68](92/122): Loss: 0.0002\n","===> Epoch[68](93/122): Loss: 0.0002\n","===> Epoch[68](94/122): Loss: 0.0002\n","===> Epoch[68](95/122): Loss: 0.0002\n","===> Epoch[68](96/122): Loss: 0.0002\n","===> Epoch[68](97/122): Loss: 0.0002\n","===> Epoch[68](98/122): Loss: 0.0002\n","===> Epoch[68](99/122): Loss: 0.0002\n","===> Epoch[68](100/122): Loss: 0.0002\n","===> Epoch[68](101/122): Loss: 0.0002\n","===> Epoch[68](102/122): Loss: 0.0002\n","===> Epoch[68](103/122): Loss: 0.0002\n","===> Epoch[68](104/122): Loss: 0.0002\n","===> Epoch[68](105/122): Loss: 0.0002\n","===> Epoch[68](106/122): Loss: 0.0002\n","===> Epoch[68](107/122): Loss: 0.0002\n","===> Epoch[68](108/122): Loss: 0.0003\n","===> Epoch[68](109/122): Loss: 0.0002\n","===> Epoch[68](110/122): Loss: 0.0002\n","===> Epoch[68](111/122): Loss: 0.0002\n","===> Epoch[68](112/122): Loss: 0.0002\n","===> Epoch[68](113/122): Loss: 0.0002\n","===> Epoch[68](114/122): Loss: 0.0002\n","===> Epoch[68](115/122): Loss: 0.0003\n","===> Epoch[68](116/122): Loss: 0.0002\n","===> Epoch[68](117/122): Loss: 0.0002\n","===> Epoch[68](118/122): Loss: 0.0002\n","===> Epoch[68](119/122): Loss: 0.0002\n","===> Epoch[68](120/122): Loss: 0.0002\n","===> Epoch[68](121/122): Loss: 0.0002\n","===> Epoch[68](122/122): Loss: 0.0002\n","psnr_indiv=  [36.923522274753466] global_psnr=  36.923522274753466\n","===> Epoch 68 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.793639425615815}\n","train_per_mod {'flair': 36.923522274753466}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.2376 dB\n","===> Epoch[69](1/122): Loss: 0.0003\n","===> Epoch[69](2/122): Loss: 0.0002\n","===> Epoch[69](3/122): Loss: 0.0002\n","===> Epoch[69](4/122): Loss: 0.0002\n","===> Epoch[69](5/122): Loss: 0.0002\n","===> Epoch[69](6/122): Loss: 0.0002\n","===> Epoch[69](7/122): Loss: 0.0002\n","===> Epoch[69](8/122): Loss: 0.0002\n","===> Epoch[69](9/122): Loss: 0.0002\n","===> Epoch[69](10/122): Loss: 0.0002\n","===> Epoch[69](11/122): Loss: 0.0002\n","===> Epoch[69](12/122): Loss: 0.0002\n","===> Epoch[69](13/122): Loss: 0.0002\n","===> Epoch[69](14/122): Loss: 0.0002\n","===> Epoch[69](15/122): Loss: 0.0002\n","===> Epoch[69](16/122): Loss: 0.0002\n","===> Epoch[69](17/122): Loss: 0.0002\n","===> Epoch[69](18/122): Loss: 0.0002\n","===> Epoch[69](19/122): Loss: 0.0002\n","===> Epoch[69](20/122): Loss: 0.0002\n","===> Epoch[69](21/122): Loss: 0.0002\n","===> Epoch[69](22/122): Loss: 0.0002\n","===> Epoch[69](23/122): Loss: 0.0002\n","===> Epoch[69](24/122): Loss: 0.0003\n","===> Epoch[69](25/122): Loss: 0.0002\n","===> Epoch[69](26/122): Loss: 0.0002\n","===> Epoch[69](27/122): Loss: 0.0002\n","===> Epoch[69](28/122): Loss: 0.0002\n","===> Epoch[69](29/122): Loss: 0.0002\n","===> Epoch[69](30/122): Loss: 0.0002\n","===> Epoch[69](31/122): Loss: 0.0002\n","===> Epoch[69](32/122): Loss: 0.0002\n","===> Epoch[69](33/122): Loss: 0.0003\n","===> Epoch[69](34/122): Loss: 0.0002\n","===> Epoch[69](35/122): Loss: 0.0002\n","===> Epoch[69](36/122): Loss: 0.0002\n","===> Epoch[69](37/122): Loss: 0.0002\n","===> Epoch[69](38/122): Loss: 0.0002\n","===> Epoch[69](39/122): Loss: 0.0002\n","===> Epoch[69](40/122): Loss: 0.0002\n","===> Epoch[69](41/122): Loss: 0.0002\n","===> Epoch[69](42/122): Loss: 0.0002\n","===> Epoch[69](43/122): Loss: 0.0002\n","===> Epoch[69](44/122): Loss: 0.0002\n","===> Epoch[69](45/122): Loss: 0.0002\n","===> Epoch[69](46/122): Loss: 0.0002\n","===> Epoch[69](47/122): Loss: 0.0002\n","===> Epoch[69](48/122): Loss: 0.0002\n","===> Epoch[69](49/122): Loss: 0.0002\n","===> Epoch[69](50/122): Loss: 0.0002\n","===> Epoch[69](51/122): Loss: 0.0002\n","===> Epoch[69](52/122): Loss: 0.0002\n","===> Epoch[69](53/122): Loss: 0.0002\n","===> Epoch[69](54/122): Loss: 0.0002\n","===> Epoch[69](55/122): Loss: 0.0002\n","===> Epoch[69](56/122): Loss: 0.0002\n","===> Epoch[69](57/122): Loss: 0.0002\n","===> Epoch[69](58/122): Loss: 0.0002\n","===> Epoch[69](59/122): Loss: 0.0002\n","===> Epoch[69](60/122): Loss: 0.0003\n","===> Epoch[69](61/122): Loss: 0.0002\n","===> Epoch[69](62/122): Loss: 0.0002\n","===> Epoch[69](63/122): Loss: 0.0002\n","===> Epoch[69](64/122): Loss: 0.0002\n","===> Epoch[69](65/122): Loss: 0.0002\n","===> Epoch[69](66/122): Loss: 0.0002\n","===> Epoch[69](67/122): Loss: 0.0002\n","===> Epoch[69](68/122): Loss: 0.0002\n","===> Epoch[69](69/122): Loss: 0.0003\n","===> Epoch[69](70/122): Loss: 0.0002\n","===> Epoch[69](71/122): Loss: 0.0002\n","===> Epoch[69](72/122): Loss: 0.0002\n","===> Epoch[69](73/122): Loss: 0.0002\n","===> Epoch[69](74/122): Loss: 0.0002\n","===> Epoch[69](75/122): Loss: 0.0002\n","===> Epoch[69](76/122): Loss: 0.0002\n","===> Epoch[69](77/122): Loss: 0.0002\n","===> Epoch[69](78/122): Loss: 0.0002\n","===> Epoch[69](79/122): Loss: 0.0002\n","===> Epoch[69](80/122): Loss: 0.0002\n","===> Epoch[69](81/122): Loss: 0.0002\n","===> Epoch[69](82/122): Loss: 0.0002\n","===> Epoch[69](83/122): Loss: 0.0002\n","===> Epoch[69](84/122): Loss: 0.0002\n","===> Epoch[69](85/122): Loss: 0.0002\n","===> Epoch[69](86/122): Loss: 0.0002\n","===> Epoch[69](87/122): Loss: 0.0002\n","===> Epoch[69](88/122): Loss: 0.0002\n","===> Epoch[69](89/122): Loss: 0.0002\n","===> Epoch[69](90/122): Loss: 0.0002\n","===> Epoch[69](91/122): Loss: 0.0002\n","===> Epoch[69](92/122): Loss: 0.0002\n","===> Epoch[69](93/122): Loss: 0.0002\n","===> Epoch[69](94/122): Loss: 0.0002\n","===> Epoch[69](95/122): Loss: 0.0002\n","===> Epoch[69](96/122): Loss: 0.0002\n","===> Epoch[69](97/122): Loss: 0.0002\n","===> Epoch[69](98/122): Loss: 0.0002\n","===> Epoch[69](99/122): Loss: 0.0002\n","===> Epoch[69](100/122): Loss: 0.0002\n","===> Epoch[69](101/122): Loss: 0.0002\n","===> Epoch[69](102/122): Loss: 0.0002\n","===> Epoch[69](103/122): Loss: 0.0003\n","===> Epoch[69](104/122): Loss: 0.0002\n","===> Epoch[69](105/122): Loss: 0.0002\n","===> Epoch[69](106/122): Loss: 0.0002\n","===> Epoch[69](107/122): Loss: 0.0002\n","===> Epoch[69](108/122): Loss: 0.0002\n","===> Epoch[69](109/122): Loss: 0.0002\n","===> Epoch[69](110/122): Loss: 0.0002\n","===> Epoch[69](111/122): Loss: 0.0003\n","===> Epoch[69](112/122): Loss: 0.0002\n","===> Epoch[69](113/122): Loss: 0.0002\n","===> Epoch[69](114/122): Loss: 0.0002\n","===> Epoch[69](115/122): Loss: 0.0002\n","===> Epoch[69](116/122): Loss: 0.0002\n","===> Epoch[69](117/122): Loss: 0.0002\n","===> Epoch[69](118/122): Loss: 0.0002\n","===> Epoch[69](119/122): Loss: 0.0002\n","===> Epoch[69](120/122): Loss: 0.0002\n","===> Epoch[69](121/122): Loss: 0.0002\n","===> Epoch[69](122/122): Loss: 0.0003\n","psnr_indiv=  [36.91995782448611] global_psnr=  36.91995782448611\n","===> Epoch 69 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.773123759338155}\n","train_per_mod {'flair': 36.91995782448611}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.2438 dB\n","===> Epoch[70](1/122): Loss: 0.0002\n","===> Epoch[70](2/122): Loss: 0.0002\n","===> Epoch[70](3/122): Loss: 0.0002\n","===> Epoch[70](4/122): Loss: 0.0002\n","===> Epoch[70](5/122): Loss: 0.0002\n","===> Epoch[70](6/122): Loss: 0.0002\n","===> Epoch[70](7/122): Loss: 0.0002\n","===> Epoch[70](8/122): Loss: 0.0002\n","===> Epoch[70](9/122): Loss: 0.0001\n","===> Epoch[70](10/122): Loss: 0.0002\n","===> Epoch[70](11/122): Loss: 0.0002\n","===> Epoch[70](12/122): Loss: 0.0002\n","===> Epoch[70](13/122): Loss: 0.0002\n","===> Epoch[70](14/122): Loss: 0.0002\n","===> Epoch[70](15/122): Loss: 0.0002\n","===> Epoch[70](16/122): Loss: 0.0003\n","===> Epoch[70](17/122): Loss: 0.0002\n","===> Epoch[70](18/122): Loss: 0.0002\n","===> Epoch[70](19/122): Loss: 0.0002\n","===> Epoch[70](20/122): Loss: 0.0002\n","===> Epoch[70](21/122): Loss: 0.0002\n","===> Epoch[70](22/122): Loss: 0.0002\n","===> Epoch[70](23/122): Loss: 0.0002\n","===> Epoch[70](24/122): Loss: 0.0003\n","===> Epoch[70](25/122): Loss: 0.0002\n","===> Epoch[70](26/122): Loss: 0.0002\n","===> Epoch[70](27/122): Loss: 0.0002\n","===> Epoch[70](28/122): Loss: 0.0002\n","===> Epoch[70](29/122): Loss: 0.0002\n","===> Epoch[70](30/122): Loss: 0.0002\n","===> Epoch[70](31/122): Loss: 0.0002\n","===> Epoch[70](32/122): Loss: 0.0002\n","===> Epoch[70](33/122): Loss: 0.0002\n","===> Epoch[70](34/122): Loss: 0.0002\n","===> Epoch[70](35/122): Loss: 0.0002\n","===> Epoch[70](36/122): Loss: 0.0002\n","===> Epoch[70](37/122): Loss: 0.0002\n","===> Epoch[70](38/122): Loss: 0.0002\n","===> Epoch[70](39/122): Loss: 0.0002\n","===> Epoch[70](40/122): Loss: 0.0002\n","===> Epoch[70](41/122): Loss: 0.0002\n","===> Epoch[70](42/122): Loss: 0.0002\n","===> Epoch[70](43/122): Loss: 0.0002\n","===> Epoch[70](44/122): Loss: 0.0002\n","===> Epoch[70](45/122): Loss: 0.0002\n","===> Epoch[70](46/122): Loss: 0.0002\n","===> Epoch[70](47/122): Loss: 0.0001\n","===> Epoch[70](48/122): Loss: 0.0002\n","===> Epoch[70](49/122): Loss: 0.0002\n","===> Epoch[70](50/122): Loss: 0.0002\n","===> Epoch[70](51/122): Loss: 0.0002\n","===> Epoch[70](52/122): Loss: 0.0002\n","===> Epoch[70](53/122): Loss: 0.0002\n","===> Epoch[70](54/122): Loss: 0.0002\n","===> Epoch[70](55/122): Loss: 0.0002\n","===> Epoch[70](56/122): Loss: 0.0002\n","===> Epoch[70](57/122): Loss: 0.0002\n","===> Epoch[70](58/122): Loss: 0.0003\n","===> Epoch[70](59/122): Loss: 0.0002\n","===> Epoch[70](60/122): Loss: 0.0002\n","===> Epoch[70](61/122): Loss: 0.0002\n","===> Epoch[70](62/122): Loss: 0.0002\n","===> Epoch[70](63/122): Loss: 0.0002\n","===> Epoch[70](64/122): Loss: 0.0002\n","===> Epoch[70](65/122): Loss: 0.0002\n","===> Epoch[70](66/122): Loss: 0.0002\n","===> Epoch[70](67/122): Loss: 0.0002\n","===> Epoch[70](68/122): Loss: 0.0002\n","===> Epoch[70](69/122): Loss: 0.0002\n","===> Epoch[70](70/122): Loss: 0.0002\n","===> Epoch[70](71/122): Loss: 0.0002\n","===> Epoch[70](72/122): Loss: 0.0002\n","===> Epoch[70](73/122): Loss: 0.0002\n","===> Epoch[70](74/122): Loss: 0.0002\n","===> Epoch[70](75/122): Loss: 0.0002\n","===> Epoch[70](76/122): Loss: 0.0002\n","===> Epoch[70](77/122): Loss: 0.0002\n","===> Epoch[70](78/122): Loss: 0.0002\n","===> Epoch[70](79/122): Loss: 0.0002\n","===> Epoch[70](80/122): Loss: 0.0002\n","===> Epoch[70](81/122): Loss: 0.0002\n","===> Epoch[70](82/122): Loss: 0.0002\n","===> Epoch[70](83/122): Loss: 0.0003\n","===> Epoch[70](84/122): Loss: 0.0002\n","===> Epoch[70](85/122): Loss: 0.0003\n","===> Epoch[70](86/122): Loss: 0.0002\n","===> Epoch[70](87/122): Loss: 0.0002\n","===> Epoch[70](88/122): Loss: 0.0002\n","===> Epoch[70](89/122): Loss: 0.0002\n","===> Epoch[70](90/122): Loss: 0.0002\n","===> Epoch[70](91/122): Loss: 0.0002\n","===> Epoch[70](92/122): Loss: 0.0002\n","===> Epoch[70](93/122): Loss: 0.0002\n","===> Epoch[70](94/122): Loss: 0.0002\n","===> Epoch[70](95/122): Loss: 0.0002\n","===> Epoch[70](96/122): Loss: 0.0002\n","===> Epoch[70](97/122): Loss: 0.0002\n","===> Epoch[70](98/122): Loss: 0.0002\n","===> Epoch[70](99/122): Loss: 0.0002\n","===> Epoch[70](100/122): Loss: 0.0002\n","===> Epoch[70](101/122): Loss: 0.0002\n","===> Epoch[70](102/122): Loss: 0.0002\n","===> Epoch[70](103/122): Loss: 0.0002\n","===> Epoch[70](104/122): Loss: 0.0003\n","===> Epoch[70](105/122): Loss: 0.0002\n","===> Epoch[70](106/122): Loss: 0.0002\n","===> Epoch[70](107/122): Loss: 0.0002\n","===> Epoch[70](108/122): Loss: 0.0002\n","===> Epoch[70](109/122): Loss: 0.0002\n","===> Epoch[70](110/122): Loss: 0.0002\n","===> Epoch[70](111/122): Loss: 0.0002\n","===> Epoch[70](112/122): Loss: 0.0002\n","===> Epoch[70](113/122): Loss: 0.0002\n","===> Epoch[70](114/122): Loss: 0.0002\n","===> Epoch[70](115/122): Loss: 0.0002\n","===> Epoch[70](116/122): Loss: 0.0002\n","===> Epoch[70](117/122): Loss: 0.0002\n","===> Epoch[70](118/122): Loss: 0.0002\n","===> Epoch[70](119/122): Loss: 0.0002\n","===> Epoch[70](120/122): Loss: 0.0002\n","===> Epoch[70](121/122): Loss: 0.0002\n","===> Epoch[70](122/122): Loss: 0.0001\n","psnr_indiv=  [36.95077175837671] global_psnr=  36.95077175837671\n","===> Epoch 70 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.779734641706334}\n","train_per_mod {'flair': 36.95077175837671}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.2922 dB\n","===> Epoch[71](1/122): Loss: 0.0002\n","===> Epoch[71](2/122): Loss: 0.0002\n","===> Epoch[71](3/122): Loss: 0.0002\n","===> Epoch[71](4/122): Loss: 0.0002\n","===> Epoch[71](5/122): Loss: 0.0002\n","===> Epoch[71](6/122): Loss: 0.0002\n","===> Epoch[71](7/122): Loss: 0.0002\n","===> Epoch[71](8/122): Loss: 0.0002\n","===> Epoch[71](9/122): Loss: 0.0002\n","===> Epoch[71](10/122): Loss: 0.0003\n","===> Epoch[71](11/122): Loss: 0.0002\n","===> Epoch[71](12/122): Loss: 0.0002\n","===> Epoch[71](13/122): Loss: 0.0002\n","===> Epoch[71](14/122): Loss: 0.0002\n","===> Epoch[71](15/122): Loss: 0.0002\n","===> Epoch[71](16/122): Loss: 0.0002\n","===> Epoch[71](17/122): Loss: 0.0002\n","===> Epoch[71](18/122): Loss: 0.0002\n","===> Epoch[71](19/122): Loss: 0.0002\n","===> Epoch[71](20/122): Loss: 0.0002\n","===> Epoch[71](21/122): Loss: 0.0002\n","===> Epoch[71](22/122): Loss: 0.0002\n","===> Epoch[71](23/122): Loss: 0.0002\n","===> Epoch[71](24/122): Loss: 0.0002\n","===> Epoch[71](25/122): Loss: 0.0002\n","===> Epoch[71](26/122): Loss: 0.0002\n","===> Epoch[71](27/122): Loss: 0.0002\n","===> Epoch[71](28/122): Loss: 0.0002\n","===> Epoch[71](29/122): Loss: 0.0002\n","===> Epoch[71](30/122): Loss: 0.0002\n","===> Epoch[71](31/122): Loss: 0.0002\n","===> Epoch[71](32/122): Loss: 0.0002\n","===> Epoch[71](33/122): Loss: 0.0002\n","===> Epoch[71](34/122): Loss: 0.0002\n","===> Epoch[71](35/122): Loss: 0.0002\n","===> Epoch[71](36/122): Loss: 0.0002\n","===> Epoch[71](37/122): Loss: 0.0002\n","===> Epoch[71](38/122): Loss: 0.0002\n","===> Epoch[71](39/122): Loss: 0.0002\n","===> Epoch[71](40/122): Loss: 0.0002\n","===> Epoch[71](41/122): Loss: 0.0002\n","===> Epoch[71](42/122): Loss: 0.0002\n","===> Epoch[71](43/122): Loss: 0.0002\n","===> Epoch[71](44/122): Loss: 0.0002\n","===> Epoch[71](45/122): Loss: 0.0002\n","===> Epoch[71](46/122): Loss: 0.0002\n","===> Epoch[71](47/122): Loss: 0.0002\n","===> Epoch[71](48/122): Loss: 0.0002\n","===> Epoch[71](49/122): Loss: 0.0002\n","===> Epoch[71](50/122): Loss: 0.0001\n","===> Epoch[71](51/122): Loss: 0.0002\n","===> Epoch[71](52/122): Loss: 0.0002\n","===> Epoch[71](53/122): Loss: 0.0002\n","===> Epoch[71](54/122): Loss: 0.0002\n","===> Epoch[71](55/122): Loss: 0.0003\n","===> Epoch[71](56/122): Loss: 0.0002\n","===> Epoch[71](57/122): Loss: 0.0003\n","===> Epoch[71](58/122): Loss: 0.0002\n","===> Epoch[71](59/122): Loss: 0.0002\n","===> Epoch[71](60/122): Loss: 0.0003\n","===> Epoch[71](61/122): Loss: 0.0002\n","===> Epoch[71](62/122): Loss: 0.0002\n","===> Epoch[71](63/122): Loss: 0.0002\n","===> Epoch[71](64/122): Loss: 0.0002\n","===> Epoch[71](65/122): Loss: 0.0002\n","===> Epoch[71](66/122): Loss: 0.0002\n","===> Epoch[71](67/122): Loss: 0.0002\n","===> Epoch[71](68/122): Loss: 0.0002\n","===> Epoch[71](69/122): Loss: 0.0002\n","===> Epoch[71](70/122): Loss: 0.0002\n","===> Epoch[71](71/122): Loss: 0.0002\n","===> Epoch[71](72/122): Loss: 0.0002\n","===> Epoch[71](73/122): Loss: 0.0003\n","===> Epoch[71](74/122): Loss: 0.0002\n","===> Epoch[71](75/122): Loss: 0.0002\n","===> Epoch[71](76/122): Loss: 0.0002\n","===> Epoch[71](77/122): Loss: 0.0002\n","===> Epoch[71](78/122): Loss: 0.0002\n","===> Epoch[71](79/122): Loss: 0.0002\n","===> Epoch[71](80/122): Loss: 0.0002\n","===> Epoch[71](81/122): Loss: 0.0002\n","===> Epoch[71](82/122): Loss: 0.0002\n","===> Epoch[71](83/122): Loss: 0.0002\n","===> Epoch[71](84/122): Loss: 0.0002\n","===> Epoch[71](85/122): Loss: 0.0002\n","===> Epoch[71](86/122): Loss: 0.0002\n","===> Epoch[71](87/122): Loss: 0.0002\n","===> Epoch[71](88/122): Loss: 0.0002\n","===> Epoch[71](89/122): Loss: 0.0002\n","===> Epoch[71](90/122): Loss: 0.0002\n","===> Epoch[71](91/122): Loss: 0.0002\n","===> Epoch[71](92/122): Loss: 0.0002\n","===> Epoch[71](93/122): Loss: 0.0002\n","===> Epoch[71](94/122): Loss: 0.0002\n","===> Epoch[71](95/122): Loss: 0.0002\n","===> Epoch[71](96/122): Loss: 0.0002\n","===> Epoch[71](97/122): Loss: 0.0002\n","===> Epoch[71](98/122): Loss: 0.0002\n","===> Epoch[71](99/122): Loss: 0.0002\n","===> Epoch[71](100/122): Loss: 0.0002\n","===> Epoch[71](101/122): Loss: 0.0002\n","===> Epoch[71](102/122): Loss: 0.0002\n","===> Epoch[71](103/122): Loss: 0.0002\n","===> Epoch[71](104/122): Loss: 0.0002\n","===> Epoch[71](105/122): Loss: 0.0002\n","===> Epoch[71](106/122): Loss: 0.0002\n","===> Epoch[71](107/122): Loss: 0.0002\n","===> Epoch[71](108/122): Loss: 0.0002\n","===> Epoch[71](109/122): Loss: 0.0002\n","===> Epoch[71](110/122): Loss: 0.0002\n","===> Epoch[71](111/122): Loss: 0.0002\n","===> Epoch[71](112/122): Loss: 0.0002\n","===> Epoch[71](113/122): Loss: 0.0002\n","===> Epoch[71](114/122): Loss: 0.0002\n","===> Epoch[71](115/122): Loss: 0.0002\n","===> Epoch[71](116/122): Loss: 0.0002\n","===> Epoch[71](117/122): Loss: 0.0002\n","===> Epoch[71](118/122): Loss: 0.0002\n","===> Epoch[71](119/122): Loss: 0.0002\n","===> Epoch[71](120/122): Loss: 0.0002\n","===> Epoch[71](121/122): Loss: 0.0002\n","===> Epoch[71](122/122): Loss: 0.0002\n","psnr_indiv=  [36.92508569622297] global_psnr=  36.92508569622297\n","===> Epoch 71 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.82449116820754}\n","train_per_mod {'flair': 36.92508569622297}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.2616 dB\n","===> Epoch[72](1/122): Loss: 0.0002\n","===> Epoch[72](2/122): Loss: 0.0002\n","===> Epoch[72](3/122): Loss: 0.0002\n","===> Epoch[72](4/122): Loss: 0.0002\n","===> Epoch[72](5/122): Loss: 0.0002\n","===> Epoch[72](6/122): Loss: 0.0002\n","===> Epoch[72](7/122): Loss: 0.0002\n","===> Epoch[72](8/122): Loss: 0.0002\n","===> Epoch[72](9/122): Loss: 0.0002\n","===> Epoch[72](10/122): Loss: 0.0002\n","===> Epoch[72](11/122): Loss: 0.0002\n","===> Epoch[72](12/122): Loss: 0.0002\n","===> Epoch[72](13/122): Loss: 0.0002\n","===> Epoch[72](14/122): Loss: 0.0003\n","===> Epoch[72](15/122): Loss: 0.0002\n","===> Epoch[72](16/122): Loss: 0.0002\n","===> Epoch[72](17/122): Loss: 0.0002\n","===> Epoch[72](18/122): Loss: 0.0002\n","===> Epoch[72](19/122): Loss: 0.0003\n","===> Epoch[72](20/122): Loss: 0.0002\n","===> Epoch[72](21/122): Loss: 0.0002\n","===> Epoch[72](22/122): Loss: 0.0002\n","===> Epoch[72](23/122): Loss: 0.0002\n","===> Epoch[72](24/122): Loss: 0.0002\n","===> Epoch[72](25/122): Loss: 0.0002\n","===> Epoch[72](26/122): Loss: 0.0002\n","===> Epoch[72](27/122): Loss: 0.0002\n","===> Epoch[72](28/122): Loss: 0.0002\n","===> Epoch[72](29/122): Loss: 0.0002\n","===> Epoch[72](30/122): Loss: 0.0002\n","===> Epoch[72](31/122): Loss: 0.0002\n","===> Epoch[72](32/122): Loss: 0.0002\n","===> Epoch[72](33/122): Loss: 0.0002\n","===> Epoch[72](34/122): Loss: 0.0002\n","===> Epoch[72](35/122): Loss: 0.0002\n","===> Epoch[72](36/122): Loss: 0.0002\n","===> Epoch[72](37/122): Loss: 0.0002\n","===> Epoch[72](38/122): Loss: 0.0002\n","===> Epoch[72](39/122): Loss: 0.0003\n","===> Epoch[72](40/122): Loss: 0.0002\n","===> Epoch[72](41/122): Loss: 0.0002\n","===> Epoch[72](42/122): Loss: 0.0002\n","===> Epoch[72](43/122): Loss: 0.0003\n","===> Epoch[72](44/122): Loss: 0.0002\n","===> Epoch[72](45/122): Loss: 0.0003\n","===> Epoch[72](46/122): Loss: 0.0002\n","===> Epoch[72](47/122): Loss: 0.0002\n","===> Epoch[72](48/122): Loss: 0.0002\n","===> Epoch[72](49/122): Loss: 0.0002\n","===> Epoch[72](50/122): Loss: 0.0002\n","===> Epoch[72](51/122): Loss: 0.0002\n","===> Epoch[72](52/122): Loss: 0.0002\n","===> Epoch[72](53/122): Loss: 0.0002\n","===> Epoch[72](54/122): Loss: 0.0002\n","===> Epoch[72](55/122): Loss: 0.0002\n","===> Epoch[72](56/122): Loss: 0.0002\n","===> Epoch[72](57/122): Loss: 0.0002\n","===> Epoch[72](58/122): Loss: 0.0002\n","===> Epoch[72](59/122): Loss: 0.0002\n","===> Epoch[72](60/122): Loss: 0.0002\n","===> Epoch[72](61/122): Loss: 0.0002\n","===> Epoch[72](62/122): Loss: 0.0002\n","===> Epoch[72](63/122): Loss: 0.0002\n","===> Epoch[72](64/122): Loss: 0.0002\n","===> Epoch[72](65/122): Loss: 0.0002\n","===> Epoch[72](66/122): Loss: 0.0002\n","===> Epoch[72](67/122): Loss: 0.0002\n","===> Epoch[72](68/122): Loss: 0.0002\n","===> Epoch[72](69/122): Loss: 0.0002\n","===> Epoch[72](70/122): Loss: 0.0002\n","===> Epoch[72](71/122): Loss: 0.0002\n","===> Epoch[72](72/122): Loss: 0.0002\n","===> Epoch[72](73/122): Loss: 0.0002\n","===> Epoch[72](74/122): Loss: 0.0002\n","===> Epoch[72](75/122): Loss: 0.0002\n","===> Epoch[72](76/122): Loss: 0.0002\n","===> Epoch[72](77/122): Loss: 0.0002\n","===> Epoch[72](78/122): Loss: 0.0002\n","===> Epoch[72](79/122): Loss: 0.0002\n","===> Epoch[72](80/122): Loss: 0.0002\n","===> Epoch[72](81/122): Loss: 0.0002\n","===> Epoch[72](82/122): Loss: 0.0002\n","===> Epoch[72](83/122): Loss: 0.0003\n","===> Epoch[72](84/122): Loss: 0.0002\n","===> Epoch[72](85/122): Loss: 0.0002\n","===> Epoch[72](86/122): Loss: 0.0002\n","===> Epoch[72](87/122): Loss: 0.0002\n","===> Epoch[72](88/122): Loss: 0.0002\n","===> Epoch[72](89/122): Loss: 0.0002\n","===> Epoch[72](90/122): Loss: 0.0002\n","===> Epoch[72](91/122): Loss: 0.0002\n","===> Epoch[72](92/122): Loss: 0.0002\n","===> Epoch[72](93/122): Loss: 0.0002\n","===> Epoch[72](94/122): Loss: 0.0002\n","===> Epoch[72](95/122): Loss: 0.0002\n","===> Epoch[72](96/122): Loss: 0.0002\n","===> Epoch[72](97/122): Loss: 0.0002\n","===> Epoch[72](98/122): Loss: 0.0002\n","===> Epoch[72](99/122): Loss: 0.0002\n","===> Epoch[72](100/122): Loss: 0.0002\n","===> Epoch[72](101/122): Loss: 0.0002\n","===> Epoch[72](102/122): Loss: 0.0002\n","===> Epoch[72](103/122): Loss: 0.0003\n","===> Epoch[72](104/122): Loss: 0.0002\n","===> Epoch[72](105/122): Loss: 0.0002\n","===> Epoch[72](106/122): Loss: 0.0002\n","===> Epoch[72](107/122): Loss: 0.0002\n","===> Epoch[72](108/122): Loss: 0.0002\n","===> Epoch[72](109/122): Loss: 0.0002\n","===> Epoch[72](110/122): Loss: 0.0002\n","===> Epoch[72](111/122): Loss: 0.0002\n","===> Epoch[72](112/122): Loss: 0.0002\n","===> Epoch[72](113/122): Loss: 0.0002\n","===> Epoch[72](114/122): Loss: 0.0003\n","===> Epoch[72](115/122): Loss: 0.0002\n","===> Epoch[72](116/122): Loss: 0.0002\n","===> Epoch[72](117/122): Loss: 0.0002\n","===> Epoch[72](118/122): Loss: 0.0002\n","===> Epoch[72](119/122): Loss: 0.0002\n","===> Epoch[72](120/122): Loss: 0.0002\n","===> Epoch[72](121/122): Loss: 0.0002\n","===> Epoch[72](122/122): Loss: 0.0002\n","psnr_indiv=  [36.95803313422117] global_psnr=  36.95803313422117\n","===> Epoch 72 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.79661236053933}\n","train_per_mod {'flair': 36.95803313422117}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.2824 dB\n","===> Epoch[73](1/122): Loss: 0.0002\n","===> Epoch[73](2/122): Loss: 0.0002\n","===> Epoch[73](3/122): Loss: 0.0002\n","===> Epoch[73](4/122): Loss: 0.0002\n","===> Epoch[73](5/122): Loss: 0.0002\n","===> Epoch[73](6/122): Loss: 0.0002\n","===> Epoch[73](7/122): Loss: 0.0002\n","===> Epoch[73](8/122): Loss: 0.0002\n","===> Epoch[73](9/122): Loss: 0.0002\n","===> Epoch[73](10/122): Loss: 0.0002\n","===> Epoch[73](11/122): Loss: 0.0002\n","===> Epoch[73](12/122): Loss: 0.0002\n","===> Epoch[73](13/122): Loss: 0.0002\n","===> Epoch[73](14/122): Loss: 0.0002\n","===> Epoch[73](15/122): Loss: 0.0002\n","===> Epoch[73](16/122): Loss: 0.0002\n","===> Epoch[73](17/122): Loss: 0.0002\n","===> Epoch[73](18/122): Loss: 0.0002\n","===> Epoch[73](19/122): Loss: 0.0002\n","===> Epoch[73](20/122): Loss: 0.0002\n","===> Epoch[73](21/122): Loss: 0.0002\n","===> Epoch[73](22/122): Loss: 0.0003\n","===> Epoch[73](23/122): Loss: 0.0002\n","===> Epoch[73](24/122): Loss: 0.0002\n","===> Epoch[73](25/122): Loss: 0.0002\n","===> Epoch[73](26/122): Loss: 0.0002\n","===> Epoch[73](27/122): Loss: 0.0002\n","===> Epoch[73](28/122): Loss: 0.0002\n","===> Epoch[73](29/122): Loss: 0.0003\n","===> Epoch[73](30/122): Loss: 0.0002\n","===> Epoch[73](31/122): Loss: 0.0002\n","===> Epoch[73](32/122): Loss: 0.0002\n","===> Epoch[73](33/122): Loss: 0.0002\n","===> Epoch[73](34/122): Loss: 0.0002\n","===> Epoch[73](35/122): Loss: 0.0002\n","===> Epoch[73](36/122): Loss: 0.0002\n","===> Epoch[73](37/122): Loss: 0.0002\n","===> Epoch[73](38/122): Loss: 0.0002\n","===> Epoch[73](39/122): Loss: 0.0002\n","===> Epoch[73](40/122): Loss: 0.0002\n","===> Epoch[73](41/122): Loss: 0.0002\n","===> Epoch[73](42/122): Loss: 0.0002\n","===> Epoch[73](43/122): Loss: 0.0002\n","===> Epoch[73](44/122): Loss: 0.0002\n","===> Epoch[73](45/122): Loss: 0.0002\n","===> Epoch[73](46/122): Loss: 0.0002\n","===> Epoch[73](47/122): Loss: 0.0002\n","===> Epoch[73](48/122): Loss: 0.0002\n","===> Epoch[73](49/122): Loss: 0.0002\n","===> Epoch[73](50/122): Loss: 0.0002\n","===> Epoch[73](51/122): Loss: 0.0002\n","===> Epoch[73](52/122): Loss: 0.0002\n","===> Epoch[73](53/122): Loss: 0.0002\n","===> Epoch[73](54/122): Loss: 0.0002\n","===> Epoch[73](55/122): Loss: 0.0002\n","===> Epoch[73](56/122): Loss: 0.0002\n","===> Epoch[73](57/122): Loss: 0.0001\n","===> Epoch[73](58/122): Loss: 0.0002\n","===> Epoch[73](59/122): Loss: 0.0002\n","===> Epoch[73](60/122): Loss: 0.0002\n","===> Epoch[73](61/122): Loss: 0.0002\n","===> Epoch[73](62/122): Loss: 0.0002\n","===> Epoch[73](63/122): Loss: 0.0002\n","===> Epoch[73](64/122): Loss: 0.0002\n","===> Epoch[73](65/122): Loss: 0.0002\n","===> Epoch[73](66/122): Loss: 0.0002\n","===> Epoch[73](67/122): Loss: 0.0001\n","===> Epoch[73](68/122): Loss: 0.0002\n","===> Epoch[73](69/122): Loss: 0.0002\n","===> Epoch[73](70/122): Loss: 0.0002\n","===> Epoch[73](71/122): Loss: 0.0002\n","===> Epoch[73](72/122): Loss: 0.0002\n","===> Epoch[73](73/122): Loss: 0.0002\n","===> Epoch[73](74/122): Loss: 0.0002\n","===> Epoch[73](75/122): Loss: 0.0002\n","===> Epoch[73](76/122): Loss: 0.0002\n","===> Epoch[73](77/122): Loss: 0.0002\n","===> Epoch[73](78/122): Loss: 0.0002\n","===> Epoch[73](79/122): Loss: 0.0002\n","===> Epoch[73](80/122): Loss: 0.0002\n","===> Epoch[73](81/122): Loss: 0.0002\n","===> Epoch[73](82/122): Loss: 0.0003\n","===> Epoch[73](83/122): Loss: 0.0002\n","===> Epoch[73](84/122): Loss: 0.0002\n","===> Epoch[73](85/122): Loss: 0.0002\n","===> Epoch[73](86/122): Loss: 0.0002\n","===> Epoch[73](87/122): Loss: 0.0003\n","===> Epoch[73](88/122): Loss: 0.0003\n","===> Epoch[73](89/122): Loss: 0.0002\n","===> Epoch[73](90/122): Loss: 0.0002\n","===> Epoch[73](91/122): Loss: 0.0002\n","===> Epoch[73](92/122): Loss: 0.0003\n","===> Epoch[73](93/122): Loss: 0.0003\n","===> Epoch[73](94/122): Loss: 0.0002\n","===> Epoch[73](95/122): Loss: 0.0002\n","===> Epoch[73](96/122): Loss: 0.0002\n","===> Epoch[73](97/122): Loss: 0.0002\n","===> Epoch[73](98/122): Loss: 0.0002\n","===> Epoch[73](99/122): Loss: 0.0002\n","===> Epoch[73](100/122): Loss: 0.0002\n","===> Epoch[73](101/122): Loss: 0.0002\n","===> Epoch[73](102/122): Loss: 0.0002\n","===> Epoch[73](103/122): Loss: 0.0002\n","===> Epoch[73](104/122): Loss: 0.0002\n","===> Epoch[73](105/122): Loss: 0.0002\n","===> Epoch[73](106/122): Loss: 0.0002\n","===> Epoch[73](107/122): Loss: 0.0002\n","===> Epoch[73](108/122): Loss: 0.0002\n","===> Epoch[73](109/122): Loss: 0.0002\n","===> Epoch[73](110/122): Loss: 0.0002\n","===> Epoch[73](111/122): Loss: 0.0002\n","===> Epoch[73](112/122): Loss: 0.0002\n","===> Epoch[73](113/122): Loss: 0.0001\n","===> Epoch[73](114/122): Loss: 0.0002\n","===> Epoch[73](115/122): Loss: 0.0002\n","===> Epoch[73](116/122): Loss: 0.0002\n","===> Epoch[73](117/122): Loss: 0.0002\n","===> Epoch[73](118/122): Loss: 0.0002\n","===> Epoch[73](119/122): Loss: 0.0002\n","===> Epoch[73](120/122): Loss: 0.0002\n","===> Epoch[73](121/122): Loss: 0.0001\n","===> Epoch[73](122/122): Loss: 0.0002\n","psnr_indiv=  [36.967277446070376] global_psnr=  36.967277446070376\n","===> Epoch 73 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.81293317725912}\n","train_per_mod {'flair': 36.967277446070376}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.3030 dB\n","===> Epoch[74](1/122): Loss: 0.0002\n","===> Epoch[74](2/122): Loss: 0.0002\n","===> Epoch[74](3/122): Loss: 0.0002\n","===> Epoch[74](4/122): Loss: 0.0002\n","===> Epoch[74](5/122): Loss: 0.0002\n","===> Epoch[74](6/122): Loss: 0.0002\n","===> Epoch[74](7/122): Loss: 0.0002\n","===> Epoch[74](8/122): Loss: 0.0003\n","===> Epoch[74](9/122): Loss: 0.0002\n","===> Epoch[74](10/122): Loss: 0.0002\n","===> Epoch[74](11/122): Loss: 0.0002\n","===> Epoch[74](12/122): Loss: 0.0002\n","===> Epoch[74](13/122): Loss: 0.0002\n","===> Epoch[74](14/122): Loss: 0.0002\n","===> Epoch[74](15/122): Loss: 0.0002\n","===> Epoch[74](16/122): Loss: 0.0002\n","===> Epoch[74](17/122): Loss: 0.0002\n","===> Epoch[74](18/122): Loss: 0.0002\n","===> Epoch[74](19/122): Loss: 0.0002\n","===> Epoch[74](20/122): Loss: 0.0002\n","===> Epoch[74](21/122): Loss: 0.0002\n","===> Epoch[74](22/122): Loss: 0.0002\n","===> Epoch[74](23/122): Loss: 0.0002\n","===> Epoch[74](24/122): Loss: 0.0002\n","===> Epoch[74](25/122): Loss: 0.0002\n","===> Epoch[74](26/122): Loss: 0.0002\n","===> Epoch[74](27/122): Loss: 0.0002\n","===> Epoch[74](28/122): Loss: 0.0002\n","===> Epoch[74](29/122): Loss: 0.0002\n","===> Epoch[74](30/122): Loss: 0.0002\n","===> Epoch[74](31/122): Loss: 0.0002\n","===> Epoch[74](32/122): Loss: 0.0002\n","===> Epoch[74](33/122): Loss: 0.0002\n","===> Epoch[74](34/122): Loss: 0.0002\n","===> Epoch[74](35/122): Loss: 0.0002\n","===> Epoch[74](36/122): Loss: 0.0002\n","===> Epoch[74](37/122): Loss: 0.0002\n","===> Epoch[74](38/122): Loss: 0.0002\n","===> Epoch[74](39/122): Loss: 0.0002\n","===> Epoch[74](40/122): Loss: 0.0002\n","===> Epoch[74](41/122): Loss: 0.0002\n","===> Epoch[74](42/122): Loss: 0.0002\n","===> Epoch[74](43/122): Loss: 0.0002\n","===> Epoch[74](44/122): Loss: 0.0002\n","===> Epoch[74](45/122): Loss: 0.0002\n","===> Epoch[74](46/122): Loss: 0.0002\n","===> Epoch[74](47/122): Loss: 0.0002\n","===> Epoch[74](48/122): Loss: 0.0002\n","===> Epoch[74](49/122): Loss: 0.0002\n","===> Epoch[74](50/122): Loss: 0.0002\n","===> Epoch[74](51/122): Loss: 0.0002\n","===> Epoch[74](52/122): Loss: 0.0002\n","===> Epoch[74](53/122): Loss: 0.0002\n","===> Epoch[74](54/122): Loss: 0.0002\n","===> Epoch[74](55/122): Loss: 0.0002\n","===> Epoch[74](56/122): Loss: 0.0002\n","===> Epoch[74](57/122): Loss: 0.0002\n","===> Epoch[74](58/122): Loss: 0.0002\n","===> Epoch[74](59/122): Loss: 0.0002\n","===> Epoch[74](60/122): Loss: 0.0002\n","===> Epoch[74](61/122): Loss: 0.0002\n","===> Epoch[74](62/122): Loss: 0.0002\n","===> Epoch[74](63/122): Loss: 0.0002\n","===> Epoch[74](64/122): Loss: 0.0002\n","===> Epoch[74](65/122): Loss: 0.0002\n","===> Epoch[74](66/122): Loss: 0.0003\n","===> Epoch[74](67/122): Loss: 0.0002\n","===> Epoch[74](68/122): Loss: 0.0002\n","===> Epoch[74](69/122): Loss: 0.0002\n","===> Epoch[74](70/122): Loss: 0.0002\n","===> Epoch[74](71/122): Loss: 0.0002\n","===> Epoch[74](72/122): Loss: 0.0002\n","===> Epoch[74](73/122): Loss: 0.0002\n","===> Epoch[74](74/122): Loss: 0.0002\n","===> Epoch[74](75/122): Loss: 0.0002\n","===> Epoch[74](76/122): Loss: 0.0002\n","===> Epoch[74](77/122): Loss: 0.0002\n","===> Epoch[74](78/122): Loss: 0.0002\n","===> Epoch[74](79/122): Loss: 0.0002\n","===> Epoch[74](80/122): Loss: 0.0003\n","===> Epoch[74](81/122): Loss: 0.0002\n","===> Epoch[74](82/122): Loss: 0.0002\n","===> Epoch[74](83/122): Loss: 0.0002\n","===> Epoch[74](84/122): Loss: 0.0002\n","===> Epoch[74](85/122): Loss: 0.0002\n","===> Epoch[74](86/122): Loss: 0.0002\n","===> Epoch[74](87/122): Loss: 0.0002\n","===> Epoch[74](88/122): Loss: 0.0002\n","===> Epoch[74](89/122): Loss: 0.0002\n","===> Epoch[74](90/122): Loss: 0.0002\n","===> Epoch[74](91/122): Loss: 0.0002\n","===> Epoch[74](92/122): Loss: 0.0002\n","===> Epoch[74](93/122): Loss: 0.0002\n","===> Epoch[74](94/122): Loss: 0.0002\n","===> Epoch[74](95/122): Loss: 0.0002\n","===> Epoch[74](96/122): Loss: 0.0001\n","===> Epoch[74](97/122): Loss: 0.0002\n","===> Epoch[74](98/122): Loss: 0.0002\n","===> Epoch[74](99/122): Loss: 0.0002\n","===> Epoch[74](100/122): Loss: 0.0002\n","===> Epoch[74](101/122): Loss: 0.0002\n","===> Epoch[74](102/122): Loss: 0.0002\n","===> Epoch[74](103/122): Loss: 0.0002\n","===> Epoch[74](104/122): Loss: 0.0002\n","===> Epoch[74](105/122): Loss: 0.0002\n","===> Epoch[74](106/122): Loss: 0.0002\n","===> Epoch[74](107/122): Loss: 0.0002\n","===> Epoch[74](108/122): Loss: 0.0002\n","===> Epoch[74](109/122): Loss: 0.0002\n","===> Epoch[74](110/122): Loss: 0.0001\n","===> Epoch[74](111/122): Loss: 0.0002\n","===> Epoch[74](112/122): Loss: 0.0002\n","===> Epoch[74](113/122): Loss: 0.0002\n","===> Epoch[74](114/122): Loss: 0.0002\n","===> Epoch[74](115/122): Loss: 0.0002\n","===> Epoch[74](116/122): Loss: 0.0002\n","===> Epoch[74](117/122): Loss: 0.0002\n","===> Epoch[74](118/122): Loss: 0.0002\n","===> Epoch[74](119/122): Loss: 0.0002\n","===> Epoch[74](120/122): Loss: 0.0002\n","===> Epoch[74](121/122): Loss: 0.0002\n","===> Epoch[74](122/122): Loss: 0.0002\n","psnr_indiv=  [36.94374912569392] global_psnr=  36.94374912569392\n","===> Epoch 74 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.83535928170919}\n","train_per_mod {'flair': 36.94374912569392}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.2750 dB\n","===> Epoch[75](1/122): Loss: 0.0002\n","===> Epoch[75](2/122): Loss: 0.0002\n","===> Epoch[75](3/122): Loss: 0.0002\n","===> Epoch[75](4/122): Loss: 0.0002\n","===> Epoch[75](5/122): Loss: 0.0002\n","===> Epoch[75](6/122): Loss: 0.0002\n","===> Epoch[75](7/122): Loss: 0.0002\n","===> Epoch[75](8/122): Loss: 0.0002\n","===> Epoch[75](9/122): Loss: 0.0002\n","===> Epoch[75](10/122): Loss: 0.0002\n","===> Epoch[75](11/122): Loss: 0.0002\n","===> Epoch[75](12/122): Loss: 0.0002\n","===> Epoch[75](13/122): Loss: 0.0002\n","===> Epoch[75](14/122): Loss: 0.0003\n","===> Epoch[75](15/122): Loss: 0.0002\n","===> Epoch[75](16/122): Loss: 0.0002\n","===> Epoch[75](17/122): Loss: 0.0002\n","===> Epoch[75](18/122): Loss: 0.0002\n","===> Epoch[75](19/122): Loss: 0.0002\n","===> Epoch[75](20/122): Loss: 0.0002\n","===> Epoch[75](21/122): Loss: 0.0003\n","===> Epoch[75](22/122): Loss: 0.0002\n","===> Epoch[75](23/122): Loss: 0.0002\n","===> Epoch[75](24/122): Loss: 0.0002\n","===> Epoch[75](25/122): Loss: 0.0002\n","===> Epoch[75](26/122): Loss: 0.0002\n","===> Epoch[75](27/122): Loss: 0.0002\n","===> Epoch[75](28/122): Loss: 0.0002\n","===> Epoch[75](29/122): Loss: 0.0002\n","===> Epoch[75](30/122): Loss: 0.0002\n","===> Epoch[75](31/122): Loss: 0.0002\n","===> Epoch[75](32/122): Loss: 0.0002\n","===> Epoch[75](33/122): Loss: 0.0001\n","===> Epoch[75](34/122): Loss: 0.0002\n","===> Epoch[75](35/122): Loss: 0.0002\n","===> Epoch[75](36/122): Loss: 0.0002\n","===> Epoch[75](37/122): Loss: 0.0002\n","===> Epoch[75](38/122): Loss: 0.0002\n","===> Epoch[75](39/122): Loss: 0.0002\n","===> Epoch[75](40/122): Loss: 0.0002\n","===> Epoch[75](41/122): Loss: 0.0002\n","===> Epoch[75](42/122): Loss: 0.0002\n","===> Epoch[75](43/122): Loss: 0.0002\n","===> Epoch[75](44/122): Loss: 0.0002\n","===> Epoch[75](45/122): Loss: 0.0001\n","===> Epoch[75](46/122): Loss: 0.0002\n","===> Epoch[75](47/122): Loss: 0.0002\n","===> Epoch[75](48/122): Loss: 0.0002\n","===> Epoch[75](49/122): Loss: 0.0002\n","===> Epoch[75](50/122): Loss: 0.0002\n","===> Epoch[75](51/122): Loss: 0.0002\n","===> Epoch[75](52/122): Loss: 0.0002\n","===> Epoch[75](53/122): Loss: 0.0002\n","===> Epoch[75](54/122): Loss: 0.0002\n","===> Epoch[75](55/122): Loss: 0.0002\n","===> Epoch[75](56/122): Loss: 0.0002\n","===> Epoch[75](57/122): Loss: 0.0002\n","===> Epoch[75](58/122): Loss: 0.0003\n","===> Epoch[75](59/122): Loss: 0.0002\n","===> Epoch[75](60/122): Loss: 0.0002\n","===> Epoch[75](61/122): Loss: 0.0002\n","===> Epoch[75](62/122): Loss: 0.0002\n","===> Epoch[75](63/122): Loss: 0.0002\n","===> Epoch[75](64/122): Loss: 0.0002\n","===> Epoch[75](65/122): Loss: 0.0002\n","===> Epoch[75](66/122): Loss: 0.0002\n","===> Epoch[75](67/122): Loss: 0.0002\n","===> Epoch[75](68/122): Loss: 0.0002\n","===> Epoch[75](69/122): Loss: 0.0002\n","===> Epoch[75](70/122): Loss: 0.0002\n","===> Epoch[75](71/122): Loss: 0.0002\n","===> Epoch[75](72/122): Loss: 0.0002\n","===> Epoch[75](73/122): Loss: 0.0002\n","===> Epoch[75](74/122): Loss: 0.0002\n","===> Epoch[75](75/122): Loss: 0.0002\n","===> Epoch[75](76/122): Loss: 0.0002\n","===> Epoch[75](77/122): Loss: 0.0002\n","===> Epoch[75](78/122): Loss: 0.0002\n","===> Epoch[75](79/122): Loss: 0.0002\n","===> Epoch[75](80/122): Loss: 0.0003\n","===> Epoch[75](81/122): Loss: 0.0002\n","===> Epoch[75](82/122): Loss: 0.0002\n","===> Epoch[75](83/122): Loss: 0.0002\n","===> Epoch[75](84/122): Loss: 0.0002\n","===> Epoch[75](85/122): Loss: 0.0002\n","===> Epoch[75](86/122): Loss: 0.0002\n","===> Epoch[75](87/122): Loss: 0.0002\n","===> Epoch[75](88/122): Loss: 0.0002\n","===> Epoch[75](89/122): Loss: 0.0002\n","===> Epoch[75](90/122): Loss: 0.0002\n","===> Epoch[75](91/122): Loss: 0.0002\n","===> Epoch[75](92/122): Loss: 0.0002\n","===> Epoch[75](93/122): Loss: 0.0002\n","===> Epoch[75](94/122): Loss: 0.0002\n","===> Epoch[75](95/122): Loss: 0.0002\n","===> Epoch[75](96/122): Loss: 0.0002\n","===> Epoch[75](97/122): Loss: 0.0002\n","===> Epoch[75](98/122): Loss: 0.0002\n","===> Epoch[75](99/122): Loss: 0.0002\n","===> Epoch[75](100/122): Loss: 0.0002\n","===> Epoch[75](101/122): Loss: 0.0003\n","===> Epoch[75](102/122): Loss: 0.0002\n","===> Epoch[75](103/122): Loss: 0.0002\n","===> Epoch[75](104/122): Loss: 0.0002\n","===> Epoch[75](105/122): Loss: 0.0002\n","===> Epoch[75](106/122): Loss: 0.0002\n","===> Epoch[75](107/122): Loss: 0.0002\n","===> Epoch[75](108/122): Loss: 0.0002\n","===> Epoch[75](109/122): Loss: 0.0002\n","===> Epoch[75](110/122): Loss: 0.0002\n","===> Epoch[75](111/122): Loss: 0.0001\n","===> Epoch[75](112/122): Loss: 0.0002\n","===> Epoch[75](113/122): Loss: 0.0002\n","===> Epoch[75](114/122): Loss: 0.0002\n","===> Epoch[75](115/122): Loss: 0.0002\n","===> Epoch[75](116/122): Loss: 0.0002\n","===> Epoch[75](117/122): Loss: 0.0002\n","===> Epoch[75](118/122): Loss: 0.0002\n","===> Epoch[75](119/122): Loss: 0.0002\n","===> Epoch[75](120/122): Loss: 0.0002\n","===> Epoch[75](121/122): Loss: 0.0002\n","===> Epoch[75](122/122): Loss: 0.0003\n","psnr_indiv=  [36.97747504129288] global_psnr=  36.97747504129288\n","===> Epoch 75 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.809886003177105}\n","train_per_mod {'flair': 36.97747504129288}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.2585 dB\n","===> Epoch[76](1/122): Loss: 0.0002\n","===> Epoch[76](2/122): Loss: 0.0002\n","===> Epoch[76](3/122): Loss: 0.0002\n","===> Epoch[76](4/122): Loss: 0.0002\n","===> Epoch[76](5/122): Loss: 0.0002\n","===> Epoch[76](6/122): Loss: 0.0002\n","===> Epoch[76](7/122): Loss: 0.0002\n","===> Epoch[76](8/122): Loss: 0.0002\n","===> Epoch[76](9/122): Loss: 0.0002\n","===> Epoch[76](10/122): Loss: 0.0002\n","===> Epoch[76](11/122): Loss: 0.0002\n","===> Epoch[76](12/122): Loss: 0.0002\n","===> Epoch[76](13/122): Loss: 0.0002\n","===> Epoch[76](14/122): Loss: 0.0002\n","===> Epoch[76](15/122): Loss: 0.0002\n","===> Epoch[76](16/122): Loss: 0.0002\n","===> Epoch[76](17/122): Loss: 0.0002\n","===> Epoch[76](18/122): Loss: 0.0002\n","===> Epoch[76](19/122): Loss: 0.0002\n","===> Epoch[76](20/122): Loss: 0.0002\n","===> Epoch[76](21/122): Loss: 0.0002\n","===> Epoch[76](22/122): Loss: 0.0002\n","===> Epoch[76](23/122): Loss: 0.0002\n","===> Epoch[76](24/122): Loss: 0.0002\n","===> Epoch[76](25/122): Loss: 0.0002\n","===> Epoch[76](26/122): Loss: 0.0002\n","===> Epoch[76](27/122): Loss: 0.0002\n","===> Epoch[76](28/122): Loss: 0.0002\n","===> Epoch[76](29/122): Loss: 0.0002\n","===> Epoch[76](30/122): Loss: 0.0002\n","===> Epoch[76](31/122): Loss: 0.0002\n","===> Epoch[76](32/122): Loss: 0.0002\n","===> Epoch[76](33/122): Loss: 0.0002\n","===> Epoch[76](34/122): Loss: 0.0002\n","===> Epoch[76](35/122): Loss: 0.0002\n","===> Epoch[76](36/122): Loss: 0.0002\n","===> Epoch[76](37/122): Loss: 0.0002\n","===> Epoch[76](38/122): Loss: 0.0003\n","===> Epoch[76](39/122): Loss: 0.0002\n","===> Epoch[76](40/122): Loss: 0.0002\n","===> Epoch[76](41/122): Loss: 0.0002\n","===> Epoch[76](42/122): Loss: 0.0003\n","===> Epoch[76](43/122): Loss: 0.0002\n","===> Epoch[76](44/122): Loss: 0.0002\n","===> Epoch[76](45/122): Loss: 0.0002\n","===> Epoch[76](46/122): Loss: 0.0002\n","===> Epoch[76](47/122): Loss: 0.0002\n","===> Epoch[76](48/122): Loss: 0.0002\n","===> Epoch[76](49/122): Loss: 0.0002\n","===> Epoch[76](50/122): Loss: 0.0002\n","===> Epoch[76](51/122): Loss: 0.0002\n","===> Epoch[76](52/122): Loss: 0.0002\n","===> Epoch[76](53/122): Loss: 0.0002\n","===> Epoch[76](54/122): Loss: 0.0002\n","===> Epoch[76](55/122): Loss: 0.0002\n","===> Epoch[76](56/122): Loss: 0.0002\n","===> Epoch[76](57/122): Loss: 0.0002\n","===> Epoch[76](58/122): Loss: 0.0002\n","===> Epoch[76](59/122): Loss: 0.0002\n","===> Epoch[76](60/122): Loss: 0.0002\n","===> Epoch[76](61/122): Loss: 0.0002\n","===> Epoch[76](62/122): Loss: 0.0001\n","===> Epoch[76](63/122): Loss: 0.0001\n","===> Epoch[76](64/122): Loss: 0.0002\n","===> Epoch[76](65/122): Loss: 0.0002\n","===> Epoch[76](66/122): Loss: 0.0002\n","===> Epoch[76](67/122): Loss: 0.0002\n","===> Epoch[76](68/122): Loss: 0.0002\n","===> Epoch[76](69/122): Loss: 0.0002\n","===> Epoch[76](70/122): Loss: 0.0002\n","===> Epoch[76](71/122): Loss: 0.0002\n","===> Epoch[76](72/122): Loss: 0.0002\n","===> Epoch[76](73/122): Loss: 0.0002\n","===> Epoch[76](74/122): Loss: 0.0002\n","===> Epoch[76](75/122): Loss: 0.0001\n","===> Epoch[76](76/122): Loss: 0.0002\n","===> Epoch[76](77/122): Loss: 0.0002\n","===> Epoch[76](78/122): Loss: 0.0002\n","===> Epoch[76](79/122): Loss: 0.0003\n","===> Epoch[76](80/122): Loss: 0.0002\n","===> Epoch[76](81/122): Loss: 0.0002\n","===> Epoch[76](82/122): Loss: 0.0002\n","===> Epoch[76](83/122): Loss: 0.0002\n","===> Epoch[76](84/122): Loss: 0.0002\n","===> Epoch[76](85/122): Loss: 0.0003\n","===> Epoch[76](86/122): Loss: 0.0002\n","===> Epoch[76](87/122): Loss: 0.0002\n","===> Epoch[76](88/122): Loss: 0.0002\n","===> Epoch[76](89/122): Loss: 0.0002\n","===> Epoch[76](90/122): Loss: 0.0002\n","===> Epoch[76](91/122): Loss: 0.0002\n","===> Epoch[76](92/122): Loss: 0.0002\n","===> Epoch[76](93/122): Loss: 0.0002\n","===> Epoch[76](94/122): Loss: 0.0002\n","===> Epoch[76](95/122): Loss: 0.0002\n","===> Epoch[76](96/122): Loss: 0.0002\n","===> Epoch[76](97/122): Loss: 0.0002\n","===> Epoch[76](98/122): Loss: 0.0002\n","===> Epoch[76](99/122): Loss: 0.0002\n","===> Epoch[76](100/122): Loss: 0.0002\n","===> Epoch[76](101/122): Loss: 0.0002\n","===> Epoch[76](102/122): Loss: 0.0002\n","===> Epoch[76](103/122): Loss: 0.0002\n","===> Epoch[76](104/122): Loss: 0.0002\n","===> Epoch[76](105/122): Loss: 0.0002\n","===> Epoch[76](106/122): Loss: 0.0002\n","===> Epoch[76](107/122): Loss: 0.0002\n","===> Epoch[76](108/122): Loss: 0.0002\n","===> Epoch[76](109/122): Loss: 0.0001\n","===> Epoch[76](110/122): Loss: 0.0002\n","===> Epoch[76](111/122): Loss: 0.0002\n","===> Epoch[76](112/122): Loss: 0.0002\n","===> Epoch[76](113/122): Loss: 0.0002\n","===> Epoch[76](114/122): Loss: 0.0002\n","===> Epoch[76](115/122): Loss: 0.0002\n","===> Epoch[76](116/122): Loss: 0.0002\n","===> Epoch[76](117/122): Loss: 0.0002\n","===> Epoch[76](118/122): Loss: 0.0002\n","===> Epoch[76](119/122): Loss: 0.0002\n","===> Epoch[76](120/122): Loss: 0.0002\n","===> Epoch[76](121/122): Loss: 0.0002\n","===> Epoch[76](122/122): Loss: 0.0003\n","psnr_indiv=  [36.980299549970006] global_psnr=  36.980299549970006\n","===> Epoch 76 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.78828059099525}\n","train_per_mod {'flair': 36.980299549970006}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.3175 dB\n","===> Epoch[77](1/122): Loss: 0.0003\n","===> Epoch[77](2/122): Loss: 0.0002\n","===> Epoch[77](3/122): Loss: 0.0002\n","===> Epoch[77](4/122): Loss: 0.0002\n","===> Epoch[77](5/122): Loss: 0.0002\n","===> Epoch[77](6/122): Loss: 0.0002\n","===> Epoch[77](7/122): Loss: 0.0002\n","===> Epoch[77](8/122): Loss: 0.0002\n","===> Epoch[77](9/122): Loss: 0.0002\n","===> Epoch[77](10/122): Loss: 0.0002\n","===> Epoch[77](11/122): Loss: 0.0002\n","===> Epoch[77](12/122): Loss: 0.0002\n","===> Epoch[77](13/122): Loss: 0.0002\n","===> Epoch[77](14/122): Loss: 0.0002\n","===> Epoch[77](15/122): Loss: 0.0002\n","===> Epoch[77](16/122): Loss: 0.0002\n","===> Epoch[77](17/122): Loss: 0.0001\n","===> Epoch[77](18/122): Loss: 0.0002\n","===> Epoch[77](19/122): Loss: 0.0002\n","===> Epoch[77](20/122): Loss: 0.0002\n","===> Epoch[77](21/122): Loss: 0.0002\n","===> Epoch[77](22/122): Loss: 0.0002\n","===> Epoch[77](23/122): Loss: 0.0001\n","===> Epoch[77](24/122): Loss: 0.0002\n","===> Epoch[77](25/122): Loss: 0.0002\n","===> Epoch[77](26/122): Loss: 0.0002\n","===> Epoch[77](27/122): Loss: 0.0002\n","===> Epoch[77](28/122): Loss: 0.0002\n","===> Epoch[77](29/122): Loss: 0.0002\n","===> Epoch[77](30/122): Loss: 0.0003\n","===> Epoch[77](31/122): Loss: 0.0002\n","===> Epoch[77](32/122): Loss: 0.0002\n","===> Epoch[77](33/122): Loss: 0.0002\n","===> Epoch[77](34/122): Loss: 0.0002\n","===> Epoch[77](35/122): Loss: 0.0002\n","===> Epoch[77](36/122): Loss: 0.0002\n","===> Epoch[77](37/122): Loss: 0.0002\n","===> Epoch[77](38/122): Loss: 0.0002\n","===> Epoch[77](39/122): Loss: 0.0002\n","===> Epoch[77](40/122): Loss: 0.0002\n","===> Epoch[77](41/122): Loss: 0.0002\n","===> Epoch[77](42/122): Loss: 0.0002\n","===> Epoch[77](43/122): Loss: 0.0002\n","===> Epoch[77](44/122): Loss: 0.0002\n","===> Epoch[77](45/122): Loss: 0.0002\n","===> Epoch[77](46/122): Loss: 0.0002\n","===> Epoch[77](47/122): Loss: 0.0002\n","===> Epoch[77](48/122): Loss: 0.0002\n","===> Epoch[77](49/122): Loss: 0.0002\n","===> Epoch[77](50/122): Loss: 0.0002\n","===> Epoch[77](51/122): Loss: 0.0002\n","===> Epoch[77](52/122): Loss: 0.0002\n","===> Epoch[77](53/122): Loss: 0.0002\n","===> Epoch[77](54/122): Loss: 0.0002\n","===> Epoch[77](55/122): Loss: 0.0002\n","===> Epoch[77](56/122): Loss: 0.0002\n","===> Epoch[77](57/122): Loss: 0.0002\n","===> Epoch[77](58/122): Loss: 0.0002\n","===> Epoch[77](59/122): Loss: 0.0002\n","===> Epoch[77](60/122): Loss: 0.0002\n","===> Epoch[77](61/122): Loss: 0.0002\n","===> Epoch[77](62/122): Loss: 0.0002\n","===> Epoch[77](63/122): Loss: 0.0002\n","===> Epoch[77](64/122): Loss: 0.0002\n","===> Epoch[77](65/122): Loss: 0.0002\n","===> Epoch[77](66/122): Loss: 0.0002\n","===> Epoch[77](67/122): Loss: 0.0002\n","===> Epoch[77](68/122): Loss: 0.0002\n","===> Epoch[77](69/122): Loss: 0.0002\n","===> Epoch[77](70/122): Loss: 0.0002\n","===> Epoch[77](71/122): Loss: 0.0002\n","===> Epoch[77](72/122): Loss: 0.0002\n","===> Epoch[77](73/122): Loss: 0.0002\n","===> Epoch[77](74/122): Loss: 0.0002\n","===> Epoch[77](75/122): Loss: 0.0002\n","===> Epoch[77](76/122): Loss: 0.0002\n","===> Epoch[77](77/122): Loss: 0.0002\n","===> Epoch[77](78/122): Loss: 0.0002\n","===> Epoch[77](79/122): Loss: 0.0002\n","===> Epoch[77](80/122): Loss: 0.0002\n","===> Epoch[77](81/122): Loss: 0.0002\n","===> Epoch[77](82/122): Loss: 0.0002\n","===> Epoch[77](83/122): Loss: 0.0002\n","===> Epoch[77](84/122): Loss: 0.0002\n","===> Epoch[77](85/122): Loss: 0.0002\n","===> Epoch[77](86/122): Loss: 0.0002\n","===> Epoch[77](87/122): Loss: 0.0003\n","===> Epoch[77](88/122): Loss: 0.0002\n","===> Epoch[77](89/122): Loss: 0.0002\n","===> Epoch[77](90/122): Loss: 0.0002\n","===> Epoch[77](91/122): Loss: 0.0002\n","===> Epoch[77](92/122): Loss: 0.0002\n","===> Epoch[77](93/122): Loss: 0.0002\n","===> Epoch[77](94/122): Loss: 0.0002\n","===> Epoch[77](95/122): Loss: 0.0002\n","===> Epoch[77](96/122): Loss: 0.0002\n","===> Epoch[77](97/122): Loss: 0.0002\n","===> Epoch[77](98/122): Loss: 0.0002\n","===> Epoch[77](99/122): Loss: 0.0002\n","===> Epoch[77](100/122): Loss: 0.0002\n","===> Epoch[77](101/122): Loss: 0.0002\n","===> Epoch[77](102/122): Loss: 0.0002\n","===> Epoch[77](103/122): Loss: 0.0002\n","===> Epoch[77](104/122): Loss: 0.0002\n","===> Epoch[77](105/122): Loss: 0.0002\n","===> Epoch[77](106/122): Loss: 0.0002\n","===> Epoch[77](107/122): Loss: 0.0002\n","===> Epoch[77](108/122): Loss: 0.0002\n","===> Epoch[77](109/122): Loss: 0.0002\n","===> Epoch[77](110/122): Loss: 0.0002\n","===> Epoch[77](111/122): Loss: 0.0002\n","===> Epoch[77](112/122): Loss: 0.0002\n","===> Epoch[77](113/122): Loss: 0.0002\n","===> Epoch[77](114/122): Loss: 0.0002\n","===> Epoch[77](115/122): Loss: 0.0002\n","===> Epoch[77](116/122): Loss: 0.0002\n","===> Epoch[77](117/122): Loss: 0.0002\n","===> Epoch[77](118/122): Loss: 0.0002\n","===> Epoch[77](119/122): Loss: 0.0002\n","===> Epoch[77](120/122): Loss: 0.0003\n","===> Epoch[77](121/122): Loss: 0.0002\n","===> Epoch[77](122/122): Loss: 0.0002\n","psnr_indiv=  [37.00589051602513] global_psnr=  37.00589051602513\n","===> Epoch 77 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.84640501401613}\n","train_per_mod {'flair': 37.00589051602513}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.3253 dB\n","===> Epoch[78](1/122): Loss: 0.0002\n","===> Epoch[78](2/122): Loss: 0.0002\n","===> Epoch[78](3/122): Loss: 0.0002\n","===> Epoch[78](4/122): Loss: 0.0002\n","===> Epoch[78](5/122): Loss: 0.0002\n","===> Epoch[78](6/122): Loss: 0.0002\n","===> Epoch[78](7/122): Loss: 0.0002\n","===> Epoch[78](8/122): Loss: 0.0002\n","===> Epoch[78](9/122): Loss: 0.0002\n","===> Epoch[78](10/122): Loss: 0.0002\n","===> Epoch[78](11/122): Loss: 0.0002\n","===> Epoch[78](12/122): Loss: 0.0002\n","===> Epoch[78](13/122): Loss: 0.0002\n","===> Epoch[78](14/122): Loss: 0.0002\n","===> Epoch[78](15/122): Loss: 0.0002\n","===> Epoch[78](16/122): Loss: 0.0002\n","===> Epoch[78](17/122): Loss: 0.0001\n","===> Epoch[78](18/122): Loss: 0.0002\n","===> Epoch[78](19/122): Loss: 0.0002\n","===> Epoch[78](20/122): Loss: 0.0002\n","===> Epoch[78](21/122): Loss: 0.0002\n","===> Epoch[78](22/122): Loss: 0.0002\n","===> Epoch[78](23/122): Loss: 0.0002\n","===> Epoch[78](24/122): Loss: 0.0002\n","===> Epoch[78](25/122): Loss: 0.0002\n","===> Epoch[78](26/122): Loss: 0.0002\n","===> Epoch[78](27/122): Loss: 0.0002\n","===> Epoch[78](28/122): Loss: 0.0002\n","===> Epoch[78](29/122): Loss: 0.0002\n","===> Epoch[78](30/122): Loss: 0.0002\n","===> Epoch[78](31/122): Loss: 0.0002\n","===> Epoch[78](32/122): Loss: 0.0002\n","===> Epoch[78](33/122): Loss: 0.0002\n","===> Epoch[78](34/122): Loss: 0.0002\n","===> Epoch[78](35/122): Loss: 0.0002\n","===> Epoch[78](36/122): Loss: 0.0002\n","===> Epoch[78](37/122): Loss: 0.0002\n","===> Epoch[78](38/122): Loss: 0.0002\n","===> Epoch[78](39/122): Loss: 0.0002\n","===> Epoch[78](40/122): Loss: 0.0002\n","===> Epoch[78](41/122): Loss: 0.0002\n","===> Epoch[78](42/122): Loss: 0.0002\n","===> Epoch[78](43/122): Loss: 0.0002\n","===> Epoch[78](44/122): Loss: 0.0002\n","===> Epoch[78](45/122): Loss: 0.0002\n","===> Epoch[78](46/122): Loss: 0.0002\n","===> Epoch[78](47/122): Loss: 0.0002\n","===> Epoch[78](48/122): Loss: 0.0002\n","===> Epoch[78](49/122): Loss: 0.0002\n","===> Epoch[78](50/122): Loss: 0.0002\n","===> Epoch[78](51/122): Loss: 0.0002\n","===> Epoch[78](52/122): Loss: 0.0002\n","===> Epoch[78](53/122): Loss: 0.0002\n","===> Epoch[78](54/122): Loss: 0.0002\n","===> Epoch[78](55/122): Loss: 0.0002\n","===> Epoch[78](56/122): Loss: 0.0002\n","===> Epoch[78](57/122): Loss: 0.0002\n","===> Epoch[78](58/122): Loss: 0.0002\n","===> Epoch[78](59/122): Loss: 0.0002\n","===> Epoch[78](60/122): Loss: 0.0002\n","===> Epoch[78](61/122): Loss: 0.0002\n","===> Epoch[78](62/122): Loss: 0.0002\n","===> Epoch[78](63/122): Loss: 0.0002\n","===> Epoch[78](64/122): Loss: 0.0002\n","===> Epoch[78](65/122): Loss: 0.0002\n","===> Epoch[78](66/122): Loss: 0.0002\n","===> Epoch[78](67/122): Loss: 0.0002\n","===> Epoch[78](68/122): Loss: 0.0002\n","===> Epoch[78](69/122): Loss: 0.0002\n","===> Epoch[78](70/122): Loss: 0.0002\n","===> Epoch[78](71/122): Loss: 0.0002\n","===> Epoch[78](72/122): Loss: 0.0002\n","===> Epoch[78](73/122): Loss: 0.0002\n","===> Epoch[78](74/122): Loss: 0.0002\n","===> Epoch[78](75/122): Loss: 0.0002\n","===> Epoch[78](76/122): Loss: 0.0002\n","===> Epoch[78](77/122): Loss: 0.0002\n","===> Epoch[78](78/122): Loss: 0.0002\n","===> Epoch[78](79/122): Loss: 0.0002\n","===> Epoch[78](80/122): Loss: 0.0002\n","===> Epoch[78](81/122): Loss: 0.0002\n","===> Epoch[78](82/122): Loss: 0.0002\n","===> Epoch[78](83/122): Loss: 0.0002\n","===> Epoch[78](84/122): Loss: 0.0002\n","===> Epoch[78](85/122): Loss: 0.0002\n","===> Epoch[78](86/122): Loss: 0.0002\n","===> Epoch[78](87/122): Loss: 0.0002\n","===> Epoch[78](88/122): Loss: 0.0002\n","===> Epoch[78](89/122): Loss: 0.0002\n","===> Epoch[78](90/122): Loss: 0.0002\n","===> Epoch[78](91/122): Loss: 0.0002\n","===> Epoch[78](92/122): Loss: 0.0002\n","===> Epoch[78](93/122): Loss: 0.0002\n","===> Epoch[78](94/122): Loss: 0.0001\n","===> Epoch[78](95/122): Loss: 0.0002\n","===> Epoch[78](96/122): Loss: 0.0002\n","===> Epoch[78](97/122): Loss: 0.0002\n","===> Epoch[78](98/122): Loss: 0.0002\n","===> Epoch[78](99/122): Loss: 0.0002\n","===> Epoch[78](100/122): Loss: 0.0002\n","===> Epoch[78](101/122): Loss: 0.0002\n","===> Epoch[78](102/122): Loss: 0.0002\n","===> Epoch[78](103/122): Loss: 0.0002\n","===> Epoch[78](104/122): Loss: 0.0002\n","===> Epoch[78](105/122): Loss: 0.0002\n","===> Epoch[78](106/122): Loss: 0.0002\n","===> Epoch[78](107/122): Loss: 0.0002\n","===> Epoch[78](108/122): Loss: 0.0002\n","===> Epoch[78](109/122): Loss: 0.0002\n","===> Epoch[78](110/122): Loss: 0.0002\n","===> Epoch[78](111/122): Loss: 0.0002\n","===> Epoch[78](112/122): Loss: 0.0002\n","===> Epoch[78](113/122): Loss: 0.0002\n","===> Epoch[78](114/122): Loss: 0.0002\n","===> Epoch[78](115/122): Loss: 0.0002\n","===> Epoch[78](116/122): Loss: 0.0002\n","===> Epoch[78](117/122): Loss: 0.0002\n","===> Epoch[78](118/122): Loss: 0.0002\n","===> Epoch[78](119/122): Loss: 0.0002\n","===> Epoch[78](120/122): Loss: 0.0002\n","===> Epoch[78](121/122): Loss: 0.0002\n","===> Epoch[78](122/122): Loss: 0.0002\n","psnr_indiv=  [37.00847972831419] global_psnr=  37.00847972831419\n","===> Epoch 78 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.86239799417004}\n","train_per_mod {'flair': 37.00847972831419}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.2668 dB\n","===> Epoch[79](1/122): Loss: 0.0002\n","===> Epoch[79](2/122): Loss: 0.0002\n","===> Epoch[79](3/122): Loss: 0.0002\n","===> Epoch[79](4/122): Loss: 0.0002\n","===> Epoch[79](5/122): Loss: 0.0001\n","===> Epoch[79](6/122): Loss: 0.0002\n","===> Epoch[79](7/122): Loss: 0.0002\n","===> Epoch[79](8/122): Loss: 0.0002\n","===> Epoch[79](9/122): Loss: 0.0002\n","===> Epoch[79](10/122): Loss: 0.0002\n","===> Epoch[79](11/122): Loss: 0.0002\n","===> Epoch[79](12/122): Loss: 0.0002\n","===> Epoch[79](13/122): Loss: 0.0002\n","===> Epoch[79](14/122): Loss: 0.0002\n","===> Epoch[79](15/122): Loss: 0.0002\n","===> Epoch[79](16/122): Loss: 0.0002\n","===> Epoch[79](17/122): Loss: 0.0002\n","===> Epoch[79](18/122): Loss: 0.0002\n","===> Epoch[79](19/122): Loss: 0.0002\n","===> Epoch[79](20/122): Loss: 0.0002\n","===> Epoch[79](21/122): Loss: 0.0002\n","===> Epoch[79](22/122): Loss: 0.0002\n","===> Epoch[79](23/122): Loss: 0.0002\n","===> Epoch[79](24/122): Loss: 0.0002\n","===> Epoch[79](25/122): Loss: 0.0002\n","===> Epoch[79](26/122): Loss: 0.0002\n","===> Epoch[79](27/122): Loss: 0.0002\n","===> Epoch[79](28/122): Loss: 0.0002\n","===> Epoch[79](29/122): Loss: 0.0002\n","===> Epoch[79](30/122): Loss: 0.0002\n","===> Epoch[79](31/122): Loss: 0.0002\n","===> Epoch[79](32/122): Loss: 0.0002\n","===> Epoch[79](33/122): Loss: 0.0002\n","===> Epoch[79](34/122): Loss: 0.0002\n","===> Epoch[79](35/122): Loss: 0.0002\n","===> Epoch[79](36/122): Loss: 0.0002\n","===> Epoch[79](37/122): Loss: 0.0002\n","===> Epoch[79](38/122): Loss: 0.0002\n","===> Epoch[79](39/122): Loss: 0.0002\n","===> Epoch[79](40/122): Loss: 0.0002\n","===> Epoch[79](41/122): Loss: 0.0002\n","===> Epoch[79](42/122): Loss: 0.0002\n","===> Epoch[79](43/122): Loss: 0.0002\n","===> Epoch[79](44/122): Loss: 0.0002\n","===> Epoch[79](45/122): Loss: 0.0002\n","===> Epoch[79](46/122): Loss: 0.0002\n","===> Epoch[79](47/122): Loss: 0.0002\n","===> Epoch[79](48/122): Loss: 0.0002\n","===> Epoch[79](49/122): Loss: 0.0002\n","===> Epoch[79](50/122): Loss: 0.0002\n","===> Epoch[79](51/122): Loss: 0.0002\n","===> Epoch[79](52/122): Loss: 0.0002\n","===> Epoch[79](53/122): Loss: 0.0002\n","===> Epoch[79](54/122): Loss: 0.0002\n","===> Epoch[79](55/122): Loss: 0.0002\n","===> Epoch[79](56/122): Loss: 0.0002\n","===> Epoch[79](57/122): Loss: 0.0002\n","===> Epoch[79](58/122): Loss: 0.0002\n","===> Epoch[79](59/122): Loss: 0.0002\n","===> Epoch[79](60/122): Loss: 0.0002\n","===> Epoch[79](61/122): Loss: 0.0002\n","===> Epoch[79](62/122): Loss: 0.0002\n","===> Epoch[79](63/122): Loss: 0.0001\n","===> Epoch[79](64/122): Loss: 0.0002\n","===> Epoch[79](65/122): Loss: 0.0002\n","===> Epoch[79](66/122): Loss: 0.0002\n","===> Epoch[79](67/122): Loss: 0.0002\n","===> Epoch[79](68/122): Loss: 0.0002\n","===> Epoch[79](69/122): Loss: 0.0002\n","===> Epoch[79](70/122): Loss: 0.0002\n","===> Epoch[79](71/122): Loss: 0.0002\n","===> Epoch[79](72/122): Loss: 0.0002\n","===> Epoch[79](73/122): Loss: 0.0002\n","===> Epoch[79](74/122): Loss: 0.0002\n","===> Epoch[79](75/122): Loss: 0.0002\n","===> Epoch[79](76/122): Loss: 0.0002\n","===> Epoch[79](77/122): Loss: 0.0002\n","===> Epoch[79](78/122): Loss: 0.0001\n","===> Epoch[79](79/122): Loss: 0.0002\n","===> Epoch[79](80/122): Loss: 0.0002\n","===> Epoch[79](81/122): Loss: 0.0002\n","===> Epoch[79](82/122): Loss: 0.0002\n","===> Epoch[79](83/122): Loss: 0.0002\n","===> Epoch[79](84/122): Loss: 0.0002\n","===> Epoch[79](85/122): Loss: 0.0002\n","===> Epoch[79](86/122): Loss: 0.0002\n","===> Epoch[79](87/122): Loss: 0.0002\n","===> Epoch[79](88/122): Loss: 0.0002\n","===> Epoch[79](89/122): Loss: 0.0002\n","===> Epoch[79](90/122): Loss: 0.0002\n","===> Epoch[79](91/122): Loss: 0.0002\n","===> Epoch[79](92/122): Loss: 0.0002\n","===> Epoch[79](93/122): Loss: 0.0002\n","===> Epoch[79](94/122): Loss: 0.0002\n","===> Epoch[79](95/122): Loss: 0.0002\n","===> Epoch[79](96/122): Loss: 0.0002\n","===> Epoch[79](97/122): Loss: 0.0002\n","===> Epoch[79](98/122): Loss: 0.0002\n","===> Epoch[79](99/122): Loss: 0.0002\n","===> Epoch[79](100/122): Loss: 0.0002\n","===> Epoch[79](101/122): Loss: 0.0002\n","===> Epoch[79](102/122): Loss: 0.0002\n","===> Epoch[79](103/122): Loss: 0.0002\n","===> Epoch[79](104/122): Loss: 0.0002\n","===> Epoch[79](105/122): Loss: 0.0002\n","===> Epoch[79](106/122): Loss: 0.0002\n","===> Epoch[79](107/122): Loss: 0.0002\n","===> Epoch[79](108/122): Loss: 0.0002\n","===> Epoch[79](109/122): Loss: 0.0002\n","===> Epoch[79](110/122): Loss: 0.0002\n","===> Epoch[79](111/122): Loss: 0.0002\n","===> Epoch[79](112/122): Loss: 0.0002\n","===> Epoch[79](113/122): Loss: 0.0002\n","===> Epoch[79](114/122): Loss: 0.0002\n","===> Epoch[79](115/122): Loss: 0.0002\n","===> Epoch[79](116/122): Loss: 0.0002\n","===> Epoch[79](117/122): Loss: 0.0002\n","===> Epoch[79](118/122): Loss: 0.0002\n","===> Epoch[79](119/122): Loss: 0.0002\n","===> Epoch[79](120/122): Loss: 0.0002\n","===> Epoch[79](121/122): Loss: 0.0002\n","===> Epoch[79](122/122): Loss: 0.0002\n","psnr_indiv=  [37.029263512932495] global_psnr=  37.029263512932495\n","===> Epoch 79 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.79696813136306}\n","train_per_mod {'flair': 37.029263512932495}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.3459 dB\n","===> Epoch[80](1/122): Loss: 0.0002\n","===> Epoch[80](2/122): Loss: 0.0002\n","===> Epoch[80](3/122): Loss: 0.0002\n","===> Epoch[80](4/122): Loss: 0.0002\n","===> Epoch[80](5/122): Loss: 0.0003\n","===> Epoch[80](6/122): Loss: 0.0001\n","===> Epoch[80](7/122): Loss: 0.0002\n","===> Epoch[80](8/122): Loss: 0.0002\n","===> Epoch[80](9/122): Loss: 0.0002\n","===> Epoch[80](10/122): Loss: 0.0003\n","===> Epoch[80](11/122): Loss: 0.0002\n","===> Epoch[80](12/122): Loss: 0.0002\n","===> Epoch[80](13/122): Loss: 0.0002\n","===> Epoch[80](14/122): Loss: 0.0002\n","===> Epoch[80](15/122): Loss: 0.0002\n","===> Epoch[80](16/122): Loss: 0.0002\n","===> Epoch[80](17/122): Loss: 0.0002\n","===> Epoch[80](18/122): Loss: 0.0002\n","===> Epoch[80](19/122): Loss: 0.0002\n","===> Epoch[80](20/122): Loss: 0.0002\n","===> Epoch[80](21/122): Loss: 0.0002\n","===> Epoch[80](22/122): Loss: 0.0002\n","===> Epoch[80](23/122): Loss: 0.0002\n","===> Epoch[80](24/122): Loss: 0.0002\n","===> Epoch[80](25/122): Loss: 0.0002\n","===> Epoch[80](26/122): Loss: 0.0002\n","===> Epoch[80](27/122): Loss: 0.0002\n","===> Epoch[80](28/122): Loss: 0.0002\n","===> Epoch[80](29/122): Loss: 0.0002\n","===> Epoch[80](30/122): Loss: 0.0002\n","===> Epoch[80](31/122): Loss: 0.0002\n","===> Epoch[80](32/122): Loss: 0.0002\n","===> Epoch[80](33/122): Loss: 0.0002\n","===> Epoch[80](34/122): Loss: 0.0002\n","===> Epoch[80](35/122): Loss: 0.0002\n","===> Epoch[80](36/122): Loss: 0.0002\n","===> Epoch[80](37/122): Loss: 0.0001\n","===> Epoch[80](38/122): Loss: 0.0002\n","===> Epoch[80](39/122): Loss: 0.0002\n","===> Epoch[80](40/122): Loss: 0.0002\n","===> Epoch[80](41/122): Loss: 0.0002\n","===> Epoch[80](42/122): Loss: 0.0002\n","===> Epoch[80](43/122): Loss: 0.0002\n","===> Epoch[80](44/122): Loss: 0.0002\n","===> Epoch[80](45/122): Loss: 0.0002\n","===> Epoch[80](46/122): Loss: 0.0002\n","===> Epoch[80](47/122): Loss: 0.0002\n","===> Epoch[80](48/122): Loss: 0.0002\n","===> Epoch[80](49/122): Loss: 0.0002\n","===> Epoch[80](50/122): Loss: 0.0002\n","===> Epoch[80](51/122): Loss: 0.0002\n","===> Epoch[80](52/122): Loss: 0.0002\n","===> Epoch[80](53/122): Loss: 0.0002\n","===> Epoch[80](54/122): Loss: 0.0002\n","===> Epoch[80](55/122): Loss: 0.0002\n","===> Epoch[80](56/122): Loss: 0.0002\n","===> Epoch[80](57/122): Loss: 0.0002\n","===> Epoch[80](58/122): Loss: 0.0002\n","===> Epoch[80](59/122): Loss: 0.0002\n","===> Epoch[80](60/122): Loss: 0.0002\n","===> Epoch[80](61/122): Loss: 0.0002\n","===> Epoch[80](62/122): Loss: 0.0002\n","===> Epoch[80](63/122): Loss: 0.0002\n","===> Epoch[80](64/122): Loss: 0.0002\n","===> Epoch[80](65/122): Loss: 0.0002\n","===> Epoch[80](66/122): Loss: 0.0002\n","===> Epoch[80](67/122): Loss: 0.0002\n","===> Epoch[80](68/122): Loss: 0.0002\n","===> Epoch[80](69/122): Loss: 0.0002\n","===> Epoch[80](70/122): Loss: 0.0002\n","===> Epoch[80](71/122): Loss: 0.0002\n","===> Epoch[80](72/122): Loss: 0.0002\n","===> Epoch[80](73/122): Loss: 0.0002\n","===> Epoch[80](74/122): Loss: 0.0002\n","===> Epoch[80](75/122): Loss: 0.0002\n","===> Epoch[80](76/122): Loss: 0.0002\n","===> Epoch[80](77/122): Loss: 0.0002\n","===> Epoch[80](78/122): Loss: 0.0002\n","===> Epoch[80](79/122): Loss: 0.0002\n","===> Epoch[80](80/122): Loss: 0.0002\n","===> Epoch[80](81/122): Loss: 0.0002\n","===> Epoch[80](82/122): Loss: 0.0002\n","===> Epoch[80](83/122): Loss: 0.0002\n","===> Epoch[80](84/122): Loss: 0.0002\n","===> Epoch[80](85/122): Loss: 0.0002\n","===> Epoch[80](86/122): Loss: 0.0002\n","===> Epoch[80](87/122): Loss: 0.0002\n","===> Epoch[80](88/122): Loss: 0.0002\n","===> Epoch[80](89/122): Loss: 0.0002\n","===> Epoch[80](90/122): Loss: 0.0002\n","===> Epoch[80](91/122): Loss: 0.0002\n","===> Epoch[80](92/122): Loss: 0.0001\n","===> Epoch[80](93/122): Loss: 0.0002\n","===> Epoch[80](94/122): Loss: 0.0002\n","===> Epoch[80](95/122): Loss: 0.0002\n","===> Epoch[80](96/122): Loss: 0.0002\n","===> Epoch[80](97/122): Loss: 0.0002\n","===> Epoch[80](98/122): Loss: 0.0002\n","===> Epoch[80](99/122): Loss: 0.0003\n","===> Epoch[80](100/122): Loss: 0.0002\n","===> Epoch[80](101/122): Loss: 0.0002\n","===> Epoch[80](102/122): Loss: 0.0002\n","===> Epoch[80](103/122): Loss: 0.0002\n","===> Epoch[80](104/122): Loss: 0.0002\n","===> Epoch[80](105/122): Loss: 0.0002\n","===> Epoch[80](106/122): Loss: 0.0002\n","===> Epoch[80](107/122): Loss: 0.0002\n","===> Epoch[80](108/122): Loss: 0.0002\n","===> Epoch[80](109/122): Loss: 0.0002\n","===> Epoch[80](110/122): Loss: 0.0002\n","===> Epoch[80](111/122): Loss: 0.0002\n","===> Epoch[80](112/122): Loss: 0.0002\n","===> Epoch[80](113/122): Loss: 0.0002\n","===> Epoch[80](114/122): Loss: 0.0002\n","===> Epoch[80](115/122): Loss: 0.0002\n","===> Epoch[80](116/122): Loss: 0.0002\n","===> Epoch[80](117/122): Loss: 0.0002\n","===> Epoch[80](118/122): Loss: 0.0002\n","===> Epoch[80](119/122): Loss: 0.0002\n","===> Epoch[80](120/122): Loss: 0.0002\n","===> Epoch[80](121/122): Loss: 0.0002\n","===> Epoch[80](122/122): Loss: 0.0002\n","psnr_indiv=  [37.00182954530627] global_psnr=  37.00182954530627\n","===> Epoch 80 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.88392004178792}\n","train_per_mod {'flair': 37.00182954530627}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.3649 dB\n","===> Epoch[81](1/122): Loss: 0.0002\n","===> Epoch[81](2/122): Loss: 0.0002\n","===> Epoch[81](3/122): Loss: 0.0001\n","===> Epoch[81](4/122): Loss: 0.0002\n","===> Epoch[81](5/122): Loss: 0.0002\n","===> Epoch[81](6/122): Loss: 0.0002\n","===> Epoch[81](7/122): Loss: 0.0002\n","===> Epoch[81](8/122): Loss: 0.0002\n","===> Epoch[81](9/122): Loss: 0.0002\n","===> Epoch[81](10/122): Loss: 0.0002\n","===> Epoch[81](11/122): Loss: 0.0002\n","===> Epoch[81](12/122): Loss: 0.0002\n","===> Epoch[81](13/122): Loss: 0.0002\n","===> Epoch[81](14/122): Loss: 0.0002\n","===> Epoch[81](15/122): Loss: 0.0002\n","===> Epoch[81](16/122): Loss: 0.0002\n","===> Epoch[81](17/122): Loss: 0.0002\n","===> Epoch[81](18/122): Loss: 0.0002\n","===> Epoch[81](19/122): Loss: 0.0002\n","===> Epoch[81](20/122): Loss: 0.0002\n","===> Epoch[81](21/122): Loss: 0.0002\n","===> Epoch[81](22/122): Loss: 0.0002\n","===> Epoch[81](23/122): Loss: 0.0002\n","===> Epoch[81](24/122): Loss: 0.0002\n","===> Epoch[81](25/122): Loss: 0.0003\n","===> Epoch[81](26/122): Loss: 0.0002\n","===> Epoch[81](27/122): Loss: 0.0002\n","===> Epoch[81](28/122): Loss: 0.0002\n","===> Epoch[81](29/122): Loss: 0.0002\n","===> Epoch[81](30/122): Loss: 0.0002\n","===> Epoch[81](31/122): Loss: 0.0002\n","===> Epoch[81](32/122): Loss: 0.0002\n","===> Epoch[81](33/122): Loss: 0.0002\n","===> Epoch[81](34/122): Loss: 0.0002\n","===> Epoch[81](35/122): Loss: 0.0002\n","===> Epoch[81](36/122): Loss: 0.0002\n","===> Epoch[81](37/122): Loss: 0.0002\n","===> Epoch[81](38/122): Loss: 0.0002\n","===> Epoch[81](39/122): Loss: 0.0002\n","===> Epoch[81](40/122): Loss: 0.0001\n","===> Epoch[81](41/122): Loss: 0.0002\n","===> Epoch[81](42/122): Loss: 0.0002\n","===> Epoch[81](43/122): Loss: 0.0001\n","===> Epoch[81](44/122): Loss: 0.0002\n","===> Epoch[81](45/122): Loss: 0.0002\n","===> Epoch[81](46/122): Loss: 0.0002\n","===> Epoch[81](47/122): Loss: 0.0002\n","===> Epoch[81](48/122): Loss: 0.0002\n","===> Epoch[81](49/122): Loss: 0.0002\n","===> Epoch[81](50/122): Loss: 0.0002\n","===> Epoch[81](51/122): Loss: 0.0002\n","===> Epoch[81](52/122): Loss: 0.0002\n","===> Epoch[81](53/122): Loss: 0.0002\n","===> Epoch[81](54/122): Loss: 0.0002\n","===> Epoch[81](55/122): Loss: 0.0002\n","===> Epoch[81](56/122): Loss: 0.0002\n","===> Epoch[81](57/122): Loss: 0.0002\n","===> Epoch[81](58/122): Loss: 0.0002\n","===> Epoch[81](59/122): Loss: 0.0002\n","===> Epoch[81](60/122): Loss: 0.0002\n","===> Epoch[81](61/122): Loss: 0.0002\n","===> Epoch[81](62/122): Loss: 0.0002\n","===> Epoch[81](63/122): Loss: 0.0002\n","===> Epoch[81](64/122): Loss: 0.0002\n","===> Epoch[81](65/122): Loss: 0.0002\n","===> Epoch[81](66/122): Loss: 0.0002\n","===> Epoch[81](67/122): Loss: 0.0002\n","===> Epoch[81](68/122): Loss: 0.0002\n","===> Epoch[81](69/122): Loss: 0.0003\n","===> Epoch[81](70/122): Loss: 0.0002\n","===> Epoch[81](71/122): Loss: 0.0002\n","===> Epoch[81](72/122): Loss: 0.0002\n","===> Epoch[81](73/122): Loss: 0.0002\n","===> Epoch[81](74/122): Loss: 0.0002\n","===> Epoch[81](75/122): Loss: 0.0002\n","===> Epoch[81](76/122): Loss: 0.0002\n","===> Epoch[81](77/122): Loss: 0.0002\n","===> Epoch[81](78/122): Loss: 0.0002\n","===> Epoch[81](79/122): Loss: 0.0002\n","===> Epoch[81](80/122): Loss: 0.0002\n","===> Epoch[81](81/122): Loss: 0.0002\n","===> Epoch[81](82/122): Loss: 0.0002\n","===> Epoch[81](83/122): Loss: 0.0002\n","===> Epoch[81](84/122): Loss: 0.0002\n","===> Epoch[81](85/122): Loss: 0.0002\n","===> Epoch[81](86/122): Loss: 0.0002\n","===> Epoch[81](87/122): Loss: 0.0002\n","===> Epoch[81](88/122): Loss: 0.0002\n","===> Epoch[81](89/122): Loss: 0.0002\n","===> Epoch[81](90/122): Loss: 0.0002\n","===> Epoch[81](91/122): Loss: 0.0002\n","===> Epoch[81](92/122): Loss: 0.0002\n","===> Epoch[81](93/122): Loss: 0.0002\n","===> Epoch[81](94/122): Loss: 0.0002\n","===> Epoch[81](95/122): Loss: 0.0001\n","===> Epoch[81](96/122): Loss: 0.0002\n","===> Epoch[81](97/122): Loss: 0.0002\n","===> Epoch[81](98/122): Loss: 0.0002\n","===> Epoch[81](99/122): Loss: 0.0002\n","===> Epoch[81](100/122): Loss: 0.0002\n","===> Epoch[81](101/122): Loss: 0.0002\n","===> Epoch[81](102/122): Loss: 0.0002\n","===> Epoch[81](103/122): Loss: 0.0002\n","===> Epoch[81](104/122): Loss: 0.0002\n","===> Epoch[81](105/122): Loss: 0.0002\n","===> Epoch[81](106/122): Loss: 0.0002\n","===> Epoch[81](107/122): Loss: 0.0002\n","===> Epoch[81](108/122): Loss: 0.0002\n","===> Epoch[81](109/122): Loss: 0.0002\n","===> Epoch[81](110/122): Loss: 0.0002\n","===> Epoch[81](111/122): Loss: 0.0002\n","===> Epoch[81](112/122): Loss: 0.0002\n","===> Epoch[81](113/122): Loss: 0.0002\n","===> Epoch[81](114/122): Loss: 0.0002\n","===> Epoch[81](115/122): Loss: 0.0002\n","===> Epoch[81](116/122): Loss: 0.0002\n","===> Epoch[81](117/122): Loss: 0.0002\n","===> Epoch[81](118/122): Loss: 0.0002\n","===> Epoch[81](119/122): Loss: 0.0002\n","===> Epoch[81](120/122): Loss: 0.0002\n","===> Epoch[81](121/122): Loss: 0.0002\n","===> Epoch[81](122/122): Loss: 0.0002\n","psnr_indiv=  [37.04087611812975] global_psnr=  37.04087611812975\n","===> Epoch 81 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.903254075709384}\n","train_per_mod {'flair': 37.04087611812975}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.3294 dB\n","===> Epoch[82](1/122): Loss: 0.0002\n","===> Epoch[82](2/122): Loss: 0.0002\n","===> Epoch[82](3/122): Loss: 0.0002\n","===> Epoch[82](4/122): Loss: 0.0002\n","===> Epoch[82](5/122): Loss: 0.0002\n","===> Epoch[82](6/122): Loss: 0.0002\n","===> Epoch[82](7/122): Loss: 0.0002\n","===> Epoch[82](8/122): Loss: 0.0002\n","===> Epoch[82](9/122): Loss: 0.0002\n","===> Epoch[82](10/122): Loss: 0.0002\n","===> Epoch[82](11/122): Loss: 0.0002\n","===> Epoch[82](12/122): Loss: 0.0002\n","===> Epoch[82](13/122): Loss: 0.0002\n","===> Epoch[82](14/122): Loss: 0.0002\n","===> Epoch[82](15/122): Loss: 0.0002\n","===> Epoch[82](16/122): Loss: 0.0002\n","===> Epoch[82](17/122): Loss: 0.0002\n","===> Epoch[82](18/122): Loss: 0.0002\n","===> Epoch[82](19/122): Loss: 0.0002\n","===> Epoch[82](20/122): Loss: 0.0002\n","===> Epoch[82](21/122): Loss: 0.0002\n","===> Epoch[82](22/122): Loss: 0.0002\n","===> Epoch[82](23/122): Loss: 0.0002\n","===> Epoch[82](24/122): Loss: 0.0002\n","===> Epoch[82](25/122): Loss: 0.0002\n","===> Epoch[82](26/122): Loss: 0.0002\n","===> Epoch[82](27/122): Loss: 0.0002\n","===> Epoch[82](28/122): Loss: 0.0002\n","===> Epoch[82](29/122): Loss: 0.0002\n","===> Epoch[82](30/122): Loss: 0.0002\n","===> Epoch[82](31/122): Loss: 0.0002\n","===> Epoch[82](32/122): Loss: 0.0002\n","===> Epoch[82](33/122): Loss: 0.0002\n","===> Epoch[82](34/122): Loss: 0.0002\n","===> Epoch[82](35/122): Loss: 0.0003\n","===> Epoch[82](36/122): Loss: 0.0002\n","===> Epoch[82](37/122): Loss: 0.0002\n","===> Epoch[82](38/122): Loss: 0.0002\n","===> Epoch[82](39/122): Loss: 0.0002\n","===> Epoch[82](40/122): Loss: 0.0002\n","===> Epoch[82](41/122): Loss: 0.0002\n","===> Epoch[82](42/122): Loss: 0.0002\n","===> Epoch[82](43/122): Loss: 0.0002\n","===> Epoch[82](44/122): Loss: 0.0002\n","===> Epoch[82](45/122): Loss: 0.0002\n","===> Epoch[82](46/122): Loss: 0.0002\n","===> Epoch[82](47/122): Loss: 0.0002\n","===> Epoch[82](48/122): Loss: 0.0003\n","===> Epoch[82](49/122): Loss: 0.0002\n","===> Epoch[82](50/122): Loss: 0.0002\n","===> Epoch[82](51/122): Loss: 0.0002\n","===> Epoch[82](52/122): Loss: 0.0002\n","===> Epoch[82](53/122): Loss: 0.0002\n","===> Epoch[82](54/122): Loss: 0.0002\n","===> Epoch[82](55/122): Loss: 0.0002\n","===> Epoch[82](56/122): Loss: 0.0002\n","===> Epoch[82](57/122): Loss: 0.0002\n","===> Epoch[82](58/122): Loss: 0.0002\n","===> Epoch[82](59/122): Loss: 0.0002\n","===> Epoch[82](60/122): Loss: 0.0002\n","===> Epoch[82](61/122): Loss: 0.0002\n","===> Epoch[82](62/122): Loss: 0.0002\n","===> Epoch[82](63/122): Loss: 0.0002\n","===> Epoch[82](64/122): Loss: 0.0002\n","===> Epoch[82](65/122): Loss: 0.0002\n","===> Epoch[82](66/122): Loss: 0.0002\n","===> Epoch[82](67/122): Loss: 0.0003\n","===> Epoch[82](68/122): Loss: 0.0002\n","===> Epoch[82](69/122): Loss: 0.0002\n","===> Epoch[82](70/122): Loss: 0.0002\n","===> Epoch[82](71/122): Loss: 0.0002\n","===> Epoch[82](72/122): Loss: 0.0002\n","===> Epoch[82](73/122): Loss: 0.0002\n","===> Epoch[82](74/122): Loss: 0.0002\n","===> Epoch[82](75/122): Loss: 0.0002\n","===> Epoch[82](76/122): Loss: 0.0002\n","===> Epoch[82](77/122): Loss: 0.0002\n","===> Epoch[82](78/122): Loss: 0.0002\n","===> Epoch[82](79/122): Loss: 0.0002\n","===> Epoch[82](80/122): Loss: 0.0002\n","===> Epoch[82](81/122): Loss: 0.0002\n","===> Epoch[82](82/122): Loss: 0.0002\n","===> Epoch[82](83/122): Loss: 0.0002\n","===> Epoch[82](84/122): Loss: 0.0002\n","===> Epoch[82](85/122): Loss: 0.0002\n","===> Epoch[82](86/122): Loss: 0.0002\n","===> Epoch[82](87/122): Loss: 0.0002\n","===> Epoch[82](88/122): Loss: 0.0002\n","===> Epoch[82](89/122): Loss: 0.0002\n","===> Epoch[82](90/122): Loss: 0.0002\n","===> Epoch[82](91/122): Loss: 0.0002\n","===> Epoch[82](92/122): Loss: 0.0002\n","===> Epoch[82](93/122): Loss: 0.0002\n","===> Epoch[82](94/122): Loss: 0.0002\n","===> Epoch[82](95/122): Loss: 0.0002\n","===> Epoch[82](96/122): Loss: 0.0001\n","===> Epoch[82](97/122): Loss: 0.0002\n","===> Epoch[82](98/122): Loss: 0.0002\n","===> Epoch[82](99/122): Loss: 0.0002\n","===> Epoch[82](100/122): Loss: 0.0002\n","===> Epoch[82](101/122): Loss: 0.0002\n","===> Epoch[82](102/122): Loss: 0.0002\n","===> Epoch[82](103/122): Loss: 0.0002\n","===> Epoch[82](104/122): Loss: 0.0002\n","===> Epoch[82](105/122): Loss: 0.0002\n","===> Epoch[82](106/122): Loss: 0.0002\n","===> Epoch[82](107/122): Loss: 0.0002\n","===> Epoch[82](108/122): Loss: 0.0002\n","===> Epoch[82](109/122): Loss: 0.0002\n","===> Epoch[82](110/122): Loss: 0.0002\n","===> Epoch[82](111/122): Loss: 0.0002\n","===> Epoch[82](112/122): Loss: 0.0002\n","===> Epoch[82](113/122): Loss: 0.0002\n","===> Epoch[82](114/122): Loss: 0.0002\n","===> Epoch[82](115/122): Loss: 0.0002\n","===> Epoch[82](116/122): Loss: 0.0002\n","===> Epoch[82](117/122): Loss: 0.0002\n","===> Epoch[82](118/122): Loss: 0.0003\n","===> Epoch[82](119/122): Loss: 0.0002\n","===> Epoch[82](120/122): Loss: 0.0002\n","===> Epoch[82](121/122): Loss: 0.0002\n","===> Epoch[82](122/122): Loss: 0.0002\n","psnr_indiv=  [37.048985398492434] global_psnr=  37.048985398492434\n","===> Epoch 82 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.86153558163464}\n","train_per_mod {'flair': 37.048985398492434}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.3087 dB\n","===> Epoch[83](1/122): Loss: 0.0002\n","===> Epoch[83](2/122): Loss: 0.0002\n","===> Epoch[83](3/122): Loss: 0.0001\n","===> Epoch[83](4/122): Loss: 0.0002\n","===> Epoch[83](5/122): Loss: 0.0002\n","===> Epoch[83](6/122): Loss: 0.0001\n","===> Epoch[83](7/122): Loss: 0.0002\n","===> Epoch[83](8/122): Loss: 0.0002\n","===> Epoch[83](9/122): Loss: 0.0002\n","===> Epoch[83](10/122): Loss: 0.0002\n","===> Epoch[83](11/122): Loss: 0.0002\n","===> Epoch[83](12/122): Loss: 0.0002\n","===> Epoch[83](13/122): Loss: 0.0002\n","===> Epoch[83](14/122): Loss: 0.0002\n","===> Epoch[83](15/122): Loss: 0.0002\n","===> Epoch[83](16/122): Loss: 0.0002\n","===> Epoch[83](17/122): Loss: 0.0002\n","===> Epoch[83](18/122): Loss: 0.0002\n","===> Epoch[83](19/122): Loss: 0.0002\n","===> Epoch[83](20/122): Loss: 0.0002\n","===> Epoch[83](21/122): Loss: 0.0002\n","===> Epoch[83](22/122): Loss: 0.0002\n","===> Epoch[83](23/122): Loss: 0.0002\n","===> Epoch[83](24/122): Loss: 0.0002\n","===> Epoch[83](25/122): Loss: 0.0002\n","===> Epoch[83](26/122): Loss: 0.0002\n","===> Epoch[83](27/122): Loss: 0.0002\n","===> Epoch[83](28/122): Loss: 0.0002\n","===> Epoch[83](29/122): Loss: 0.0002\n","===> Epoch[83](30/122): Loss: 0.0001\n","===> Epoch[83](31/122): Loss: 0.0002\n","===> Epoch[83](32/122): Loss: 0.0002\n","===> Epoch[83](33/122): Loss: 0.0002\n","===> Epoch[83](34/122): Loss: 0.0002\n","===> Epoch[83](35/122): Loss: 0.0002\n","===> Epoch[83](36/122): Loss: 0.0002\n","===> Epoch[83](37/122): Loss: 0.0002\n","===> Epoch[83](38/122): Loss: 0.0003\n","===> Epoch[83](39/122): Loss: 0.0002\n","===> Epoch[83](40/122): Loss: 0.0002\n","===> Epoch[83](41/122): Loss: 0.0002\n","===> Epoch[83](42/122): Loss: 0.0002\n","===> Epoch[83](43/122): Loss: 0.0002\n","===> Epoch[83](44/122): Loss: 0.0002\n","===> Epoch[83](45/122): Loss: 0.0002\n","===> Epoch[83](46/122): Loss: 0.0002\n","===> Epoch[83](47/122): Loss: 0.0002\n","===> Epoch[83](48/122): Loss: 0.0002\n","===> Epoch[83](49/122): Loss: 0.0002\n","===> Epoch[83](50/122): Loss: 0.0001\n","===> Epoch[83](51/122): Loss: 0.0002\n","===> Epoch[83](52/122): Loss: 0.0002\n","===> Epoch[83](53/122): Loss: 0.0002\n","===> Epoch[83](54/122): Loss: 0.0002\n","===> Epoch[83](55/122): Loss: 0.0002\n","===> Epoch[83](56/122): Loss: 0.0002\n","===> Epoch[83](57/122): Loss: 0.0002\n","===> Epoch[83](58/122): Loss: 0.0002\n","===> Epoch[83](59/122): Loss: 0.0001\n","===> Epoch[83](60/122): Loss: 0.0002\n","===> Epoch[83](61/122): Loss: 0.0002\n","===> Epoch[83](62/122): Loss: 0.0002\n","===> Epoch[83](63/122): Loss: 0.0002\n","===> Epoch[83](64/122): Loss: 0.0002\n","===> Epoch[83](65/122): Loss: 0.0002\n","===> Epoch[83](66/122): Loss: 0.0002\n","===> Epoch[83](67/122): Loss: 0.0003\n","===> Epoch[83](68/122): Loss: 0.0002\n","===> Epoch[83](69/122): Loss: 0.0002\n","===> Epoch[83](70/122): Loss: 0.0002\n","===> Epoch[83](71/122): Loss: 0.0002\n","===> Epoch[83](72/122): Loss: 0.0002\n","===> Epoch[83](73/122): Loss: 0.0002\n","===> Epoch[83](74/122): Loss: 0.0002\n","===> Epoch[83](75/122): Loss: 0.0002\n","===> Epoch[83](76/122): Loss: 0.0002\n","===> Epoch[83](77/122): Loss: 0.0002\n","===> Epoch[83](78/122): Loss: 0.0003\n","===> Epoch[83](79/122): Loss: 0.0001\n","===> Epoch[83](80/122): Loss: 0.0002\n","===> Epoch[83](81/122): Loss: 0.0002\n","===> Epoch[83](82/122): Loss: 0.0003\n","===> Epoch[83](83/122): Loss: 0.0002\n","===> Epoch[83](84/122): Loss: 0.0002\n","===> Epoch[83](85/122): Loss: 0.0001\n","===> Epoch[83](86/122): Loss: 0.0002\n","===> Epoch[83](87/122): Loss: 0.0002\n","===> Epoch[83](88/122): Loss: 0.0002\n","===> Epoch[83](89/122): Loss: 0.0002\n","===> Epoch[83](90/122): Loss: 0.0002\n","===> Epoch[83](91/122): Loss: 0.0002\n","===> Epoch[83](92/122): Loss: 0.0002\n","===> Epoch[83](93/122): Loss: 0.0002\n","===> Epoch[83](94/122): Loss: 0.0002\n","===> Epoch[83](95/122): Loss: 0.0002\n","===> Epoch[83](96/122): Loss: 0.0002\n","===> Epoch[83](97/122): Loss: 0.0002\n","===> Epoch[83](98/122): Loss: 0.0002\n","===> Epoch[83](99/122): Loss: 0.0002\n","===> Epoch[83](100/122): Loss: 0.0002\n","===> Epoch[83](101/122): Loss: 0.0002\n","===> Epoch[83](102/122): Loss: 0.0002\n","===> Epoch[83](103/122): Loss: 0.0002\n","===> Epoch[83](104/122): Loss: 0.0002\n","===> Epoch[83](105/122): Loss: 0.0003\n","===> Epoch[83](106/122): Loss: 0.0002\n","===> Epoch[83](107/122): Loss: 0.0001\n","===> Epoch[83](108/122): Loss: 0.0002\n","===> Epoch[83](109/122): Loss: 0.0002\n","===> Epoch[83](110/122): Loss: 0.0002\n","===> Epoch[83](111/122): Loss: 0.0002\n","===> Epoch[83](112/122): Loss: 0.0002\n","===> Epoch[83](113/122): Loss: 0.0002\n","===> Epoch[83](114/122): Loss: 0.0003\n","===> Epoch[83](115/122): Loss: 0.0002\n","===> Epoch[83](116/122): Loss: 0.0002\n","===> Epoch[83](117/122): Loss: 0.0002\n","===> Epoch[83](118/122): Loss: 0.0002\n","===> Epoch[83](119/122): Loss: 0.0002\n","===> Epoch[83](120/122): Loss: 0.0001\n","===> Epoch[83](121/122): Loss: 0.0002\n","===> Epoch[83](122/122): Loss: 0.0002\n","psnr_indiv=  [37.05286218612686] global_psnr=  37.05286218612686\n","===> Epoch 83 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.852246834799004}\n","train_per_mod {'flair': 37.05286218612686}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.1755 dB\n","===> Epoch[84](1/122): Loss: 0.0002\n","===> Epoch[84](2/122): Loss: 0.0002\n","===> Epoch[84](3/122): Loss: 0.0002\n","===> Epoch[84](4/122): Loss: 0.0002\n","===> Epoch[84](5/122): Loss: 0.0002\n","===> Epoch[84](6/122): Loss: 0.0002\n","===> Epoch[84](7/122): Loss: 0.0003\n","===> Epoch[84](8/122): Loss: 0.0002\n","===> Epoch[84](9/122): Loss: 0.0002\n","===> Epoch[84](10/122): Loss: 0.0002\n","===> Epoch[84](11/122): Loss: 0.0002\n","===> Epoch[84](12/122): Loss: 0.0002\n","===> Epoch[84](13/122): Loss: 0.0001\n","===> Epoch[84](14/122): Loss: 0.0002\n","===> Epoch[84](15/122): Loss: 0.0002\n","===> Epoch[84](16/122): Loss: 0.0002\n","===> Epoch[84](17/122): Loss: 0.0002\n","===> Epoch[84](18/122): Loss: 0.0002\n","===> Epoch[84](19/122): Loss: 0.0002\n","===> Epoch[84](20/122): Loss: 0.0002\n","===> Epoch[84](21/122): Loss: 0.0003\n","===> Epoch[84](22/122): Loss: 0.0002\n","===> Epoch[84](23/122): Loss: 0.0002\n","===> Epoch[84](24/122): Loss: 0.0002\n","===> Epoch[84](25/122): Loss: 0.0002\n","===> Epoch[84](26/122): Loss: 0.0002\n","===> Epoch[84](27/122): Loss: 0.0002\n","===> Epoch[84](28/122): Loss: 0.0002\n","===> Epoch[84](29/122): Loss: 0.0002\n","===> Epoch[84](30/122): Loss: 0.0002\n","===> Epoch[84](31/122): Loss: 0.0002\n","===> Epoch[84](32/122): Loss: 0.0002\n","===> Epoch[84](33/122): Loss: 0.0002\n","===> Epoch[84](34/122): Loss: 0.0002\n","===> Epoch[84](35/122): Loss: 0.0002\n","===> Epoch[84](36/122): Loss: 0.0002\n","===> Epoch[84](37/122): Loss: 0.0002\n","===> Epoch[84](38/122): Loss: 0.0002\n","===> Epoch[84](39/122): Loss: 0.0002\n","===> Epoch[84](40/122): Loss: 0.0002\n","===> Epoch[84](41/122): Loss: 0.0002\n","===> Epoch[84](42/122): Loss: 0.0002\n","===> Epoch[84](43/122): Loss: 0.0002\n","===> Epoch[84](44/122): Loss: 0.0002\n","===> Epoch[84](45/122): Loss: 0.0002\n","===> Epoch[84](46/122): Loss: 0.0002\n","===> Epoch[84](47/122): Loss: 0.0002\n","===> Epoch[84](48/122): Loss: 0.0002\n","===> Epoch[84](49/122): Loss: 0.0002\n","===> Epoch[84](50/122): Loss: 0.0002\n","===> Epoch[84](51/122): Loss: 0.0002\n","===> Epoch[84](52/122): Loss: 0.0002\n","===> Epoch[84](53/122): Loss: 0.0002\n","===> Epoch[84](54/122): Loss: 0.0002\n","===> Epoch[84](55/122): Loss: 0.0002\n","===> Epoch[84](56/122): Loss: 0.0002\n","===> Epoch[84](57/122): Loss: 0.0002\n","===> Epoch[84](58/122): Loss: 0.0002\n","===> Epoch[84](59/122): Loss: 0.0002\n","===> Epoch[84](60/122): Loss: 0.0002\n","===> Epoch[84](61/122): Loss: 0.0002\n","===> Epoch[84](62/122): Loss: 0.0002\n","===> Epoch[84](63/122): Loss: 0.0002\n","===> Epoch[84](64/122): Loss: 0.0002\n","===> Epoch[84](65/122): Loss: 0.0002\n","===> Epoch[84](66/122): Loss: 0.0002\n","===> Epoch[84](67/122): Loss: 0.0002\n","===> Epoch[84](68/122): Loss: 0.0002\n","===> Epoch[84](69/122): Loss: 0.0002\n","===> Epoch[84](70/122): Loss: 0.0002\n","===> Epoch[84](71/122): Loss: 0.0002\n","===> Epoch[84](72/122): Loss: 0.0002\n","===> Epoch[84](73/122): Loss: 0.0002\n","===> Epoch[84](74/122): Loss: 0.0002\n","===> Epoch[84](75/122): Loss: 0.0002\n","===> Epoch[84](76/122): Loss: 0.0002\n","===> Epoch[84](77/122): Loss: 0.0002\n","===> Epoch[84](78/122): Loss: 0.0002\n","===> Epoch[84](79/122): Loss: 0.0002\n","===> Epoch[84](80/122): Loss: 0.0002\n","===> Epoch[84](81/122): Loss: 0.0002\n","===> Epoch[84](82/122): Loss: 0.0002\n","===> Epoch[84](83/122): Loss: 0.0002\n","===> Epoch[84](84/122): Loss: 0.0002\n","===> Epoch[84](85/122): Loss: 0.0002\n","===> Epoch[84](86/122): Loss: 0.0003\n","===> Epoch[84](87/122): Loss: 0.0002\n","===> Epoch[84](88/122): Loss: 0.0002\n","===> Epoch[84](89/122): Loss: 0.0002\n","===> Epoch[84](90/122): Loss: 0.0002\n","===> Epoch[84](91/122): Loss: 0.0002\n","===> Epoch[84](92/122): Loss: 0.0001\n","===> Epoch[84](93/122): Loss: 0.0002\n","===> Epoch[84](94/122): Loss: 0.0002\n","===> Epoch[84](95/122): Loss: 0.0002\n","===> Epoch[84](96/122): Loss: 0.0002\n","===> Epoch[84](97/122): Loss: 0.0002\n","===> Epoch[84](98/122): Loss: 0.0002\n","===> Epoch[84](99/122): Loss: 0.0002\n","===> Epoch[84](100/122): Loss: 0.0002\n","===> Epoch[84](101/122): Loss: 0.0002\n","===> Epoch[84](102/122): Loss: 0.0002\n","===> Epoch[84](103/122): Loss: 0.0002\n","===> Epoch[84](104/122): Loss: 0.0002\n","===> Epoch[84](105/122): Loss: 0.0002\n","===> Epoch[84](106/122): Loss: 0.0002\n","===> Epoch[84](107/122): Loss: 0.0002\n","===> Epoch[84](108/122): Loss: 0.0002\n","===> Epoch[84](109/122): Loss: 0.0002\n","===> Epoch[84](110/122): Loss: 0.0002\n","===> Epoch[84](111/122): Loss: 0.0002\n","===> Epoch[84](112/122): Loss: 0.0002\n","===> Epoch[84](113/122): Loss: 0.0002\n","===> Epoch[84](114/122): Loss: 0.0002\n","===> Epoch[84](115/122): Loss: 0.0002\n","===> Epoch[84](116/122): Loss: 0.0002\n","===> Epoch[84](117/122): Loss: 0.0002\n","===> Epoch[84](118/122): Loss: 0.0002\n","===> Epoch[84](119/122): Loss: 0.0002\n","===> Epoch[84](120/122): Loss: 0.0002\n","===> Epoch[84](121/122): Loss: 0.0002\n","===> Epoch[84](122/122): Loss: 0.0002\n","psnr_indiv=  [37.03660376394867] global_psnr=  37.03660376394867\n","===> Epoch 84 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.72162548523538}\n","train_per_mod {'flair': 37.03660376394867}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.3590 dB\n","===> Epoch[85](1/122): Loss: 0.0002\n","===> Epoch[85](2/122): Loss: 0.0002\n","===> Epoch[85](3/122): Loss: 0.0002\n","===> Epoch[85](4/122): Loss: 0.0002\n","===> Epoch[85](5/122): Loss: 0.0002\n","===> Epoch[85](6/122): Loss: 0.0002\n","===> Epoch[85](7/122): Loss: 0.0002\n","===> Epoch[85](8/122): Loss: 0.0002\n","===> Epoch[85](9/122): Loss: 0.0002\n","===> Epoch[85](10/122): Loss: 0.0002\n","===> Epoch[85](11/122): Loss: 0.0002\n","===> Epoch[85](12/122): Loss: 0.0002\n","===> Epoch[85](13/122): Loss: 0.0002\n","===> Epoch[85](14/122): Loss: 0.0002\n","===> Epoch[85](15/122): Loss: 0.0002\n","===> Epoch[85](16/122): Loss: 0.0002\n","===> Epoch[85](17/122): Loss: 0.0002\n","===> Epoch[85](18/122): Loss: 0.0003\n","===> Epoch[85](19/122): Loss: 0.0002\n","===> Epoch[85](20/122): Loss: 0.0002\n","===> Epoch[85](21/122): Loss: 0.0002\n","===> Epoch[85](22/122): Loss: 0.0002\n","===> Epoch[85](23/122): Loss: 0.0002\n","===> Epoch[85](24/122): Loss: 0.0002\n","===> Epoch[85](25/122): Loss: 0.0002\n","===> Epoch[85](26/122): Loss: 0.0002\n","===> Epoch[85](27/122): Loss: 0.0002\n","===> Epoch[85](28/122): Loss: 0.0002\n","===> Epoch[85](29/122): Loss: 0.0003\n","===> Epoch[85](30/122): Loss: 0.0002\n","===> Epoch[85](31/122): Loss: 0.0002\n","===> Epoch[85](32/122): Loss: 0.0002\n","===> Epoch[85](33/122): Loss: 0.0002\n","===> Epoch[85](34/122): Loss: 0.0002\n","===> Epoch[85](35/122): Loss: 0.0002\n","===> Epoch[85](36/122): Loss: 0.0002\n","===> Epoch[85](37/122): Loss: 0.0002\n","===> Epoch[85](38/122): Loss: 0.0002\n","===> Epoch[85](39/122): Loss: 0.0002\n","===> Epoch[85](40/122): Loss: 0.0002\n","===> Epoch[85](41/122): Loss: 0.0002\n","===> Epoch[85](42/122): Loss: 0.0002\n","===> Epoch[85](43/122): Loss: 0.0002\n","===> Epoch[85](44/122): Loss: 0.0002\n","===> Epoch[85](45/122): Loss: 0.0002\n","===> Epoch[85](46/122): Loss: 0.0002\n","===> Epoch[85](47/122): Loss: 0.0002\n","===> Epoch[85](48/122): Loss: 0.0002\n","===> Epoch[85](49/122): Loss: 0.0002\n","===> Epoch[85](50/122): Loss: 0.0002\n","===> Epoch[85](51/122): Loss: 0.0002\n","===> Epoch[85](52/122): Loss: 0.0002\n","===> Epoch[85](53/122): Loss: 0.0002\n","===> Epoch[85](54/122): Loss: 0.0002\n","===> Epoch[85](55/122): Loss: 0.0002\n","===> Epoch[85](56/122): Loss: 0.0002\n","===> Epoch[85](57/122): Loss: 0.0002\n","===> Epoch[85](58/122): Loss: 0.0002\n","===> Epoch[85](59/122): Loss: 0.0002\n","===> Epoch[85](60/122): Loss: 0.0002\n","===> Epoch[85](61/122): Loss: 0.0002\n","===> Epoch[85](62/122): Loss: 0.0002\n","===> Epoch[85](63/122): Loss: 0.0001\n","===> Epoch[85](64/122): Loss: 0.0002\n","===> Epoch[85](65/122): Loss: 0.0002\n","===> Epoch[85](66/122): Loss: 0.0002\n","===> Epoch[85](67/122): Loss: 0.0002\n","===> Epoch[85](68/122): Loss: 0.0002\n","===> Epoch[85](69/122): Loss: 0.0002\n","===> Epoch[85](70/122): Loss: 0.0002\n","===> Epoch[85](71/122): Loss: 0.0002\n","===> Epoch[85](72/122): Loss: 0.0002\n","===> Epoch[85](73/122): Loss: 0.0002\n","===> Epoch[85](74/122): Loss: 0.0002\n","===> Epoch[85](75/122): Loss: 0.0002\n","===> Epoch[85](76/122): Loss: 0.0002\n","===> Epoch[85](77/122): Loss: 0.0002\n","===> Epoch[85](78/122): Loss: 0.0002\n","===> Epoch[85](79/122): Loss: 0.0002\n","===> Epoch[85](80/122): Loss: 0.0002\n","===> Epoch[85](81/122): Loss: 0.0002\n","===> Epoch[85](82/122): Loss: 0.0002\n","===> Epoch[85](83/122): Loss: 0.0002\n","===> Epoch[85](84/122): Loss: 0.0002\n","===> Epoch[85](85/122): Loss: 0.0002\n","===> Epoch[85](86/122): Loss: 0.0002\n","===> Epoch[85](87/122): Loss: 0.0002\n","===> Epoch[85](88/122): Loss: 0.0002\n","===> Epoch[85](89/122): Loss: 0.0002\n","===> Epoch[85](90/122): Loss: 0.0002\n","===> Epoch[85](91/122): Loss: 0.0002\n","===> Epoch[85](92/122): Loss: 0.0002\n","===> Epoch[85](93/122): Loss: 0.0002\n","===> Epoch[85](94/122): Loss: 0.0002\n","===> Epoch[85](95/122): Loss: 0.0002\n","===> Epoch[85](96/122): Loss: 0.0002\n","===> Epoch[85](97/122): Loss: 0.0002\n","===> Epoch[85](98/122): Loss: 0.0002\n","===> Epoch[85](99/122): Loss: 0.0002\n","===> Epoch[85](100/122): Loss: 0.0002\n","===> Epoch[85](101/122): Loss: 0.0002\n","===> Epoch[85](102/122): Loss: 0.0002\n","===> Epoch[85](103/122): Loss: 0.0002\n","===> Epoch[85](104/122): Loss: 0.0002\n","===> Epoch[85](105/122): Loss: 0.0002\n","===> Epoch[85](106/122): Loss: 0.0002\n","===> Epoch[85](107/122): Loss: 0.0002\n","===> Epoch[85](108/122): Loss: 0.0002\n","===> Epoch[85](109/122): Loss: 0.0002\n","===> Epoch[85](110/122): Loss: 0.0002\n","===> Epoch[85](111/122): Loss: 0.0002\n","===> Epoch[85](112/122): Loss: 0.0002\n","===> Epoch[85](113/122): Loss: 0.0002\n","===> Epoch[85](114/122): Loss: 0.0002\n","===> Epoch[85](115/122): Loss: 0.0002\n","===> Epoch[85](116/122): Loss: 0.0002\n","===> Epoch[85](117/122): Loss: 0.0002\n","===> Epoch[85](118/122): Loss: 0.0002\n","===> Epoch[85](119/122): Loss: 0.0002\n","===> Epoch[85](120/122): Loss: 0.0002\n","===> Epoch[85](121/122): Loss: 0.0002\n","===> Epoch[85](122/122): Loss: 0.0002\n","psnr_indiv=  [37.08788273005411] global_psnr=  37.08788273005411\n","===> Epoch 85 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.90594894477689}\n","train_per_mod {'flair': 37.08788273005411}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.3743 dB\n","===> Epoch[86](1/122): Loss: 0.0002\n","===> Epoch[86](2/122): Loss: 0.0002\n","===> Epoch[86](3/122): Loss: 0.0002\n","===> Epoch[86](4/122): Loss: 0.0002\n","===> Epoch[86](5/122): Loss: 0.0002\n","===> Epoch[86](6/122): Loss: 0.0002\n","===> Epoch[86](7/122): Loss: 0.0002\n","===> Epoch[86](8/122): Loss: 0.0002\n","===> Epoch[86](9/122): Loss: 0.0002\n","===> Epoch[86](10/122): Loss: 0.0002\n","===> Epoch[86](11/122): Loss: 0.0002\n","===> Epoch[86](12/122): Loss: 0.0002\n","===> Epoch[86](13/122): Loss: 0.0002\n","===> Epoch[86](14/122): Loss: 0.0002\n","===> Epoch[86](15/122): Loss: 0.0002\n","===> Epoch[86](16/122): Loss: 0.0002\n","===> Epoch[86](17/122): Loss: 0.0002\n","===> Epoch[86](18/122): Loss: 0.0003\n","===> Epoch[86](19/122): Loss: 0.0002\n","===> Epoch[86](20/122): Loss: 0.0002\n","===> Epoch[86](21/122): Loss: 0.0002\n","===> Epoch[86](22/122): Loss: 0.0002\n","===> Epoch[86](23/122): Loss: 0.0002\n","===> Epoch[86](24/122): Loss: 0.0002\n","===> Epoch[86](25/122): Loss: 0.0002\n","===> Epoch[86](26/122): Loss: 0.0002\n","===> Epoch[86](27/122): Loss: 0.0002\n","===> Epoch[86](28/122): Loss: 0.0002\n","===> Epoch[86](29/122): Loss: 0.0002\n","===> Epoch[86](30/122): Loss: 0.0002\n","===> Epoch[86](31/122): Loss: 0.0002\n","===> Epoch[86](32/122): Loss: 0.0002\n","===> Epoch[86](33/122): Loss: 0.0003\n","===> Epoch[86](34/122): Loss: 0.0002\n","===> Epoch[86](35/122): Loss: 0.0002\n","===> Epoch[86](36/122): Loss: 0.0002\n","===> Epoch[86](37/122): Loss: 0.0002\n","===> Epoch[86](38/122): Loss: 0.0002\n","===> Epoch[86](39/122): Loss: 0.0002\n","===> Epoch[86](40/122): Loss: 0.0002\n","===> Epoch[86](41/122): Loss: 0.0002\n","===> Epoch[86](42/122): Loss: 0.0002\n","===> Epoch[86](43/122): Loss: 0.0002\n","===> Epoch[86](44/122): Loss: 0.0002\n","===> Epoch[86](45/122): Loss: 0.0002\n","===> Epoch[86](46/122): Loss: 0.0002\n","===> Epoch[86](47/122): Loss: 0.0002\n","===> Epoch[86](48/122): Loss: 0.0002\n","===> Epoch[86](49/122): Loss: 0.0002\n","===> Epoch[86](50/122): Loss: 0.0002\n","===> Epoch[86](51/122): Loss: 0.0002\n","===> Epoch[86](52/122): Loss: 0.0002\n","===> Epoch[86](53/122): Loss: 0.0002\n","===> Epoch[86](54/122): Loss: 0.0002\n","===> Epoch[86](55/122): Loss: 0.0002\n","===> Epoch[86](56/122): Loss: 0.0002\n","===> Epoch[86](57/122): Loss: 0.0002\n","===> Epoch[86](58/122): Loss: 0.0002\n","===> Epoch[86](59/122): Loss: 0.0002\n","===> Epoch[86](60/122): Loss: 0.0002\n","===> Epoch[86](61/122): Loss: 0.0002\n","===> Epoch[86](62/122): Loss: 0.0002\n","===> Epoch[86](63/122): Loss: 0.0002\n","===> Epoch[86](64/122): Loss: 0.0002\n","===> Epoch[86](65/122): Loss: 0.0002\n","===> Epoch[86](66/122): Loss: 0.0002\n","===> Epoch[86](67/122): Loss: 0.0002\n","===> Epoch[86](68/122): Loss: 0.0002\n","===> Epoch[86](69/122): Loss: 0.0002\n","===> Epoch[86](70/122): Loss: 0.0002\n","===> Epoch[86](71/122): Loss: 0.0002\n","===> Epoch[86](72/122): Loss: 0.0002\n","===> Epoch[86](73/122): Loss: 0.0002\n","===> Epoch[86](74/122): Loss: 0.0002\n","===> Epoch[86](75/122): Loss: 0.0002\n","===> Epoch[86](76/122): Loss: 0.0002\n","===> Epoch[86](77/122): Loss: 0.0002\n","===> Epoch[86](78/122): Loss: 0.0003\n","===> Epoch[86](79/122): Loss: 0.0002\n","===> Epoch[86](80/122): Loss: 0.0002\n","===> Epoch[86](81/122): Loss: 0.0002\n","===> Epoch[86](82/122): Loss: 0.0002\n","===> Epoch[86](83/122): Loss: 0.0002\n","===> Epoch[86](84/122): Loss: 0.0002\n","===> Epoch[86](85/122): Loss: 0.0002\n","===> Epoch[86](86/122): Loss: 0.0002\n","===> Epoch[86](87/122): Loss: 0.0002\n","===> Epoch[86](88/122): Loss: 0.0002\n","===> Epoch[86](89/122): Loss: 0.0002\n","===> Epoch[86](90/122): Loss: 0.0002\n","===> Epoch[86](91/122): Loss: 0.0002\n","===> Epoch[86](92/122): Loss: 0.0002\n","===> Epoch[86](93/122): Loss: 0.0002\n","===> Epoch[86](94/122): Loss: 0.0002\n","===> Epoch[86](95/122): Loss: 0.0002\n","===> Epoch[86](96/122): Loss: 0.0002\n","===> Epoch[86](97/122): Loss: 0.0002\n","===> Epoch[86](98/122): Loss: 0.0002\n","===> Epoch[86](99/122): Loss: 0.0002\n","===> Epoch[86](100/122): Loss: 0.0002\n","===> Epoch[86](101/122): Loss: 0.0002\n","===> Epoch[86](102/122): Loss: 0.0002\n","===> Epoch[86](103/122): Loss: 0.0002\n","===> Epoch[86](104/122): Loss: 0.0002\n","===> Epoch[86](105/122): Loss: 0.0002\n","===> Epoch[86](106/122): Loss: 0.0002\n","===> Epoch[86](107/122): Loss: 0.0002\n","===> Epoch[86](108/122): Loss: 0.0002\n","===> Epoch[86](109/122): Loss: 0.0002\n","===> Epoch[86](110/122): Loss: 0.0003\n","===> Epoch[86](111/122): Loss: 0.0002\n","===> Epoch[86](112/122): Loss: 0.0002\n","===> Epoch[86](113/122): Loss: 0.0002\n","===> Epoch[86](114/122): Loss: 0.0002\n","===> Epoch[86](115/122): Loss: 0.0002\n","===> Epoch[86](116/122): Loss: 0.0002\n","===> Epoch[86](117/122): Loss: 0.0002\n","===> Epoch[86](118/122): Loss: 0.0002\n","===> Epoch[86](119/122): Loss: 0.0002\n","===> Epoch[86](120/122): Loss: 0.0002\n","===> Epoch[86](121/122): Loss: 0.0002\n","===> Epoch[86](122/122): Loss: 0.0002\n","psnr_indiv=  [37.08830389551696] global_psnr=  37.08830389551696\n","===> Epoch 86 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.91449870354461}\n","train_per_mod {'flair': 37.08830389551696}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.3630 dB\n","===> Epoch[87](1/122): Loss: 0.0002\n","===> Epoch[87](2/122): Loss: 0.0002\n","===> Epoch[87](3/122): Loss: 0.0002\n","===> Epoch[87](4/122): Loss: 0.0002\n","===> Epoch[87](5/122): Loss: 0.0002\n","===> Epoch[87](6/122): Loss: 0.0002\n","===> Epoch[87](7/122): Loss: 0.0002\n","===> Epoch[87](8/122): Loss: 0.0002\n","===> Epoch[87](9/122): Loss: 0.0002\n","===> Epoch[87](10/122): Loss: 0.0002\n","===> Epoch[87](11/122): Loss: 0.0002\n","===> Epoch[87](12/122): Loss: 0.0002\n","===> Epoch[87](13/122): Loss: 0.0001\n","===> Epoch[87](14/122): Loss: 0.0002\n","===> Epoch[87](15/122): Loss: 0.0002\n","===> Epoch[87](16/122): Loss: 0.0002\n","===> Epoch[87](17/122): Loss: 0.0002\n","===> Epoch[87](18/122): Loss: 0.0002\n","===> Epoch[87](19/122): Loss: 0.0002\n","===> Epoch[87](20/122): Loss: 0.0002\n","===> Epoch[87](21/122): Loss: 0.0002\n","===> Epoch[87](22/122): Loss: 0.0002\n","===> Epoch[87](23/122): Loss: 0.0002\n","===> Epoch[87](24/122): Loss: 0.0002\n","===> Epoch[87](25/122): Loss: 0.0002\n","===> Epoch[87](26/122): Loss: 0.0002\n","===> Epoch[87](27/122): Loss: 0.0002\n","===> Epoch[87](28/122): Loss: 0.0002\n","===> Epoch[87](29/122): Loss: 0.0002\n","===> Epoch[87](30/122): Loss: 0.0002\n","===> Epoch[87](31/122): Loss: 0.0003\n","===> Epoch[87](32/122): Loss: 0.0001\n","===> Epoch[87](33/122): Loss: 0.0002\n","===> Epoch[87](34/122): Loss: 0.0002\n","===> Epoch[87](35/122): Loss: 0.0002\n","===> Epoch[87](36/122): Loss: 0.0002\n","===> Epoch[87](37/122): Loss: 0.0002\n","===> Epoch[87](38/122): Loss: 0.0002\n","===> Epoch[87](39/122): Loss: 0.0002\n","===> Epoch[87](40/122): Loss: 0.0002\n","===> Epoch[87](41/122): Loss: 0.0002\n","===> Epoch[87](42/122): Loss: 0.0002\n","===> Epoch[87](43/122): Loss: 0.0002\n","===> Epoch[87](44/122): Loss: 0.0002\n","===> Epoch[87](45/122): Loss: 0.0002\n","===> Epoch[87](46/122): Loss: 0.0002\n","===> Epoch[87](47/122): Loss: 0.0002\n","===> Epoch[87](48/122): Loss: 0.0002\n","===> Epoch[87](49/122): Loss: 0.0002\n","===> Epoch[87](50/122): Loss: 0.0002\n","===> Epoch[87](51/122): Loss: 0.0002\n","===> Epoch[87](52/122): Loss: 0.0002\n","===> Epoch[87](53/122): Loss: 0.0003\n","===> Epoch[87](54/122): Loss: 0.0002\n","===> Epoch[87](55/122): Loss: 0.0002\n","===> Epoch[87](56/122): Loss: 0.0002\n","===> Epoch[87](57/122): Loss: 0.0002\n","===> Epoch[87](58/122): Loss: 0.0002\n","===> Epoch[87](59/122): Loss: 0.0002\n","===> Epoch[87](60/122): Loss: 0.0002\n","===> Epoch[87](61/122): Loss: 0.0002\n","===> Epoch[87](62/122): Loss: 0.0002\n","===> Epoch[87](63/122): Loss: 0.0002\n","===> Epoch[87](64/122): Loss: 0.0002\n","===> Epoch[87](65/122): Loss: 0.0002\n","===> Epoch[87](66/122): Loss: 0.0002\n","===> Epoch[87](67/122): Loss: 0.0002\n","===> Epoch[87](68/122): Loss: 0.0002\n","===> Epoch[87](69/122): Loss: 0.0002\n","===> Epoch[87](70/122): Loss: 0.0002\n","===> Epoch[87](71/122): Loss: 0.0002\n","===> Epoch[87](72/122): Loss: 0.0002\n","===> Epoch[87](73/122): Loss: 0.0002\n","===> Epoch[87](74/122): Loss: 0.0002\n","===> Epoch[87](75/122): Loss: 0.0002\n","===> Epoch[87](76/122): Loss: 0.0002\n","===> Epoch[87](77/122): Loss: 0.0002\n","===> Epoch[87](78/122): Loss: 0.0002\n","===> Epoch[87](79/122): Loss: 0.0002\n","===> Epoch[87](80/122): Loss: 0.0002\n","===> Epoch[87](81/122): Loss: 0.0002\n","===> Epoch[87](82/122): Loss: 0.0002\n","===> Epoch[87](83/122): Loss: 0.0002\n","===> Epoch[87](84/122): Loss: 0.0002\n","===> Epoch[87](85/122): Loss: 0.0002\n","===> Epoch[87](86/122): Loss: 0.0002\n","===> Epoch[87](87/122): Loss: 0.0002\n","===> Epoch[87](88/122): Loss: 0.0002\n","===> Epoch[87](89/122): Loss: 0.0002\n","===> Epoch[87](90/122): Loss: 0.0002\n","===> Epoch[87](91/122): Loss: 0.0002\n","===> Epoch[87](92/122): Loss: 0.0002\n","===> Epoch[87](93/122): Loss: 0.0003\n","===> Epoch[87](94/122): Loss: 0.0002\n","===> Epoch[87](95/122): Loss: 0.0002\n","===> Epoch[87](96/122): Loss: 0.0002\n","===> Epoch[87](97/122): Loss: 0.0002\n","===> Epoch[87](98/122): Loss: 0.0002\n","===> Epoch[87](99/122): Loss: 0.0002\n","===> Epoch[87](100/122): Loss: 0.0002\n","===> Epoch[87](101/122): Loss: 0.0002\n","===> Epoch[87](102/122): Loss: 0.0002\n","===> Epoch[87](103/122): Loss: 0.0002\n","===> Epoch[87](104/122): Loss: 0.0002\n","===> Epoch[87](105/122): Loss: 0.0002\n","===> Epoch[87](106/122): Loss: 0.0002\n","===> Epoch[87](107/122): Loss: 0.0002\n","===> Epoch[87](108/122): Loss: 0.0002\n","===> Epoch[87](109/122): Loss: 0.0002\n","===> Epoch[87](110/122): Loss: 0.0001\n","===> Epoch[87](111/122): Loss: 0.0002\n","===> Epoch[87](112/122): Loss: 0.0002\n","===> Epoch[87](113/122): Loss: 0.0002\n","===> Epoch[87](114/122): Loss: 0.0002\n","===> Epoch[87](115/122): Loss: 0.0002\n","===> Epoch[87](116/122): Loss: 0.0002\n","===> Epoch[87](117/122): Loss: 0.0002\n","===> Epoch[87](118/122): Loss: 0.0002\n","===> Epoch[87](119/122): Loss: 0.0002\n","===> Epoch[87](120/122): Loss: 0.0002\n","===> Epoch[87](121/122): Loss: 0.0002\n","===> Epoch[87](122/122): Loss: 0.0001\n","psnr_indiv=  [37.09627976869331] global_psnr=  37.09627976869331\n","===> Epoch 87 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.90508425680983}\n","train_per_mod {'flair': 37.09627976869331}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.3595 dB\n","===> Epoch[88](1/122): Loss: 0.0002\n","===> Epoch[88](2/122): Loss: 0.0002\n","===> Epoch[88](3/122): Loss: 0.0002\n","===> Epoch[88](4/122): Loss: 0.0002\n","===> Epoch[88](5/122): Loss: 0.0002\n","===> Epoch[88](6/122): Loss: 0.0002\n","===> Epoch[88](7/122): Loss: 0.0002\n","===> Epoch[88](8/122): Loss: 0.0002\n","===> Epoch[88](9/122): Loss: 0.0002\n","===> Epoch[88](10/122): Loss: 0.0002\n","===> Epoch[88](11/122): Loss: 0.0002\n","===> Epoch[88](12/122): Loss: 0.0002\n","===> Epoch[88](13/122): Loss: 0.0002\n","===> Epoch[88](14/122): Loss: 0.0002\n","===> Epoch[88](15/122): Loss: 0.0002\n","===> Epoch[88](16/122): Loss: 0.0002\n","===> Epoch[88](17/122): Loss: 0.0002\n","===> Epoch[88](18/122): Loss: 0.0002\n","===> Epoch[88](19/122): Loss: 0.0002\n","===> Epoch[88](20/122): Loss: 0.0002\n","===> Epoch[88](21/122): Loss: 0.0002\n","===> Epoch[88](22/122): Loss: 0.0002\n","===> Epoch[88](23/122): Loss: 0.0002\n","===> Epoch[88](24/122): Loss: 0.0002\n","===> Epoch[88](25/122): Loss: 0.0002\n","===> Epoch[88](26/122): Loss: 0.0002\n","===> Epoch[88](27/122): Loss: 0.0002\n","===> Epoch[88](28/122): Loss: 0.0002\n","===> Epoch[88](29/122): Loss: 0.0002\n","===> Epoch[88](30/122): Loss: 0.0002\n","===> Epoch[88](31/122): Loss: 0.0002\n","===> Epoch[88](32/122): Loss: 0.0002\n","===> Epoch[88](33/122): Loss: 0.0002\n","===> Epoch[88](34/122): Loss: 0.0002\n","===> Epoch[88](35/122): Loss: 0.0002\n","===> Epoch[88](36/122): Loss: 0.0002\n","===> Epoch[88](37/122): Loss: 0.0002\n","===> Epoch[88](38/122): Loss: 0.0002\n","===> Epoch[88](39/122): Loss: 0.0002\n","===> Epoch[88](40/122): Loss: 0.0002\n","===> Epoch[88](41/122): Loss: 0.0002\n","===> Epoch[88](42/122): Loss: 0.0002\n","===> Epoch[88](43/122): Loss: 0.0002\n","===> Epoch[88](44/122): Loss: 0.0002\n","===> Epoch[88](45/122): Loss: 0.0002\n","===> Epoch[88](46/122): Loss: 0.0002\n","===> Epoch[88](47/122): Loss: 0.0002\n","===> Epoch[88](48/122): Loss: 0.0002\n","===> Epoch[88](49/122): Loss: 0.0002\n","===> Epoch[88](50/122): Loss: 0.0002\n","===> Epoch[88](51/122): Loss: 0.0002\n","===> Epoch[88](52/122): Loss: 0.0002\n","===> Epoch[88](53/122): Loss: 0.0002\n","===> Epoch[88](54/122): Loss: 0.0002\n","===> Epoch[88](55/122): Loss: 0.0002\n","===> Epoch[88](56/122): Loss: 0.0002\n","===> Epoch[88](57/122): Loss: 0.0001\n","===> Epoch[88](58/122): Loss: 0.0002\n","===> Epoch[88](59/122): Loss: 0.0002\n","===> Epoch[88](60/122): Loss: 0.0002\n","===> Epoch[88](61/122): Loss: 0.0002\n","===> Epoch[88](62/122): Loss: 0.0003\n","===> Epoch[88](63/122): Loss: 0.0002\n","===> Epoch[88](64/122): Loss: 0.0002\n","===> Epoch[88](65/122): Loss: 0.0002\n","===> Epoch[88](66/122): Loss: 0.0002\n","===> Epoch[88](67/122): Loss: 0.0003\n","===> Epoch[88](68/122): Loss: 0.0002\n","===> Epoch[88](69/122): Loss: 0.0002\n","===> Epoch[88](70/122): Loss: 0.0002\n","===> Epoch[88](71/122): Loss: 0.0002\n","===> Epoch[88](72/122): Loss: 0.0002\n","===> Epoch[88](73/122): Loss: 0.0002\n","===> Epoch[88](74/122): Loss: 0.0002\n","===> Epoch[88](75/122): Loss: 0.0002\n","===> Epoch[88](76/122): Loss: 0.0002\n","===> Epoch[88](77/122): Loss: 0.0002\n","===> Epoch[88](78/122): Loss: 0.0002\n","===> Epoch[88](79/122): Loss: 0.0002\n","===> Epoch[88](80/122): Loss: 0.0002\n","===> Epoch[88](81/122): Loss: 0.0002\n","===> Epoch[88](82/122): Loss: 0.0002\n","===> Epoch[88](83/122): Loss: 0.0002\n","===> Epoch[88](84/122): Loss: 0.0002\n","===> Epoch[88](85/122): Loss: 0.0002\n","===> Epoch[88](86/122): Loss: 0.0002\n","===> Epoch[88](87/122): Loss: 0.0002\n","===> Epoch[88](88/122): Loss: 0.0002\n","===> Epoch[88](89/122): Loss: 0.0002\n","===> Epoch[88](90/122): Loss: 0.0002\n","===> Epoch[88](91/122): Loss: 0.0002\n","===> Epoch[88](92/122): Loss: 0.0002\n","===> Epoch[88](93/122): Loss: 0.0002\n","===> Epoch[88](94/122): Loss: 0.0002\n","===> Epoch[88](95/122): Loss: 0.0002\n","===> Epoch[88](96/122): Loss: 0.0002\n","===> Epoch[88](97/122): Loss: 0.0002\n","===> Epoch[88](98/122): Loss: 0.0002\n","===> Epoch[88](99/122): Loss: 0.0002\n","===> Epoch[88](100/122): Loss: 0.0002\n","===> Epoch[88](101/122): Loss: 0.0002\n","===> Epoch[88](102/122): Loss: 0.0002\n","===> Epoch[88](103/122): Loss: 0.0002\n","===> Epoch[88](104/122): Loss: 0.0002\n","===> Epoch[88](105/122): Loss: 0.0002\n","===> Epoch[88](106/122): Loss: 0.0002\n","===> Epoch[88](107/122): Loss: 0.0002\n","===> Epoch[88](108/122): Loss: 0.0002\n","===> Epoch[88](109/122): Loss: 0.0002\n","===> Epoch[88](110/122): Loss: 0.0002\n","===> Epoch[88](111/122): Loss: 0.0002\n","===> Epoch[88](112/122): Loss: 0.0002\n","===> Epoch[88](113/122): Loss: 0.0002\n","===> Epoch[88](114/122): Loss: 0.0002\n","===> Epoch[88](115/122): Loss: 0.0002\n","===> Epoch[88](116/122): Loss: 0.0002\n","===> Epoch[88](117/122): Loss: 0.0002\n","===> Epoch[88](118/122): Loss: 0.0002\n","===> Epoch[88](119/122): Loss: 0.0002\n","===> Epoch[88](120/122): Loss: 0.0002\n","===> Epoch[88](121/122): Loss: 0.0002\n","===> Epoch[88](122/122): Loss: 0.0002\n","psnr_indiv=  [37.093158082413034] global_psnr=  37.093158082413034\n","===> Epoch 88 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.90484694309336}\n","train_per_mod {'flair': 37.093158082413034}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.4110 dB\n","===> Epoch[89](1/122): Loss: 0.0002\n","===> Epoch[89](2/122): Loss: 0.0002\n","===> Epoch[89](3/122): Loss: 0.0002\n","===> Epoch[89](4/122): Loss: 0.0002\n","===> Epoch[89](5/122): Loss: 0.0002\n","===> Epoch[89](6/122): Loss: 0.0002\n","===> Epoch[89](7/122): Loss: 0.0002\n","===> Epoch[89](8/122): Loss: 0.0002\n","===> Epoch[89](9/122): Loss: 0.0002\n","===> Epoch[89](10/122): Loss: 0.0002\n","===> Epoch[89](11/122): Loss: 0.0002\n","===> Epoch[89](12/122): Loss: 0.0002\n","===> Epoch[89](13/122): Loss: 0.0002\n","===> Epoch[89](14/122): Loss: 0.0002\n","===> Epoch[89](15/122): Loss: 0.0002\n","===> Epoch[89](16/122): Loss: 0.0001\n","===> Epoch[89](17/122): Loss: 0.0001\n","===> Epoch[89](18/122): Loss: 0.0002\n","===> Epoch[89](19/122): Loss: 0.0002\n","===> Epoch[89](20/122): Loss: 0.0002\n","===> Epoch[89](21/122): Loss: 0.0002\n","===> Epoch[89](22/122): Loss: 0.0002\n","===> Epoch[89](23/122): Loss: 0.0002\n","===> Epoch[89](24/122): Loss: 0.0002\n","===> Epoch[89](25/122): Loss: 0.0002\n","===> Epoch[89](26/122): Loss: 0.0002\n","===> Epoch[89](27/122): Loss: 0.0002\n","===> Epoch[89](28/122): Loss: 0.0002\n","===> Epoch[89](29/122): Loss: 0.0002\n","===> Epoch[89](30/122): Loss: 0.0002\n","===> Epoch[89](31/122): Loss: 0.0002\n","===> Epoch[89](32/122): Loss: 0.0002\n","===> Epoch[89](33/122): Loss: 0.0002\n","===> Epoch[89](34/122): Loss: 0.0002\n","===> Epoch[89](35/122): Loss: 0.0002\n","===> Epoch[89](36/122): Loss: 0.0002\n","===> Epoch[89](37/122): Loss: 0.0002\n","===> Epoch[89](38/122): Loss: 0.0002\n","===> Epoch[89](39/122): Loss: 0.0002\n","===> Epoch[89](40/122): Loss: 0.0001\n","===> Epoch[89](41/122): Loss: 0.0002\n","===> Epoch[89](42/122): Loss: 0.0002\n","===> Epoch[89](43/122): Loss: 0.0002\n","===> Epoch[89](44/122): Loss: 0.0002\n","===> Epoch[89](45/122): Loss: 0.0002\n","===> Epoch[89](46/122): Loss: 0.0002\n","===> Epoch[89](47/122): Loss: 0.0002\n","===> Epoch[89](48/122): Loss: 0.0002\n","===> Epoch[89](49/122): Loss: 0.0002\n","===> Epoch[89](50/122): Loss: 0.0002\n","===> Epoch[89](51/122): Loss: 0.0002\n","===> Epoch[89](52/122): Loss: 0.0002\n","===> Epoch[89](53/122): Loss: 0.0002\n","===> Epoch[89](54/122): Loss: 0.0002\n","===> Epoch[89](55/122): Loss: 0.0002\n","===> Epoch[89](56/122): Loss: 0.0002\n","===> Epoch[89](57/122): Loss: 0.0002\n","===> Epoch[89](58/122): Loss: 0.0002\n","===> Epoch[89](59/122): Loss: 0.0002\n","===> Epoch[89](60/122): Loss: 0.0002\n","===> Epoch[89](61/122): Loss: 0.0002\n","===> Epoch[89](62/122): Loss: 0.0002\n","===> Epoch[89](63/122): Loss: 0.0002\n","===> Epoch[89](64/122): Loss: 0.0002\n","===> Epoch[89](65/122): Loss: 0.0002\n","===> Epoch[89](66/122): Loss: 0.0002\n","===> Epoch[89](67/122): Loss: 0.0002\n","===> Epoch[89](68/122): Loss: 0.0002\n","===> Epoch[89](69/122): Loss: 0.0002\n","===> Epoch[89](70/122): Loss: 0.0002\n","===> Epoch[89](71/122): Loss: 0.0002\n","===> Epoch[89](72/122): Loss: 0.0002\n","===> Epoch[89](73/122): Loss: 0.0002\n","===> Epoch[89](74/122): Loss: 0.0002\n","===> Epoch[89](75/122): Loss: 0.0001\n","===> Epoch[89](76/122): Loss: 0.0002\n","===> Epoch[89](77/122): Loss: 0.0002\n","===> Epoch[89](78/122): Loss: 0.0002\n","===> Epoch[89](79/122): Loss: 0.0002\n","===> Epoch[89](80/122): Loss: 0.0002\n","===> Epoch[89](81/122): Loss: 0.0002\n","===> Epoch[89](82/122): Loss: 0.0002\n","===> Epoch[89](83/122): Loss: 0.0002\n","===> Epoch[89](84/122): Loss: 0.0002\n","===> Epoch[89](85/122): Loss: 0.0002\n","===> Epoch[89](86/122): Loss: 0.0002\n","===> Epoch[89](87/122): Loss: 0.0002\n","===> Epoch[89](88/122): Loss: 0.0002\n","===> Epoch[89](89/122): Loss: 0.0002\n","===> Epoch[89](90/122): Loss: 0.0002\n","===> Epoch[89](91/122): Loss: 0.0002\n","===> Epoch[89](92/122): Loss: 0.0002\n","===> Epoch[89](93/122): Loss: 0.0002\n","===> Epoch[89](94/122): Loss: 0.0002\n","===> Epoch[89](95/122): Loss: 0.0001\n","===> Epoch[89](96/122): Loss: 0.0002\n","===> Epoch[89](97/122): Loss: 0.0002\n","===> Epoch[89](98/122): Loss: 0.0002\n","===> Epoch[89](99/122): Loss: 0.0002\n","===> Epoch[89](100/122): Loss: 0.0002\n","===> Epoch[89](101/122): Loss: 0.0002\n","===> Epoch[89](102/122): Loss: 0.0002\n","===> Epoch[89](103/122): Loss: 0.0002\n","===> Epoch[89](104/122): Loss: 0.0002\n","===> Epoch[89](105/122): Loss: 0.0002\n","===> Epoch[89](106/122): Loss: 0.0002\n","===> Epoch[89](107/122): Loss: 0.0002\n","===> Epoch[89](108/122): Loss: 0.0002\n","===> Epoch[89](109/122): Loss: 0.0002\n","===> Epoch[89](110/122): Loss: 0.0002\n","===> Epoch[89](111/122): Loss: 0.0002\n","===> Epoch[89](112/122): Loss: 0.0002\n","===> Epoch[89](113/122): Loss: 0.0002\n","===> Epoch[89](114/122): Loss: 0.0002\n","===> Epoch[89](115/122): Loss: 0.0003\n","===> Epoch[89](116/122): Loss: 0.0002\n","===> Epoch[89](117/122): Loss: 0.0002\n","===> Epoch[89](118/122): Loss: 0.0002\n","===> Epoch[89](119/122): Loss: 0.0002\n","===> Epoch[89](120/122): Loss: 0.0002\n","===> Epoch[89](121/122): Loss: 0.0002\n","===> Epoch[89](122/122): Loss: 0.0002\n","psnr_indiv=  [37.10654266462364] global_psnr=  37.10654266462364\n","===> Epoch 89 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.953659318736406}\n","train_per_mod {'flair': 37.10654266462364}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.3842 dB\n","===> Epoch[90](1/122): Loss: 0.0002\n","===> Epoch[90](2/122): Loss: 0.0002\n","===> Epoch[90](3/122): Loss: 0.0002\n","===> Epoch[90](4/122): Loss: 0.0002\n","===> Epoch[90](5/122): Loss: 0.0002\n","===> Epoch[90](6/122): Loss: 0.0002\n","===> Epoch[90](7/122): Loss: 0.0002\n","===> Epoch[90](8/122): Loss: 0.0002\n","===> Epoch[90](9/122): Loss: 0.0002\n","===> Epoch[90](10/122): Loss: 0.0002\n","===> Epoch[90](11/122): Loss: 0.0002\n","===> Epoch[90](12/122): Loss: 0.0002\n","===> Epoch[90](13/122): Loss: 0.0002\n","===> Epoch[90](14/122): Loss: 0.0002\n","===> Epoch[90](15/122): Loss: 0.0002\n","===> Epoch[90](16/122): Loss: 0.0002\n","===> Epoch[90](17/122): Loss: 0.0002\n","===> Epoch[90](18/122): Loss: 0.0002\n","===> Epoch[90](19/122): Loss: 0.0002\n","===> Epoch[90](20/122): Loss: 0.0001\n","===> Epoch[90](21/122): Loss: 0.0002\n","===> Epoch[90](22/122): Loss: 0.0002\n","===> Epoch[90](23/122): Loss: 0.0002\n","===> Epoch[90](24/122): Loss: 0.0002\n","===> Epoch[90](25/122): Loss: 0.0002\n","===> Epoch[90](26/122): Loss: 0.0002\n","===> Epoch[90](27/122): Loss: 0.0002\n","===> Epoch[90](28/122): Loss: 0.0002\n","===> Epoch[90](29/122): Loss: 0.0002\n","===> Epoch[90](30/122): Loss: 0.0002\n","===> Epoch[90](31/122): Loss: 0.0002\n","===> Epoch[90](32/122): Loss: 0.0002\n","===> Epoch[90](33/122): Loss: 0.0002\n","===> Epoch[90](34/122): Loss: 0.0002\n","===> Epoch[90](35/122): Loss: 0.0002\n","===> Epoch[90](36/122): Loss: 0.0002\n","===> Epoch[90](37/122): Loss: 0.0002\n","===> Epoch[90](38/122): Loss: 0.0002\n","===> Epoch[90](39/122): Loss: 0.0002\n","===> Epoch[90](40/122): Loss: 0.0002\n","===> Epoch[90](41/122): Loss: 0.0002\n","===> Epoch[90](42/122): Loss: 0.0002\n","===> Epoch[90](43/122): Loss: 0.0002\n","===> Epoch[90](44/122): Loss: 0.0002\n","===> Epoch[90](45/122): Loss: 0.0002\n","===> Epoch[90](46/122): Loss: 0.0002\n","===> Epoch[90](47/122): Loss: 0.0002\n","===> Epoch[90](48/122): Loss: 0.0002\n","===> Epoch[90](49/122): Loss: 0.0002\n","===> Epoch[90](50/122): Loss: 0.0002\n","===> Epoch[90](51/122): Loss: 0.0002\n","===> Epoch[90](52/122): Loss: 0.0002\n","===> Epoch[90](53/122): Loss: 0.0002\n","===> Epoch[90](54/122): Loss: 0.0002\n","===> Epoch[90](55/122): Loss: 0.0002\n","===> Epoch[90](56/122): Loss: 0.0002\n","===> Epoch[90](57/122): Loss: 0.0002\n","===> Epoch[90](58/122): Loss: 0.0002\n","===> Epoch[90](59/122): Loss: 0.0002\n","===> Epoch[90](60/122): Loss: 0.0001\n","===> Epoch[90](61/122): Loss: 0.0002\n","===> Epoch[90](62/122): Loss: 0.0002\n","===> Epoch[90](63/122): Loss: 0.0002\n","===> Epoch[90](64/122): Loss: 0.0002\n","===> Epoch[90](65/122): Loss: 0.0002\n","===> Epoch[90](66/122): Loss: 0.0002\n","===> Epoch[90](67/122): Loss: 0.0002\n","===> Epoch[90](68/122): Loss: 0.0002\n","===> Epoch[90](69/122): Loss: 0.0002\n","===> Epoch[90](70/122): Loss: 0.0002\n","===> Epoch[90](71/122): Loss: 0.0002\n","===> Epoch[90](72/122): Loss: 0.0002\n","===> Epoch[90](73/122): Loss: 0.0002\n","===> Epoch[90](74/122): Loss: 0.0002\n","===> Epoch[90](75/122): Loss: 0.0002\n","===> Epoch[90](76/122): Loss: 0.0002\n","===> Epoch[90](77/122): Loss: 0.0002\n","===> Epoch[90](78/122): Loss: 0.0002\n","===> Epoch[90](79/122): Loss: 0.0002\n","===> Epoch[90](80/122): Loss: 0.0002\n","===> Epoch[90](81/122): Loss: 0.0002\n","===> Epoch[90](82/122): Loss: 0.0002\n","===> Epoch[90](83/122): Loss: 0.0002\n","===> Epoch[90](84/122): Loss: 0.0002\n","===> Epoch[90](85/122): Loss: 0.0002\n","===> Epoch[90](86/122): Loss: 0.0002\n","===> Epoch[90](87/122): Loss: 0.0002\n","===> Epoch[90](88/122): Loss: 0.0002\n","===> Epoch[90](89/122): Loss: 0.0002\n","===> Epoch[90](90/122): Loss: 0.0002\n","===> Epoch[90](91/122): Loss: 0.0002\n","===> Epoch[90](92/122): Loss: 0.0002\n","===> Epoch[90](93/122): Loss: 0.0003\n","===> Epoch[90](94/122): Loss: 0.0002\n","===> Epoch[90](95/122): Loss: 0.0002\n","===> Epoch[90](96/122): Loss: 0.0002\n","===> Epoch[90](97/122): Loss: 0.0002\n","===> Epoch[90](98/122): Loss: 0.0002\n","===> Epoch[90](99/122): Loss: 0.0002\n","===> Epoch[90](100/122): Loss: 0.0002\n","===> Epoch[90](101/122): Loss: 0.0002\n","===> Epoch[90](102/122): Loss: 0.0002\n","===> Epoch[90](103/122): Loss: 0.0002\n","===> Epoch[90](104/122): Loss: 0.0001\n","===> Epoch[90](105/122): Loss: 0.0003\n","===> Epoch[90](106/122): Loss: 0.0003\n","===> Epoch[90](107/122): Loss: 0.0002\n","===> Epoch[90](108/122): Loss: 0.0003\n","===> Epoch[90](109/122): Loss: 0.0002\n","===> Epoch[90](110/122): Loss: 0.0003\n","===> Epoch[90](111/122): Loss: 0.0002\n","===> Epoch[90](112/122): Loss: 0.0003\n","===> Epoch[90](113/122): Loss: 0.0003\n","===> Epoch[90](114/122): Loss: 0.0003\n","===> Epoch[90](115/122): Loss: 0.0002\n","===> Epoch[90](116/122): Loss: 0.0002\n","===> Epoch[90](117/122): Loss: 0.0004\n","===> Epoch[90](118/122): Loss: 0.0004\n","===> Epoch[90](119/122): Loss: 0.0035\n","===> Epoch[90](120/122): Loss: 0.0033\n","===> Epoch[90](121/122): Loss: 0.0180\n","===> Epoch[90](122/122): Loss: 0.0051\n","psnr_indiv=  [33.55350209835965] global_psnr=  33.55350209835965\n","===> Epoch 90 Complete: Avg. Loss: 0.0004\n","test_per_mod {'flair': 40.93220701531716}\n","train_per_mod {'flair': 33.55350209835965}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 29.0623 dB\n","===> Epoch[91](1/122): Loss: 0.0016\n","===> Epoch[91](2/122): Loss: 0.0061\n","===> Epoch[91](3/122): Loss: 0.0019\n","===> Epoch[91](4/122): Loss: 0.0038\n","===> Epoch[91](5/122): Loss: 0.0026\n","===> Epoch[91](6/122): Loss: 0.0019\n","===> Epoch[91](7/122): Loss: 0.0014\n","===> Epoch[91](8/122): Loss: 0.0017\n","===> Epoch[91](9/122): Loss: 0.0019\n","===> Epoch[91](10/122): Loss: 0.0018\n","===> Epoch[91](11/122): Loss: 0.0015\n","===> Epoch[91](12/122): Loss: 0.0010\n","===> Epoch[91](13/122): Loss: 0.0009\n","===> Epoch[91](14/122): Loss: 0.0012\n","===> Epoch[91](15/122): Loss: 0.0011\n","===> Epoch[91](16/122): Loss: 0.0009\n","===> Epoch[91](17/122): Loss: 0.0007\n","===> Epoch[91](18/122): Loss: 0.0005\n","===> Epoch[91](19/122): Loss: 0.0006\n","===> Epoch[91](20/122): Loss: 0.0007\n","===> Epoch[91](21/122): Loss: 0.0007\n","===> Epoch[91](22/122): Loss: 0.0005\n","===> Epoch[91](23/122): Loss: 0.0004\n","===> Epoch[91](24/122): Loss: 0.0004\n","===> Epoch[91](25/122): Loss: 0.0005\n","===> Epoch[91](26/122): Loss: 0.0004\n","===> Epoch[91](27/122): Loss: 0.0004\n","===> Epoch[91](28/122): Loss: 0.0005\n","===> Epoch[91](29/122): Loss: 0.0004\n","===> Epoch[91](30/122): Loss: 0.0005\n","===> Epoch[91](31/122): Loss: 0.0004\n","===> Epoch[91](32/122): Loss: 0.0004\n","===> Epoch[91](33/122): Loss: 0.0004\n","===> Epoch[91](34/122): Loss: 0.0003\n","===> Epoch[91](35/122): Loss: 0.0003\n","===> Epoch[91](36/122): Loss: 0.0004\n","===> Epoch[91](37/122): Loss: 0.0003\n","===> Epoch[91](38/122): Loss: 0.0003\n","===> Epoch[91](39/122): Loss: 0.0003\n","===> Epoch[91](40/122): Loss: 0.0003\n","===> Epoch[91](41/122): Loss: 0.0004\n","===> Epoch[91](42/122): Loss: 0.0003\n","===> Epoch[91](43/122): Loss: 0.0004\n","===> Epoch[91](44/122): Loss: 0.0003\n","===> Epoch[91](45/122): Loss: 0.0003\n","===> Epoch[91](46/122): Loss: 0.0002\n","===> Epoch[91](47/122): Loss: 0.0003\n","===> Epoch[91](48/122): Loss: 0.0003\n","===> Epoch[91](49/122): Loss: 0.0003\n","===> Epoch[91](50/122): Loss: 0.0002\n","===> Epoch[91](51/122): Loss: 0.0002\n","===> Epoch[91](52/122): Loss: 0.0002\n","===> Epoch[91](53/122): Loss: 0.0002\n","===> Epoch[91](54/122): Loss: 0.0002\n","===> Epoch[91](55/122): Loss: 0.0003\n","===> Epoch[91](56/122): Loss: 0.0003\n","===> Epoch[91](57/122): Loss: 0.0003\n","===> Epoch[91](58/122): Loss: 0.0003\n","===> Epoch[91](59/122): Loss: 0.0003\n","===> Epoch[91](60/122): Loss: 0.0003\n","===> Epoch[91](61/122): Loss: 0.0003\n","===> Epoch[91](62/122): Loss: 0.0003\n","===> Epoch[91](63/122): Loss: 0.0003\n","===> Epoch[91](64/122): Loss: 0.0003\n","===> Epoch[91](65/122): Loss: 0.0003\n","===> Epoch[91](66/122): Loss: 0.0003\n","===> Epoch[91](67/122): Loss: 0.0003\n","===> Epoch[91](68/122): Loss: 0.0002\n","===> Epoch[91](69/122): Loss: 0.0003\n","===> Epoch[91](70/122): Loss: 0.0003\n","===> Epoch[91](71/122): Loss: 0.0003\n","===> Epoch[91](72/122): Loss: 0.0003\n","===> Epoch[91](73/122): Loss: 0.0003\n","===> Epoch[91](74/122): Loss: 0.0003\n","===> Epoch[91](75/122): Loss: 0.0002\n","===> Epoch[91](76/122): Loss: 0.0002\n","===> Epoch[91](77/122): Loss: 0.0003\n","===> Epoch[91](78/122): Loss: 0.0002\n","===> Epoch[91](79/122): Loss: 0.0003\n","===> Epoch[91](80/122): Loss: 0.0003\n","===> Epoch[91](81/122): Loss: 0.0003\n","===> Epoch[91](82/122): Loss: 0.0002\n","===> Epoch[91](83/122): Loss: 0.0002\n","===> Epoch[91](84/122): Loss: 0.0002\n","===> Epoch[91](85/122): Loss: 0.0003\n","===> Epoch[91](86/122): Loss: 0.0002\n","===> Epoch[91](87/122): Loss: 0.0002\n","===> Epoch[91](88/122): Loss: 0.0002\n","===> Epoch[91](89/122): Loss: 0.0002\n","===> Epoch[91](90/122): Loss: 0.0003\n","===> Epoch[91](91/122): Loss: 0.0002\n","===> Epoch[91](92/122): Loss: 0.0002\n","===> Epoch[91](93/122): Loss: 0.0002\n","===> Epoch[91](94/122): Loss: 0.0003\n","===> Epoch[91](95/122): Loss: 0.0003\n","===> Epoch[91](96/122): Loss: 0.0003\n","===> Epoch[91](97/122): Loss: 0.0003\n","===> Epoch[91](98/122): Loss: 0.0002\n","===> Epoch[91](99/122): Loss: 0.0003\n","===> Epoch[91](100/122): Loss: 0.0002\n","===> Epoch[91](101/122): Loss: 0.0002\n","===> Epoch[91](102/122): Loss: 0.0003\n","===> Epoch[91](103/122): Loss: 0.0003\n","===> Epoch[91](104/122): Loss: 0.0002\n","===> Epoch[91](105/122): Loss: 0.0003\n","===> Epoch[91](106/122): Loss: 0.0002\n","===> Epoch[91](107/122): Loss: 0.0003\n","===> Epoch[91](108/122): Loss: 0.0002\n","===> Epoch[91](109/122): Loss: 0.0003\n","===> Epoch[91](110/122): Loss: 0.0003\n","===> Epoch[91](111/122): Loss: 0.0003\n","===> Epoch[91](112/122): Loss: 0.0003\n","===> Epoch[91](113/122): Loss: 0.0003\n","===> Epoch[91](114/122): Loss: 0.0002\n","===> Epoch[91](115/122): Loss: 0.0003\n","===> Epoch[91](116/122): Loss: 0.0002\n","===> Epoch[91](117/122): Loss: 0.0002\n","===> Epoch[91](118/122): Loss: 0.0002\n","===> Epoch[91](119/122): Loss: 0.0003\n","===> Epoch[91](120/122): Loss: 0.0003\n","===> Epoch[91](121/122): Loss: 0.0002\n","===> Epoch[91](122/122): Loss: 0.0003\n","psnr_indiv=  [32.83236174021911] global_psnr=  32.83236174021911\n","===> Epoch 91 Complete: Avg. Loss: 0.0005\n","test_per_mod {'flair': 29.03781782292791}\n","train_per_mod {'flair': 32.83236174021911}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 40.3919 dB\n","===> Epoch[92](1/122): Loss: 0.0003\n","===> Epoch[92](2/122): Loss: 0.0002\n","===> Epoch[92](3/122): Loss: 0.0003\n","===> Epoch[92](4/122): Loss: 0.0002\n","===> Epoch[92](5/122): Loss: 0.0002\n","===> Epoch[92](6/122): Loss: 0.0003\n","===> Epoch[92](7/122): Loss: 0.0002\n","===> Epoch[92](8/122): Loss: 0.0003\n","===> Epoch[92](9/122): Loss: 0.0002\n","===> Epoch[92](10/122): Loss: 0.0003\n","===> Epoch[92](11/122): Loss: 0.0002\n","===> Epoch[92](12/122): Loss: 0.0002\n","===> Epoch[92](13/122): Loss: 0.0002\n","===> Epoch[92](14/122): Loss: 0.0003\n","===> Epoch[92](15/122): Loss: 0.0003\n","===> Epoch[92](16/122): Loss: 0.0003\n","===> Epoch[92](17/122): Loss: 0.0002\n","===> Epoch[92](18/122): Loss: 0.0002\n","===> Epoch[92](19/122): Loss: 0.0002\n","===> Epoch[92](20/122): Loss: 0.0002\n","===> Epoch[92](21/122): Loss: 0.0002\n","===> Epoch[92](22/122): Loss: 0.0002\n","===> Epoch[92](23/122): Loss: 0.0003\n","===> Epoch[92](24/122): Loss: 0.0002\n","===> Epoch[92](25/122): Loss: 0.0002\n","===> Epoch[92](26/122): Loss: 0.0003\n","===> Epoch[92](27/122): Loss: 0.0002\n","===> Epoch[92](28/122): Loss: 0.0002\n","===> Epoch[92](29/122): Loss: 0.0002\n","===> Epoch[92](30/122): Loss: 0.0002\n","===> Epoch[92](31/122): Loss: 0.0002\n","===> Epoch[92](32/122): Loss: 0.0002\n","===> Epoch[92](33/122): Loss: 0.0003\n","===> Epoch[92](34/122): Loss: 0.0002\n","===> Epoch[92](35/122): Loss: 0.0002\n","===> Epoch[92](36/122): Loss: 0.0002\n","===> Epoch[92](37/122): Loss: 0.0002\n","===> Epoch[92](38/122): Loss: 0.0002\n","===> Epoch[92](39/122): Loss: 0.0003\n","===> Epoch[92](40/122): Loss: 0.0002\n","===> Epoch[92](41/122): Loss: 0.0002\n","===> Epoch[92](42/122): Loss: 0.0002\n","===> Epoch[92](43/122): Loss: 0.0002\n","===> Epoch[92](44/122): Loss: 0.0002\n","===> Epoch[92](45/122): Loss: 0.0003\n","===> Epoch[92](46/122): Loss: 0.0002\n","===> Epoch[92](47/122): Loss: 0.0002\n","===> Epoch[92](48/122): Loss: 0.0003\n","===> Epoch[92](49/122): Loss: 0.0002\n","===> Epoch[92](50/122): Loss: 0.0002\n","===> Epoch[92](51/122): Loss: 0.0002\n","===> Epoch[92](52/122): Loss: 0.0003\n","===> Epoch[92](53/122): Loss: 0.0002\n","===> Epoch[92](54/122): Loss: 0.0002\n","===> Epoch[92](55/122): Loss: 0.0002\n","===> Epoch[92](56/122): Loss: 0.0002\n","===> Epoch[92](57/122): Loss: 0.0002\n","===> Epoch[92](58/122): Loss: 0.0002\n","===> Epoch[92](59/122): Loss: 0.0002\n","===> Epoch[92](60/122): Loss: 0.0003\n","===> Epoch[92](61/122): Loss: 0.0002\n","===> Epoch[92](62/122): Loss: 0.0003\n","===> Epoch[92](63/122): Loss: 0.0002\n","===> Epoch[92](64/122): Loss: 0.0002\n","===> Epoch[92](65/122): Loss: 0.0002\n","===> Epoch[92](66/122): Loss: 0.0002\n","===> Epoch[92](67/122): Loss: 0.0002\n","===> Epoch[92](68/122): Loss: 0.0002\n","===> Epoch[92](69/122): Loss: 0.0002\n","===> Epoch[92](70/122): Loss: 0.0002\n","===> Epoch[92](71/122): Loss: 0.0002\n","===> Epoch[92](72/122): Loss: 0.0003\n","===> Epoch[92](73/122): Loss: 0.0003\n","===> Epoch[92](74/122): Loss: 0.0002\n","===> Epoch[92](75/122): Loss: 0.0002\n","===> Epoch[92](76/122): Loss: 0.0002\n","===> Epoch[92](77/122): Loss: 0.0002\n","===> Epoch[92](78/122): Loss: 0.0002\n","===> Epoch[92](79/122): Loss: 0.0002\n","===> Epoch[92](80/122): Loss: 0.0002\n","===> Epoch[92](81/122): Loss: 0.0002\n","===> Epoch[92](82/122): Loss: 0.0002\n","===> Epoch[92](83/122): Loss: 0.0002\n","===> Epoch[92](84/122): Loss: 0.0002\n","===> Epoch[92](85/122): Loss: 0.0002\n","===> Epoch[92](86/122): Loss: 0.0002\n","===> Epoch[92](87/122): Loss: 0.0002\n","===> Epoch[92](88/122): Loss: 0.0002\n","===> Epoch[92](89/122): Loss: 0.0002\n","===> Epoch[92](90/122): Loss: 0.0002\n","===> Epoch[92](91/122): Loss: 0.0003\n","===> Epoch[92](92/122): Loss: 0.0002\n","===> Epoch[92](93/122): Loss: 0.0002\n","===> Epoch[92](94/122): Loss: 0.0002\n","===> Epoch[92](95/122): Loss: 0.0002\n","===> Epoch[92](96/122): Loss: 0.0002\n","===> Epoch[92](97/122): Loss: 0.0002\n","===> Epoch[92](98/122): Loss: 0.0002\n","===> Epoch[92](99/122): Loss: 0.0002\n","===> Epoch[92](100/122): Loss: 0.0002\n","===> Epoch[92](101/122): Loss: 0.0002\n","===> Epoch[92](102/122): Loss: 0.0002\n","===> Epoch[92](103/122): Loss: 0.0002\n","===> Epoch[92](104/122): Loss: 0.0002\n","===> Epoch[92](105/122): Loss: 0.0002\n","===> Epoch[92](106/122): Loss: 0.0002\n","===> Epoch[92](107/122): Loss: 0.0002\n","===> Epoch[92](108/122): Loss: 0.0002\n","===> Epoch[92](109/122): Loss: 0.0002\n","===> Epoch[92](110/122): Loss: 0.0003\n","===> Epoch[92](111/122): Loss: 0.0002\n","===> Epoch[92](112/122): Loss: 0.0002\n","===> Epoch[92](113/122): Loss: 0.0001\n","===> Epoch[92](114/122): Loss: 0.0002\n","===> Epoch[92](115/122): Loss: 0.0002\n","===> Epoch[92](116/122): Loss: 0.0002\n","===> Epoch[92](117/122): Loss: 0.0002\n","===> Epoch[92](118/122): Loss: 0.0002\n","===> Epoch[92](119/122): Loss: 0.0002\n","===> Epoch[92](120/122): Loss: 0.0003\n","===> Epoch[92](121/122): Loss: 0.0002\n","===> Epoch[92](122/122): Loss: 0.0003\n","psnr_indiv=  [36.468241233676274] global_psnr=  36.468241233676274\n","===> Epoch 92 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 39.92858576022373}\n","train_per_mod {'flair': 36.468241233676274}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.0368 dB\n","===> Epoch[93](1/122): Loss: 0.0003\n","===> Epoch[93](2/122): Loss: 0.0002\n","===> Epoch[93](3/122): Loss: 0.0002\n","===> Epoch[93](4/122): Loss: 0.0002\n","===> Epoch[93](5/122): Loss: 0.0002\n","===> Epoch[93](6/122): Loss: 0.0002\n","===> Epoch[93](7/122): Loss: 0.0002\n","===> Epoch[93](8/122): Loss: 0.0003\n","===> Epoch[93](9/122): Loss: 0.0002\n","===> Epoch[93](10/122): Loss: 0.0002\n","===> Epoch[93](11/122): Loss: 0.0002\n","===> Epoch[93](12/122): Loss: 0.0002\n","===> Epoch[93](13/122): Loss: 0.0002\n","===> Epoch[93](14/122): Loss: 0.0002\n","===> Epoch[93](15/122): Loss: 0.0003\n","===> Epoch[93](16/122): Loss: 0.0002\n","===> Epoch[93](17/122): Loss: 0.0002\n","===> Epoch[93](18/122): Loss: 0.0002\n","===> Epoch[93](19/122): Loss: 0.0002\n","===> Epoch[93](20/122): Loss: 0.0002\n","===> Epoch[93](21/122): Loss: 0.0002\n","===> Epoch[93](22/122): Loss: 0.0002\n","===> Epoch[93](23/122): Loss: 0.0002\n","===> Epoch[93](24/122): Loss: 0.0002\n","===> Epoch[93](25/122): Loss: 0.0002\n","===> Epoch[93](26/122): Loss: 0.0002\n","===> Epoch[93](27/122): Loss: 0.0002\n","===> Epoch[93](28/122): Loss: 0.0002\n","===> Epoch[93](29/122): Loss: 0.0002\n","===> Epoch[93](30/122): Loss: 0.0002\n","===> Epoch[93](31/122): Loss: 0.0002\n","===> Epoch[93](32/122): Loss: 0.0002\n","===> Epoch[93](33/122): Loss: 0.0002\n","===> Epoch[93](34/122): Loss: 0.0003\n","===> Epoch[93](35/122): Loss: 0.0003\n","===> Epoch[93](36/122): Loss: 0.0002\n","===> Epoch[93](37/122): Loss: 0.0002\n","===> Epoch[93](38/122): Loss: 0.0002\n","===> Epoch[93](39/122): Loss: 0.0002\n","===> Epoch[93](40/122): Loss: 0.0002\n","===> Epoch[93](41/122): Loss: 0.0002\n","===> Epoch[93](42/122): Loss: 0.0002\n","===> Epoch[93](43/122): Loss: 0.0002\n","===> Epoch[93](44/122): Loss: 0.0002\n","===> Epoch[93](45/122): Loss: 0.0002\n","===> Epoch[93](46/122): Loss: 0.0002\n","===> Epoch[93](47/122): Loss: 0.0002\n","===> Epoch[93](48/122): Loss: 0.0002\n","===> Epoch[93](49/122): Loss: 0.0002\n","===> Epoch[93](50/122): Loss: 0.0002\n","===> Epoch[93](51/122): Loss: 0.0002\n","===> Epoch[93](52/122): Loss: 0.0002\n","===> Epoch[93](53/122): Loss: 0.0002\n","===> Epoch[93](54/122): Loss: 0.0002\n","===> Epoch[93](55/122): Loss: 0.0002\n","===> Epoch[93](56/122): Loss: 0.0003\n","===> Epoch[93](57/122): Loss: 0.0002\n","===> Epoch[93](58/122): Loss: 0.0002\n","===> Epoch[93](59/122): Loss: 0.0002\n","===> Epoch[93](60/122): Loss: 0.0002\n","===> Epoch[93](61/122): Loss: 0.0002\n","===> Epoch[93](62/122): Loss: 0.0002\n","===> Epoch[93](63/122): Loss: 0.0002\n","===> Epoch[93](64/122): Loss: 0.0002\n","===> Epoch[93](65/122): Loss: 0.0002\n","===> Epoch[93](66/122): Loss: 0.0003\n","===> Epoch[93](67/122): Loss: 0.0002\n","===> Epoch[93](68/122): Loss: 0.0002\n","===> Epoch[93](69/122): Loss: 0.0002\n","===> Epoch[93](70/122): Loss: 0.0002\n","===> Epoch[93](71/122): Loss: 0.0002\n","===> Epoch[93](72/122): Loss: 0.0002\n","===> Epoch[93](73/122): Loss: 0.0002\n","===> Epoch[93](74/122): Loss: 0.0003\n","===> Epoch[93](75/122): Loss: 0.0002\n","===> Epoch[93](76/122): Loss: 0.0002\n","===> Epoch[93](77/122): Loss: 0.0002\n","===> Epoch[93](78/122): Loss: 0.0002\n","===> Epoch[93](79/122): Loss: 0.0002\n","===> Epoch[93](80/122): Loss: 0.0003\n","===> Epoch[93](81/122): Loss: 0.0002\n","===> Epoch[93](82/122): Loss: 0.0001\n","===> Epoch[93](83/122): Loss: 0.0002\n","===> Epoch[93](84/122): Loss: 0.0002\n","===> Epoch[93](85/122): Loss: 0.0002\n","===> Epoch[93](86/122): Loss: 0.0002\n","===> Epoch[93](87/122): Loss: 0.0002\n","===> Epoch[93](88/122): Loss: 0.0002\n","===> Epoch[93](89/122): Loss: 0.0002\n","===> Epoch[93](90/122): Loss: 0.0002\n","===> Epoch[93](91/122): Loss: 0.0002\n","===> Epoch[93](92/122): Loss: 0.0002\n","===> Epoch[93](93/122): Loss: 0.0002\n","===> Epoch[93](94/122): Loss: 0.0002\n","===> Epoch[93](95/122): Loss: 0.0002\n","===> Epoch[93](96/122): Loss: 0.0002\n","===> Epoch[93](97/122): Loss: 0.0002\n","===> Epoch[93](98/122): Loss: 0.0002\n","===> Epoch[93](99/122): Loss: 0.0002\n","===> Epoch[93](100/122): Loss: 0.0002\n","===> Epoch[93](101/122): Loss: 0.0001\n","===> Epoch[93](102/122): Loss: 0.0002\n","===> Epoch[93](103/122): Loss: 0.0002\n","===> Epoch[93](104/122): Loss: 0.0002\n","===> Epoch[93](105/122): Loss: 0.0002\n","===> Epoch[93](106/122): Loss: 0.0002\n","===> Epoch[93](107/122): Loss: 0.0002\n","===> Epoch[93](108/122): Loss: 0.0002\n","===> Epoch[93](109/122): Loss: 0.0002\n","===> Epoch[93](110/122): Loss: 0.0002\n","===> Epoch[93](111/122): Loss: 0.0002\n","===> Epoch[93](112/122): Loss: 0.0002\n","===> Epoch[93](113/122): Loss: 0.0002\n","===> Epoch[93](114/122): Loss: 0.0002\n","===> Epoch[93](115/122): Loss: 0.0002\n","===> Epoch[93](116/122): Loss: 0.0002\n","===> Epoch[93](117/122): Loss: 0.0002\n","===> Epoch[93](118/122): Loss: 0.0002\n","===> Epoch[93](119/122): Loss: 0.0002\n","===> Epoch[93](120/122): Loss: 0.0002\n","===> Epoch[93](121/122): Loss: 0.0002\n","===> Epoch[93](122/122): Loss: 0.0002\n","psnr_indiv=  [36.783280488894825] global_psnr=  36.783280488894825\n","===> Epoch 93 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.56410474037286}\n","train_per_mod {'flair': 36.783280488894825}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.2048 dB\n","===> Epoch[94](1/122): Loss: 0.0002\n","===> Epoch[94](2/122): Loss: 0.0002\n","===> Epoch[94](3/122): Loss: 0.0002\n","===> Epoch[94](4/122): Loss: 0.0002\n","===> Epoch[94](5/122): Loss: 0.0003\n","===> Epoch[94](6/122): Loss: 0.0002\n","===> Epoch[94](7/122): Loss: 0.0002\n","===> Epoch[94](8/122): Loss: 0.0002\n","===> Epoch[94](9/122): Loss: 0.0002\n","===> Epoch[94](10/122): Loss: 0.0002\n","===> Epoch[94](11/122): Loss: 0.0002\n","===> Epoch[94](12/122): Loss: 0.0002\n","===> Epoch[94](13/122): Loss: 0.0002\n","===> Epoch[94](14/122): Loss: 0.0002\n","===> Epoch[94](15/122): Loss: 0.0002\n","===> Epoch[94](16/122): Loss: 0.0002\n","===> Epoch[94](17/122): Loss: 0.0002\n","===> Epoch[94](18/122): Loss: 0.0003\n","===> Epoch[94](19/122): Loss: 0.0002\n","===> Epoch[94](20/122): Loss: 0.0002\n","===> Epoch[94](21/122): Loss: 0.0002\n","===> Epoch[94](22/122): Loss: 0.0002\n","===> Epoch[94](23/122): Loss: 0.0002\n","===> Epoch[94](24/122): Loss: 0.0002\n","===> Epoch[94](25/122): Loss: 0.0003\n","===> Epoch[94](26/122): Loss: 0.0002\n","===> Epoch[94](27/122): Loss: 0.0001\n","===> Epoch[94](28/122): Loss: 0.0002\n","===> Epoch[94](29/122): Loss: 0.0003\n","===> Epoch[94](30/122): Loss: 0.0002\n","===> Epoch[94](31/122): Loss: 0.0002\n","===> Epoch[94](32/122): Loss: 0.0002\n","===> Epoch[94](33/122): Loss: 0.0002\n","===> Epoch[94](34/122): Loss: 0.0002\n","===> Epoch[94](35/122): Loss: 0.0002\n","===> Epoch[94](36/122): Loss: 0.0002\n","===> Epoch[94](37/122): Loss: 0.0002\n","===> Epoch[94](38/122): Loss: 0.0002\n","===> Epoch[94](39/122): Loss: 0.0002\n","===> Epoch[94](40/122): Loss: 0.0002\n","===> Epoch[94](41/122): Loss: 0.0002\n","===> Epoch[94](42/122): Loss: 0.0002\n","===> Epoch[94](43/122): Loss: 0.0002\n","===> Epoch[94](44/122): Loss: 0.0002\n","===> Epoch[94](45/122): Loss: 0.0002\n","===> Epoch[94](46/122): Loss: 0.0003\n","===> Epoch[94](47/122): Loss: 0.0002\n","===> Epoch[94](48/122): Loss: 0.0002\n","===> Epoch[94](49/122): Loss: 0.0002\n","===> Epoch[94](50/122): Loss: 0.0002\n","===> Epoch[94](51/122): Loss: 0.0002\n","===> Epoch[94](52/122): Loss: 0.0002\n","===> Epoch[94](53/122): Loss: 0.0002\n","===> Epoch[94](54/122): Loss: 0.0002\n","===> Epoch[94](55/122): Loss: 0.0002\n","===> Epoch[94](56/122): Loss: 0.0002\n","===> Epoch[94](57/122): Loss: 0.0002\n","===> Epoch[94](58/122): Loss: 0.0002\n","===> Epoch[94](59/122): Loss: 0.0002\n","===> Epoch[94](60/122): Loss: 0.0002\n","===> Epoch[94](61/122): Loss: 0.0002\n","===> Epoch[94](62/122): Loss: 0.0002\n","===> Epoch[94](63/122): Loss: 0.0002\n","===> Epoch[94](64/122): Loss: 0.0002\n","===> Epoch[94](65/122): Loss: 0.0002\n","===> Epoch[94](66/122): Loss: 0.0002\n","===> Epoch[94](67/122): Loss: 0.0002\n","===> Epoch[94](68/122): Loss: 0.0002\n","===> Epoch[94](69/122): Loss: 0.0001\n","===> Epoch[94](70/122): Loss: 0.0002\n","===> Epoch[94](71/122): Loss: 0.0002\n","===> Epoch[94](72/122): Loss: 0.0002\n","===> Epoch[94](73/122): Loss: 0.0002\n","===> Epoch[94](74/122): Loss: 0.0002\n","===> Epoch[94](75/122): Loss: 0.0002\n","===> Epoch[94](76/122): Loss: 0.0002\n","===> Epoch[94](77/122): Loss: 0.0002\n","===> Epoch[94](78/122): Loss: 0.0002\n","===> Epoch[94](79/122): Loss: 0.0002\n","===> Epoch[94](80/122): Loss: 0.0002\n","===> Epoch[94](81/122): Loss: 0.0002\n","===> Epoch[94](82/122): Loss: 0.0002\n","===> Epoch[94](83/122): Loss: 0.0002\n","===> Epoch[94](84/122): Loss: 0.0002\n","===> Epoch[94](85/122): Loss: 0.0002\n","===> Epoch[94](86/122): Loss: 0.0002\n","===> Epoch[94](87/122): Loss: 0.0002\n","===> Epoch[94](88/122): Loss: 0.0002\n","===> Epoch[94](89/122): Loss: 0.0002\n","===> Epoch[94](90/122): Loss: 0.0002\n","===> Epoch[94](91/122): Loss: 0.0001\n","===> Epoch[94](92/122): Loss: 0.0002\n","===> Epoch[94](93/122): Loss: 0.0002\n","===> Epoch[94](94/122): Loss: 0.0002\n","===> Epoch[94](95/122): Loss: 0.0002\n","===> Epoch[94](96/122): Loss: 0.0002\n","===> Epoch[94](97/122): Loss: 0.0003\n","===> Epoch[94](98/122): Loss: 0.0002\n","===> Epoch[94](99/122): Loss: 0.0003\n","===> Epoch[94](100/122): Loss: 0.0002\n","===> Epoch[94](101/122): Loss: 0.0002\n","===> Epoch[94](102/122): Loss: 0.0002\n","===> Epoch[94](103/122): Loss: 0.0002\n","===> Epoch[94](104/122): Loss: 0.0002\n","===> Epoch[94](105/122): Loss: 0.0002\n","===> Epoch[94](106/122): Loss: 0.0002\n","===> Epoch[94](107/122): Loss: 0.0002\n","===> Epoch[94](108/122): Loss: 0.0002\n","===> Epoch[94](109/122): Loss: 0.0002\n","===> Epoch[94](110/122): Loss: 0.0002\n","===> Epoch[94](111/122): Loss: 0.0002\n","===> Epoch[94](112/122): Loss: 0.0002\n","===> Epoch[94](113/122): Loss: 0.0002\n","===> Epoch[94](114/122): Loss: 0.0002\n","===> Epoch[94](115/122): Loss: 0.0002\n","===> Epoch[94](116/122): Loss: 0.0002\n","===> Epoch[94](117/122): Loss: 0.0002\n","===> Epoch[94](118/122): Loss: 0.0002\n","===> Epoch[94](119/122): Loss: 0.0002\n","===> Epoch[94](120/122): Loss: 0.0002\n","===> Epoch[94](121/122): Loss: 0.0001\n","===> Epoch[94](122/122): Loss: 0.0002\n","psnr_indiv=  [36.8957963275503] global_psnr=  36.8957963275503\n","===> Epoch 94 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.73123223979411}\n","train_per_mod {'flair': 36.8957963275503}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.2863 dB\n","===> Epoch[95](1/122): Loss: 0.0002\n","===> Epoch[95](2/122): Loss: 0.0002\n","===> Epoch[95](3/122): Loss: 0.0002\n","===> Epoch[95](4/122): Loss: 0.0002\n","===> Epoch[95](5/122): Loss: 0.0002\n","===> Epoch[95](6/122): Loss: 0.0002\n","===> Epoch[95](7/122): Loss: 0.0003\n","===> Epoch[95](8/122): Loss: 0.0002\n","===> Epoch[95](9/122): Loss: 0.0002\n","===> Epoch[95](10/122): Loss: 0.0002\n","===> Epoch[95](11/122): Loss: 0.0002\n","===> Epoch[95](12/122): Loss: 0.0002\n","===> Epoch[95](13/122): Loss: 0.0002\n","===> Epoch[95](14/122): Loss: 0.0002\n","===> Epoch[95](15/122): Loss: 0.0002\n","===> Epoch[95](16/122): Loss: 0.0003\n","===> Epoch[95](17/122): Loss: 0.0002\n","===> Epoch[95](18/122): Loss: 0.0002\n","===> Epoch[95](19/122): Loss: 0.0002\n","===> Epoch[95](20/122): Loss: 0.0001\n","===> Epoch[95](21/122): Loss: 0.0002\n","===> Epoch[95](22/122): Loss: 0.0002\n","===> Epoch[95](23/122): Loss: 0.0002\n","===> Epoch[95](24/122): Loss: 0.0002\n","===> Epoch[95](25/122): Loss: 0.0002\n","===> Epoch[95](26/122): Loss: 0.0002\n","===> Epoch[95](27/122): Loss: 0.0002\n","===> Epoch[95](28/122): Loss: 0.0002\n","===> Epoch[95](29/122): Loss: 0.0002\n","===> Epoch[95](30/122): Loss: 0.0002\n","===> Epoch[95](31/122): Loss: 0.0002\n","===> Epoch[95](32/122): Loss: 0.0003\n","===> Epoch[95](33/122): Loss: 0.0002\n","===> Epoch[95](34/122): Loss: 0.0002\n","===> Epoch[95](35/122): Loss: 0.0002\n","===> Epoch[95](36/122): Loss: 0.0002\n","===> Epoch[95](37/122): Loss: 0.0002\n","===> Epoch[95](38/122): Loss: 0.0002\n","===> Epoch[95](39/122): Loss: 0.0002\n","===> Epoch[95](40/122): Loss: 0.0002\n","===> Epoch[95](41/122): Loss: 0.0002\n","===> Epoch[95](42/122): Loss: 0.0002\n","===> Epoch[95](43/122): Loss: 0.0002\n","===> Epoch[95](44/122): Loss: 0.0002\n","===> Epoch[95](45/122): Loss: 0.0002\n","===> Epoch[95](46/122): Loss: 0.0002\n","===> Epoch[95](47/122): Loss: 0.0002\n","===> Epoch[95](48/122): Loss: 0.0002\n","===> Epoch[95](49/122): Loss: 0.0002\n","===> Epoch[95](50/122): Loss: 0.0002\n","===> Epoch[95](51/122): Loss: 0.0002\n","===> Epoch[95](52/122): Loss: 0.0002\n","===> Epoch[95](53/122): Loss: 0.0002\n","===> Epoch[95](54/122): Loss: 0.0002\n","===> Epoch[95](55/122): Loss: 0.0002\n","===> Epoch[95](56/122): Loss: 0.0002\n","===> Epoch[95](57/122): Loss: 0.0002\n","===> Epoch[95](58/122): Loss: 0.0002\n","===> Epoch[95](59/122): Loss: 0.0002\n","===> Epoch[95](60/122): Loss: 0.0002\n","===> Epoch[95](61/122): Loss: 0.0001\n","===> Epoch[95](62/122): Loss: 0.0002\n","===> Epoch[95](63/122): Loss: 0.0002\n","===> Epoch[95](64/122): Loss: 0.0002\n","===> Epoch[95](65/122): Loss: 0.0002\n","===> Epoch[95](66/122): Loss: 0.0002\n","===> Epoch[95](67/122): Loss: 0.0002\n","===> Epoch[95](68/122): Loss: 0.0002\n","===> Epoch[95](69/122): Loss: 0.0002\n","===> Epoch[95](70/122): Loss: 0.0002\n","===> Epoch[95](71/122): Loss: 0.0002\n","===> Epoch[95](72/122): Loss: 0.0002\n","===> Epoch[95](73/122): Loss: 0.0002\n","===> Epoch[95](74/122): Loss: 0.0002\n","===> Epoch[95](75/122): Loss: 0.0002\n","===> Epoch[95](76/122): Loss: 0.0002\n","===> Epoch[95](77/122): Loss: 0.0002\n","===> Epoch[95](78/122): Loss: 0.0002\n","===> Epoch[95](79/122): Loss: 0.0002\n","===> Epoch[95](80/122): Loss: 0.0002\n","===> Epoch[95](81/122): Loss: 0.0002\n","===> Epoch[95](82/122): Loss: 0.0002\n","===> Epoch[95](83/122): Loss: 0.0002\n","===> Epoch[95](84/122): Loss: 0.0002\n","===> Epoch[95](85/122): Loss: 0.0002\n","===> Epoch[95](86/122): Loss: 0.0002\n","===> Epoch[95](87/122): Loss: 0.0002\n","===> Epoch[95](88/122): Loss: 0.0002\n","===> Epoch[95](89/122): Loss: 0.0002\n","===> Epoch[95](90/122): Loss: 0.0002\n","===> Epoch[95](91/122): Loss: 0.0002\n","===> Epoch[95](92/122): Loss: 0.0002\n","===> Epoch[95](93/122): Loss: 0.0002\n","===> Epoch[95](94/122): Loss: 0.0002\n","===> Epoch[95](95/122): Loss: 0.0002\n","===> Epoch[95](96/122): Loss: 0.0002\n","===> Epoch[95](97/122): Loss: 0.0002\n","===> Epoch[95](98/122): Loss: 0.0003\n","===> Epoch[95](99/122): Loss: 0.0002\n","===> Epoch[95](100/122): Loss: 0.0002\n","===> Epoch[95](101/122): Loss: 0.0002\n","===> Epoch[95](102/122): Loss: 0.0002\n","===> Epoch[95](103/122): Loss: 0.0002\n","===> Epoch[95](104/122): Loss: 0.0002\n","===> Epoch[95](105/122): Loss: 0.0002\n","===> Epoch[95](106/122): Loss: 0.0002\n","===> Epoch[95](107/122): Loss: 0.0002\n","===> Epoch[95](108/122): Loss: 0.0002\n","===> Epoch[95](109/122): Loss: 0.0002\n","===> Epoch[95](110/122): Loss: 0.0002\n","===> Epoch[95](111/122): Loss: 0.0002\n","===> Epoch[95](112/122): Loss: 0.0002\n","===> Epoch[95](113/122): Loss: 0.0002\n","===> Epoch[95](114/122): Loss: 0.0002\n","===> Epoch[95](115/122): Loss: 0.0002\n","===> Epoch[95](116/122): Loss: 0.0002\n","===> Epoch[95](117/122): Loss: 0.0002\n","===> Epoch[95](118/122): Loss: 0.0002\n","===> Epoch[95](119/122): Loss: 0.0002\n","===> Epoch[95](120/122): Loss: 0.0002\n","===> Epoch[95](121/122): Loss: 0.0002\n","===> Epoch[95](122/122): Loss: 0.0002\n","psnr_indiv=  [36.960997685444966] global_psnr=  36.960997685444966\n","===> Epoch 95 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.8177000447354}\n","train_per_mod {'flair': 36.960997685444966}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.3337 dB\n","===> Epoch[96](1/122): Loss: 0.0002\n","===> Epoch[96](2/122): Loss: 0.0002\n","===> Epoch[96](3/122): Loss: 0.0002\n","===> Epoch[96](4/122): Loss: 0.0002\n","===> Epoch[96](5/122): Loss: 0.0002\n","===> Epoch[96](6/122): Loss: 0.0002\n","===> Epoch[96](7/122): Loss: 0.0002\n","===> Epoch[96](8/122): Loss: 0.0002\n","===> Epoch[96](9/122): Loss: 0.0002\n","===> Epoch[96](10/122): Loss: 0.0002\n","===> Epoch[96](11/122): Loss: 0.0002\n","===> Epoch[96](12/122): Loss: 0.0002\n","===> Epoch[96](13/122): Loss: 0.0002\n","===> Epoch[96](14/122): Loss: 0.0002\n","===> Epoch[96](15/122): Loss: 0.0002\n","===> Epoch[96](16/122): Loss: 0.0002\n","===> Epoch[96](17/122): Loss: 0.0002\n","===> Epoch[96](18/122): Loss: 0.0002\n","===> Epoch[96](19/122): Loss: 0.0002\n","===> Epoch[96](20/122): Loss: 0.0002\n","===> Epoch[96](21/122): Loss: 0.0002\n","===> Epoch[96](22/122): Loss: 0.0002\n","===> Epoch[96](23/122): Loss: 0.0002\n","===> Epoch[96](24/122): Loss: 0.0002\n","===> Epoch[96](25/122): Loss: 0.0002\n","===> Epoch[96](26/122): Loss: 0.0002\n","===> Epoch[96](27/122): Loss: 0.0002\n","===> Epoch[96](28/122): Loss: 0.0002\n","===> Epoch[96](29/122): Loss: 0.0002\n","===> Epoch[96](30/122): Loss: 0.0002\n","===> Epoch[96](31/122): Loss: 0.0002\n","===> Epoch[96](32/122): Loss: 0.0002\n","===> Epoch[96](33/122): Loss: 0.0002\n","===> Epoch[96](34/122): Loss: 0.0002\n","===> Epoch[96](35/122): Loss: 0.0002\n","===> Epoch[96](36/122): Loss: 0.0002\n","===> Epoch[96](37/122): Loss: 0.0002\n","===> Epoch[96](38/122): Loss: 0.0002\n","===> Epoch[96](39/122): Loss: 0.0002\n","===> Epoch[96](40/122): Loss: 0.0002\n","===> Epoch[96](41/122): Loss: 0.0002\n","===> Epoch[96](42/122): Loss: 0.0002\n","===> Epoch[96](43/122): Loss: 0.0002\n","===> Epoch[96](44/122): Loss: 0.0002\n","===> Epoch[96](45/122): Loss: 0.0002\n","===> Epoch[96](46/122): Loss: 0.0002\n","===> Epoch[96](47/122): Loss: 0.0002\n","===> Epoch[96](48/122): Loss: 0.0002\n","===> Epoch[96](49/122): Loss: 0.0002\n","===> Epoch[96](50/122): Loss: 0.0002\n","===> Epoch[96](51/122): Loss: 0.0002\n","===> Epoch[96](52/122): Loss: 0.0002\n","===> Epoch[96](53/122): Loss: 0.0002\n","===> Epoch[96](54/122): Loss: 0.0002\n","===> Epoch[96](55/122): Loss: 0.0002\n","===> Epoch[96](56/122): Loss: 0.0002\n","===> Epoch[96](57/122): Loss: 0.0002\n","===> Epoch[96](58/122): Loss: 0.0002\n","===> Epoch[96](59/122): Loss: 0.0002\n","===> Epoch[96](60/122): Loss: 0.0002\n","===> Epoch[96](61/122): Loss: 0.0002\n","===> Epoch[96](62/122): Loss: 0.0001\n","===> Epoch[96](63/122): Loss: 0.0001\n","===> Epoch[96](64/122): Loss: 0.0002\n","===> Epoch[96](65/122): Loss: 0.0002\n","===> Epoch[96](66/122): Loss: 0.0002\n","===> Epoch[96](67/122): Loss: 0.0002\n","===> Epoch[96](68/122): Loss: 0.0002\n","===> Epoch[96](69/122): Loss: 0.0002\n","===> Epoch[96](70/122): Loss: 0.0002\n","===> Epoch[96](71/122): Loss: 0.0002\n","===> Epoch[96](72/122): Loss: 0.0002\n","===> Epoch[96](73/122): Loss: 0.0002\n","===> Epoch[96](74/122): Loss: 0.0003\n","===> Epoch[96](75/122): Loss: 0.0002\n","===> Epoch[96](76/122): Loss: 0.0002\n","===> Epoch[96](77/122): Loss: 0.0002\n","===> Epoch[96](78/122): Loss: 0.0002\n","===> Epoch[96](79/122): Loss: 0.0002\n","===> Epoch[96](80/122): Loss: 0.0002\n","===> Epoch[96](81/122): Loss: 0.0002\n","===> Epoch[96](82/122): Loss: 0.0002\n","===> Epoch[96](83/122): Loss: 0.0002\n","===> Epoch[96](84/122): Loss: 0.0002\n","===> Epoch[96](85/122): Loss: 0.0002\n","===> Epoch[96](86/122): Loss: 0.0002\n","===> Epoch[96](87/122): Loss: 0.0002\n","===> Epoch[96](88/122): Loss: 0.0002\n","===> Epoch[96](89/122): Loss: 0.0002\n","===> Epoch[96](90/122): Loss: 0.0002\n","===> Epoch[96](91/122): Loss: 0.0002\n","===> Epoch[96](92/122): Loss: 0.0002\n","===> Epoch[96](93/122): Loss: 0.0002\n","===> Epoch[96](94/122): Loss: 0.0002\n","===> Epoch[96](95/122): Loss: 0.0002\n","===> Epoch[96](96/122): Loss: 0.0002\n","===> Epoch[96](97/122): Loss: 0.0002\n","===> Epoch[96](98/122): Loss: 0.0002\n","===> Epoch[96](99/122): Loss: 0.0002\n","===> Epoch[96](100/122): Loss: 0.0002\n","===> Epoch[96](101/122): Loss: 0.0002\n","===> Epoch[96](102/122): Loss: 0.0002\n","===> Epoch[96](103/122): Loss: 0.0002\n","===> Epoch[96](104/122): Loss: 0.0002\n","===> Epoch[96](105/122): Loss: 0.0002\n","===> Epoch[96](106/122): Loss: 0.0002\n","===> Epoch[96](107/122): Loss: 0.0002\n","===> Epoch[96](108/122): Loss: 0.0002\n","===> Epoch[96](109/122): Loss: 0.0002\n","===> Epoch[96](110/122): Loss: 0.0002\n","===> Epoch[96](111/122): Loss: 0.0002\n","===> Epoch[96](112/122): Loss: 0.0002\n","===> Epoch[96](113/122): Loss: 0.0002\n","===> Epoch[96](114/122): Loss: 0.0002\n","===> Epoch[96](115/122): Loss: 0.0002\n","===> Epoch[96](116/122): Loss: 0.0002\n","===> Epoch[96](117/122): Loss: 0.0002\n","===> Epoch[96](118/122): Loss: 0.0002\n","===> Epoch[96](119/122): Loss: 0.0002\n","===> Epoch[96](120/122): Loss: 0.0002\n","===> Epoch[96](121/122): Loss: 0.0002\n","===> Epoch[96](122/122): Loss: 0.0002\n","psnr_indiv=  [37.003645821668606] global_psnr=  37.003645821668606\n","===> Epoch 96 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.86692718095283}\n","train_per_mod {'flair': 37.003645821668606}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.3390 dB\n","===> Epoch[97](1/122): Loss: 0.0002\n","===> Epoch[97](2/122): Loss: 0.0002\n","===> Epoch[97](3/122): Loss: 0.0003\n","===> Epoch[97](4/122): Loss: 0.0002\n","===> Epoch[97](5/122): Loss: 0.0002\n","===> Epoch[97](6/122): Loss: 0.0002\n","===> Epoch[97](7/122): Loss: 0.0002\n","===> Epoch[97](8/122): Loss: 0.0002\n","===> Epoch[97](9/122): Loss: 0.0002\n","===> Epoch[97](10/122): Loss: 0.0002\n","===> Epoch[97](11/122): Loss: 0.0002\n","===> Epoch[97](12/122): Loss: 0.0002\n","===> Epoch[97](13/122): Loss: 0.0002\n","===> Epoch[97](14/122): Loss: 0.0002\n","===> Epoch[97](15/122): Loss: 0.0002\n","===> Epoch[97](16/122): Loss: 0.0002\n","===> Epoch[97](17/122): Loss: 0.0002\n","===> Epoch[97](18/122): Loss: 0.0002\n","===> Epoch[97](19/122): Loss: 0.0002\n","===> Epoch[97](20/122): Loss: 0.0002\n","===> Epoch[97](21/122): Loss: 0.0002\n","===> Epoch[97](22/122): Loss: 0.0002\n","===> Epoch[97](23/122): Loss: 0.0002\n","===> Epoch[97](24/122): Loss: 0.0002\n","===> Epoch[97](25/122): Loss: 0.0002\n","===> Epoch[97](26/122): Loss: 0.0002\n","===> Epoch[97](27/122): Loss: 0.0002\n","===> Epoch[97](28/122): Loss: 0.0002\n","===> Epoch[97](29/122): Loss: 0.0003\n","===> Epoch[97](30/122): Loss: 0.0002\n","===> Epoch[97](31/122): Loss: 0.0002\n","===> Epoch[97](32/122): Loss: 0.0002\n","===> Epoch[97](33/122): Loss: 0.0002\n","===> Epoch[97](34/122): Loss: 0.0002\n","===> Epoch[97](35/122): Loss: 0.0002\n","===> Epoch[97](36/122): Loss: 0.0002\n","===> Epoch[97](37/122): Loss: 0.0002\n","===> Epoch[97](38/122): Loss: 0.0002\n","===> Epoch[97](39/122): Loss: 0.0002\n","===> Epoch[97](40/122): Loss: 0.0002\n","===> Epoch[97](41/122): Loss: 0.0003\n","===> Epoch[97](42/122): Loss: 0.0002\n","===> Epoch[97](43/122): Loss: 0.0002\n","===> Epoch[97](44/122): Loss: 0.0002\n","===> Epoch[97](45/122): Loss: 0.0002\n","===> Epoch[97](46/122): Loss: 0.0002\n","===> Epoch[97](47/122): Loss: 0.0002\n","===> Epoch[97](48/122): Loss: 0.0002\n","===> Epoch[97](49/122): Loss: 0.0002\n","===> Epoch[97](50/122): Loss: 0.0003\n","===> Epoch[97](51/122): Loss: 0.0002\n","===> Epoch[97](52/122): Loss: 0.0002\n","===> Epoch[97](53/122): Loss: 0.0002\n","===> Epoch[97](54/122): Loss: 0.0002\n","===> Epoch[97](55/122): Loss: 0.0002\n","===> Epoch[97](56/122): Loss: 0.0002\n","===> Epoch[97](57/122): Loss: 0.0002\n","===> Epoch[97](58/122): Loss: 0.0002\n","===> Epoch[97](59/122): Loss: 0.0002\n","===> Epoch[97](60/122): Loss: 0.0002\n","===> Epoch[97](61/122): Loss: 0.0002\n","===> Epoch[97](62/122): Loss: 0.0002\n","===> Epoch[97](63/122): Loss: 0.0002\n","===> Epoch[97](64/122): Loss: 0.0002\n","===> Epoch[97](65/122): Loss: 0.0002\n","===> Epoch[97](66/122): Loss: 0.0002\n","===> Epoch[97](67/122): Loss: 0.0002\n","===> Epoch[97](68/122): Loss: 0.0002\n","===> Epoch[97](69/122): Loss: 0.0002\n","===> Epoch[97](70/122): Loss: 0.0002\n","===> Epoch[97](71/122): Loss: 0.0002\n","===> Epoch[97](72/122): Loss: 0.0002\n","===> Epoch[97](73/122): Loss: 0.0002\n","===> Epoch[97](74/122): Loss: 0.0002\n","===> Epoch[97](75/122): Loss: 0.0002\n","===> Epoch[97](76/122): Loss: 0.0002\n","===> Epoch[97](77/122): Loss: 0.0002\n","===> Epoch[97](78/122): Loss: 0.0002\n","===> Epoch[97](79/122): Loss: 0.0002\n","===> Epoch[97](80/122): Loss: 0.0002\n","===> Epoch[97](81/122): Loss: 0.0002\n","===> Epoch[97](82/122): Loss: 0.0002\n","===> Epoch[97](83/122): Loss: 0.0002\n","===> Epoch[97](84/122): Loss: 0.0001\n","===> Epoch[97](85/122): Loss: 0.0001\n","===> Epoch[97](86/122): Loss: 0.0002\n","===> Epoch[97](87/122): Loss: 0.0002\n","===> Epoch[97](88/122): Loss: 0.0002\n","===> Epoch[97](89/122): Loss: 0.0002\n","===> Epoch[97](90/122): Loss: 0.0002\n","===> Epoch[97](91/122): Loss: 0.0002\n","===> Epoch[97](92/122): Loss: 0.0002\n","===> Epoch[97](93/122): Loss: 0.0002\n","===> Epoch[97](94/122): Loss: 0.0002\n","===> Epoch[97](95/122): Loss: 0.0002\n","===> Epoch[97](96/122): Loss: 0.0002\n","===> Epoch[97](97/122): Loss: 0.0002\n","===> Epoch[97](98/122): Loss: 0.0002\n","===> Epoch[97](99/122): Loss: 0.0002\n","===> Epoch[97](100/122): Loss: 0.0002\n","===> Epoch[97](101/122): Loss: 0.0002\n","===> Epoch[97](102/122): Loss: 0.0003\n","===> Epoch[97](103/122): Loss: 0.0002\n","===> Epoch[97](104/122): Loss: 0.0002\n","===> Epoch[97](105/122): Loss: 0.0002\n","===> Epoch[97](106/122): Loss: 0.0002\n","===> Epoch[97](107/122): Loss: 0.0002\n","===> Epoch[97](108/122): Loss: 0.0002\n","===> Epoch[97](109/122): Loss: 0.0002\n","===> Epoch[97](110/122): Loss: 0.0002\n","===> Epoch[97](111/122): Loss: 0.0002\n","===> Epoch[97](112/122): Loss: 0.0002\n","===> Epoch[97](113/122): Loss: 0.0002\n","===> Epoch[97](114/122): Loss: 0.0002\n","===> Epoch[97](115/122): Loss: 0.0002\n","===> Epoch[97](116/122): Loss: 0.0002\n","===> Epoch[97](117/122): Loss: 0.0001\n","===> Epoch[97](118/122): Loss: 0.0002\n","===> Epoch[97](119/122): Loss: 0.0002\n","===> Epoch[97](120/122): Loss: 0.0002\n","===> Epoch[97](121/122): Loss: 0.0002\n","===> Epoch[97](122/122): Loss: 0.0003\n","psnr_indiv=  [37.01645268416324] global_psnr=  37.01645268416324\n","===> Epoch 97 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.87381959042893}\n","train_per_mod {'flair': 37.01645268416324}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.3624 dB\n","===> Epoch[98](1/122): Loss: 0.0002\n","===> Epoch[98](2/122): Loss: 0.0002\n","===> Epoch[98](3/122): Loss: 0.0002\n","===> Epoch[98](4/122): Loss: 0.0002\n","===> Epoch[98](5/122): Loss: 0.0002\n","===> Epoch[98](6/122): Loss: 0.0002\n","===> Epoch[98](7/122): Loss: 0.0002\n","===> Epoch[98](8/122): Loss: 0.0002\n","===> Epoch[98](9/122): Loss: 0.0001\n","===> Epoch[98](10/122): Loss: 0.0002\n","===> Epoch[98](11/122): Loss: 0.0002\n","===> Epoch[98](12/122): Loss: 0.0002\n","===> Epoch[98](13/122): Loss: 0.0002\n","===> Epoch[98](14/122): Loss: 0.0002\n","===> Epoch[98](15/122): Loss: 0.0002\n","===> Epoch[98](16/122): Loss: 0.0002\n","===> Epoch[98](17/122): Loss: 0.0002\n","===> Epoch[98](18/122): Loss: 0.0002\n","===> Epoch[98](19/122): Loss: 0.0002\n","===> Epoch[98](20/122): Loss: 0.0002\n","===> Epoch[98](21/122): Loss: 0.0002\n","===> Epoch[98](22/122): Loss: 0.0002\n","===> Epoch[98](23/122): Loss: 0.0001\n","===> Epoch[98](24/122): Loss: 0.0002\n","===> Epoch[98](25/122): Loss: 0.0002\n","===> Epoch[98](26/122): Loss: 0.0002\n","===> Epoch[98](27/122): Loss: 0.0002\n","===> Epoch[98](28/122): Loss: 0.0002\n","===> Epoch[98](29/122): Loss: 0.0002\n","===> Epoch[98](30/122): Loss: 0.0002\n","===> Epoch[98](31/122): Loss: 0.0002\n","===> Epoch[98](32/122): Loss: 0.0002\n","===> Epoch[98](33/122): Loss: 0.0002\n","===> Epoch[98](34/122): Loss: 0.0002\n","===> Epoch[98](35/122): Loss: 0.0002\n","===> Epoch[98](36/122): Loss: 0.0002\n","===> Epoch[98](37/122): Loss: 0.0002\n","===> Epoch[98](38/122): Loss: 0.0002\n","===> Epoch[98](39/122): Loss: 0.0002\n","===> Epoch[98](40/122): Loss: 0.0002\n","===> Epoch[98](41/122): Loss: 0.0002\n","===> Epoch[98](42/122): Loss: 0.0002\n","===> Epoch[98](43/122): Loss: 0.0003\n","===> Epoch[98](44/122): Loss: 0.0002\n","===> Epoch[98](45/122): Loss: 0.0002\n","===> Epoch[98](46/122): Loss: 0.0002\n","===> Epoch[98](47/122): Loss: 0.0002\n","===> Epoch[98](48/122): Loss: 0.0002\n","===> Epoch[98](49/122): Loss: 0.0002\n","===> Epoch[98](50/122): Loss: 0.0002\n","===> Epoch[98](51/122): Loss: 0.0002\n","===> Epoch[98](52/122): Loss: 0.0002\n","===> Epoch[98](53/122): Loss: 0.0002\n","===> Epoch[98](54/122): Loss: 0.0002\n","===> Epoch[98](55/122): Loss: 0.0002\n","===> Epoch[98](56/122): Loss: 0.0002\n","===> Epoch[98](57/122): Loss: 0.0002\n","===> Epoch[98](58/122): Loss: 0.0002\n","===> Epoch[98](59/122): Loss: 0.0001\n","===> Epoch[98](60/122): Loss: 0.0002\n","===> Epoch[98](61/122): Loss: 0.0002\n","===> Epoch[98](62/122): Loss: 0.0002\n","===> Epoch[98](63/122): Loss: 0.0002\n","===> Epoch[98](64/122): Loss: 0.0002\n","===> Epoch[98](65/122): Loss: 0.0002\n","===> Epoch[98](66/122): Loss: 0.0002\n","===> Epoch[98](67/122): Loss: 0.0002\n","===> Epoch[98](68/122): Loss: 0.0002\n","===> Epoch[98](69/122): Loss: 0.0002\n","===> Epoch[98](70/122): Loss: 0.0002\n","===> Epoch[98](71/122): Loss: 0.0002\n","===> Epoch[98](72/122): Loss: 0.0002\n","===> Epoch[98](73/122): Loss: 0.0002\n","===> Epoch[98](74/122): Loss: 0.0002\n","===> Epoch[98](75/122): Loss: 0.0002\n","===> Epoch[98](76/122): Loss: 0.0002\n","===> Epoch[98](77/122): Loss: 0.0002\n","===> Epoch[98](78/122): Loss: 0.0002\n","===> Epoch[98](79/122): Loss: 0.0002\n","===> Epoch[98](80/122): Loss: 0.0002\n","===> Epoch[98](81/122): Loss: 0.0002\n","===> Epoch[98](82/122): Loss: 0.0002\n","===> Epoch[98](83/122): Loss: 0.0002\n","===> Epoch[98](84/122): Loss: 0.0001\n","===> Epoch[98](85/122): Loss: 0.0002\n","===> Epoch[98](86/122): Loss: 0.0002\n","===> Epoch[98](87/122): Loss: 0.0002\n","===> Epoch[98](88/122): Loss: 0.0002\n","===> Epoch[98](89/122): Loss: 0.0002\n","===> Epoch[98](90/122): Loss: 0.0002\n","===> Epoch[98](91/122): Loss: 0.0002\n","===> Epoch[98](92/122): Loss: 0.0002\n","===> Epoch[98](93/122): Loss: 0.0002\n","===> Epoch[98](94/122): Loss: 0.0002\n","===> Epoch[98](95/122): Loss: 0.0002\n","===> Epoch[98](96/122): Loss: 0.0002\n","===> Epoch[98](97/122): Loss: 0.0002\n","===> Epoch[98](98/122): Loss: 0.0002\n","===> Epoch[98](99/122): Loss: 0.0002\n","===> Epoch[98](100/122): Loss: 0.0002\n","===> Epoch[98](101/122): Loss: 0.0002\n","===> Epoch[98](102/122): Loss: 0.0002\n","===> Epoch[98](103/122): Loss: 0.0002\n","===> Epoch[98](104/122): Loss: 0.0002\n","===> Epoch[98](105/122): Loss: 0.0002\n","===> Epoch[98](106/122): Loss: 0.0002\n","===> Epoch[98](107/122): Loss: 0.0002\n","===> Epoch[98](108/122): Loss: 0.0002\n","===> Epoch[98](109/122): Loss: 0.0002\n","===> Epoch[98](110/122): Loss: 0.0002\n","===> Epoch[98](111/122): Loss: 0.0002\n","===> Epoch[98](112/122): Loss: 0.0002\n","===> Epoch[98](113/122): Loss: 0.0002\n","===> Epoch[98](114/122): Loss: 0.0002\n","===> Epoch[98](115/122): Loss: 0.0002\n","===> Epoch[98](116/122): Loss: 0.0002\n","===> Epoch[98](117/122): Loss: 0.0002\n","===> Epoch[98](118/122): Loss: 0.0002\n","===> Epoch[98](119/122): Loss: 0.0001\n","===> Epoch[98](120/122): Loss: 0.0002\n","===> Epoch[98](121/122): Loss: 0.0002\n","===> Epoch[98](122/122): Loss: 0.0002\n","psnr_indiv=  [37.04255484896859] global_psnr=  37.04255484896859\n","===> Epoch 98 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.897925237690956}\n","train_per_mod {'flair': 37.04255484896859}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.3771 dB\n","===> Epoch[99](1/122): Loss: 0.0002\n","===> Epoch[99](2/122): Loss: 0.0002\n","===> Epoch[99](3/122): Loss: 0.0002\n","===> Epoch[99](4/122): Loss: 0.0002\n","===> Epoch[99](5/122): Loss: 0.0002\n","===> Epoch[99](6/122): Loss: 0.0002\n","===> Epoch[99](7/122): Loss: 0.0002\n","===> Epoch[99](8/122): Loss: 0.0003\n","===> Epoch[99](9/122): Loss: 0.0002\n","===> Epoch[99](10/122): Loss: 0.0002\n","===> Epoch[99](11/122): Loss: 0.0002\n","===> Epoch[99](12/122): Loss: 0.0002\n","===> Epoch[99](13/122): Loss: 0.0002\n","===> Epoch[99](14/122): Loss: 0.0002\n","===> Epoch[99](15/122): Loss: 0.0002\n","===> Epoch[99](16/122): Loss: 0.0002\n","===> Epoch[99](17/122): Loss: 0.0002\n","===> Epoch[99](18/122): Loss: 0.0002\n","===> Epoch[99](19/122): Loss: 0.0002\n","===> Epoch[99](20/122): Loss: 0.0002\n","===> Epoch[99](21/122): Loss: 0.0002\n","===> Epoch[99](22/122): Loss: 0.0001\n","===> Epoch[99](23/122): Loss: 0.0002\n","===> Epoch[99](24/122): Loss: 0.0002\n","===> Epoch[99](25/122): Loss: 0.0002\n","===> Epoch[99](26/122): Loss: 0.0002\n","===> Epoch[99](27/122): Loss: 0.0002\n","===> Epoch[99](28/122): Loss: 0.0002\n","===> Epoch[99](29/122): Loss: 0.0002\n","===> Epoch[99](30/122): Loss: 0.0002\n","===> Epoch[99](31/122): Loss: 0.0002\n","===> Epoch[99](32/122): Loss: 0.0002\n","===> Epoch[99](33/122): Loss: 0.0002\n","===> Epoch[99](34/122): Loss: 0.0002\n","===> Epoch[99](35/122): Loss: 0.0002\n","===> Epoch[99](36/122): Loss: 0.0002\n","===> Epoch[99](37/122): Loss: 0.0002\n","===> Epoch[99](38/122): Loss: 0.0002\n","===> Epoch[99](39/122): Loss: 0.0002\n","===> Epoch[99](40/122): Loss: 0.0002\n","===> Epoch[99](41/122): Loss: 0.0002\n","===> Epoch[99](42/122): Loss: 0.0002\n","===> Epoch[99](43/122): Loss: 0.0002\n","===> Epoch[99](44/122): Loss: 0.0002\n","===> Epoch[99](45/122): Loss: 0.0002\n","===> Epoch[99](46/122): Loss: 0.0002\n","===> Epoch[99](47/122): Loss: 0.0002\n","===> Epoch[99](48/122): Loss: 0.0002\n","===> Epoch[99](49/122): Loss: 0.0002\n","===> Epoch[99](50/122): Loss: 0.0002\n","===> Epoch[99](51/122): Loss: 0.0002\n","===> Epoch[99](52/122): Loss: 0.0002\n","===> Epoch[99](53/122): Loss: 0.0002\n","===> Epoch[99](54/122): Loss: 0.0002\n","===> Epoch[99](55/122): Loss: 0.0002\n","===> Epoch[99](56/122): Loss: 0.0002\n","===> Epoch[99](57/122): Loss: 0.0002\n","===> Epoch[99](58/122): Loss: 0.0002\n","===> Epoch[99](59/122): Loss: 0.0002\n","===> Epoch[99](60/122): Loss: 0.0002\n","===> Epoch[99](61/122): Loss: 0.0002\n","===> Epoch[99](62/122): Loss: 0.0002\n","===> Epoch[99](63/122): Loss: 0.0002\n","===> Epoch[99](64/122): Loss: 0.0002\n","===> Epoch[99](65/122): Loss: 0.0003\n","===> Epoch[99](66/122): Loss: 0.0002\n","===> Epoch[99](67/122): Loss: 0.0002\n","===> Epoch[99](68/122): Loss: 0.0001\n","===> Epoch[99](69/122): Loss: 0.0002\n","===> Epoch[99](70/122): Loss: 0.0002\n","===> Epoch[99](71/122): Loss: 0.0002\n","===> Epoch[99](72/122): Loss: 0.0002\n","===> Epoch[99](73/122): Loss: 0.0002\n","===> Epoch[99](74/122): Loss: 0.0002\n","===> Epoch[99](75/122): Loss: 0.0002\n","===> Epoch[99](76/122): Loss: 0.0002\n","===> Epoch[99](77/122): Loss: 0.0002\n","===> Epoch[99](78/122): Loss: 0.0002\n","===> Epoch[99](79/122): Loss: 0.0001\n","===> Epoch[99](80/122): Loss: 0.0002\n","===> Epoch[99](81/122): Loss: 0.0002\n","===> Epoch[99](82/122): Loss: 0.0002\n","===> Epoch[99](83/122): Loss: 0.0002\n","===> Epoch[99](84/122): Loss: 0.0002\n","===> Epoch[99](85/122): Loss: 0.0002\n","===> Epoch[99](86/122): Loss: 0.0002\n","===> Epoch[99](87/122): Loss: 0.0002\n","===> Epoch[99](88/122): Loss: 0.0002\n","===> Epoch[99](89/122): Loss: 0.0002\n","===> Epoch[99](90/122): Loss: 0.0002\n","===> Epoch[99](91/122): Loss: 0.0002\n","===> Epoch[99](92/122): Loss: 0.0002\n","===> Epoch[99](93/122): Loss: 0.0002\n","===> Epoch[99](94/122): Loss: 0.0002\n","===> Epoch[99](95/122): Loss: 0.0002\n","===> Epoch[99](96/122): Loss: 0.0002\n","===> Epoch[99](97/122): Loss: 0.0003\n","===> Epoch[99](98/122): Loss: 0.0002\n","===> Epoch[99](99/122): Loss: 0.0002\n","===> Epoch[99](100/122): Loss: 0.0002\n","===> Epoch[99](101/122): Loss: 0.0002\n","===> Epoch[99](102/122): Loss: 0.0002\n","===> Epoch[99](103/122): Loss: 0.0002\n","===> Epoch[99](104/122): Loss: 0.0002\n","===> Epoch[99](105/122): Loss: 0.0002\n","===> Epoch[99](106/122): Loss: 0.0002\n","===> Epoch[99](107/122): Loss: 0.0002\n","===> Epoch[99](108/122): Loss: 0.0002\n","===> Epoch[99](109/122): Loss: 0.0002\n","===> Epoch[99](110/122): Loss: 0.0002\n","===> Epoch[99](111/122): Loss: 0.0003\n","===> Epoch[99](112/122): Loss: 0.0002\n","===> Epoch[99](113/122): Loss: 0.0002\n","===> Epoch[99](114/122): Loss: 0.0002\n","===> Epoch[99](115/122): Loss: 0.0002\n","===> Epoch[99](116/122): Loss: 0.0002\n","===> Epoch[99](117/122): Loss: 0.0002\n","===> Epoch[99](118/122): Loss: 0.0002\n","===> Epoch[99](119/122): Loss: 0.0002\n","===> Epoch[99](120/122): Loss: 0.0002\n","===> Epoch[99](121/122): Loss: 0.0002\n","===> Epoch[99](122/122): Loss: 0.0002\n","psnr_indiv=  [37.06170202658211] global_psnr=  37.06170202658211\n","===> Epoch 99 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.91585382478898}\n","train_per_mod {'flair': 37.06170202658211}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Avg. PSNR: 41.3864 dB\n","===> Epoch[100](1/122): Loss: 0.0002\n","===> Epoch[100](2/122): Loss: 0.0003\n","===> Epoch[100](3/122): Loss: 0.0002\n","===> Epoch[100](4/122): Loss: 0.0002\n","===> Epoch[100](5/122): Loss: 0.0002\n","===> Epoch[100](6/122): Loss: 0.0002\n","===> Epoch[100](7/122): Loss: 0.0002\n","===> Epoch[100](8/122): Loss: 0.0002\n","===> Epoch[100](9/122): Loss: 0.0002\n","===> Epoch[100](10/122): Loss: 0.0002\n","===> Epoch[100](11/122): Loss: 0.0002\n","===> Epoch[100](12/122): Loss: 0.0002\n","===> Epoch[100](13/122): Loss: 0.0002\n","===> Epoch[100](14/122): Loss: 0.0002\n","===> Epoch[100](15/122): Loss: 0.0002\n","===> Epoch[100](16/122): Loss: 0.0002\n","===> Epoch[100](17/122): Loss: 0.0002\n","===> Epoch[100](18/122): Loss: 0.0002\n","===> Epoch[100](19/122): Loss: 0.0002\n","===> Epoch[100](20/122): Loss: 0.0002\n","===> Epoch[100](21/122): Loss: 0.0002\n","===> Epoch[100](22/122): Loss: 0.0001\n","===> Epoch[100](23/122): Loss: 0.0002\n","===> Epoch[100](24/122): Loss: 0.0002\n","===> Epoch[100](25/122): Loss: 0.0002\n","===> Epoch[100](26/122): Loss: 0.0001\n","===> Epoch[100](27/122): Loss: 0.0003\n","===> Epoch[100](28/122): Loss: 0.0002\n","===> Epoch[100](29/122): Loss: 0.0002\n","===> Epoch[100](30/122): Loss: 0.0002\n","===> Epoch[100](31/122): Loss: 0.0002\n","===> Epoch[100](32/122): Loss: 0.0002\n","===> Epoch[100](33/122): Loss: 0.0002\n","===> Epoch[100](34/122): Loss: 0.0002\n","===> Epoch[100](35/122): Loss: 0.0002\n","===> Epoch[100](36/122): Loss: 0.0002\n","===> Epoch[100](37/122): Loss: 0.0003\n","===> Epoch[100](38/122): Loss: 0.0002\n","===> Epoch[100](39/122): Loss: 0.0002\n","===> Epoch[100](40/122): Loss: 0.0002\n","===> Epoch[100](41/122): Loss: 0.0002\n","===> Epoch[100](42/122): Loss: 0.0002\n","===> Epoch[100](43/122): Loss: 0.0002\n","===> Epoch[100](44/122): Loss: 0.0002\n","===> Epoch[100](45/122): Loss: 0.0002\n","===> Epoch[100](46/122): Loss: 0.0002\n","===> Epoch[100](47/122): Loss: 0.0002\n","===> Epoch[100](48/122): Loss: 0.0002\n","===> Epoch[100](49/122): Loss: 0.0002\n","===> Epoch[100](50/122): Loss: 0.0002\n","===> Epoch[100](51/122): Loss: 0.0002\n","===> Epoch[100](52/122): Loss: 0.0002\n","===> Epoch[100](53/122): Loss: 0.0002\n","===> Epoch[100](54/122): Loss: 0.0002\n","===> Epoch[100](55/122): Loss: 0.0002\n","===> Epoch[100](56/122): Loss: 0.0002\n","===> Epoch[100](57/122): Loss: 0.0002\n","===> Epoch[100](58/122): Loss: 0.0002\n","===> Epoch[100](59/122): Loss: 0.0002\n","===> Epoch[100](60/122): Loss: 0.0002\n","===> Epoch[100](61/122): Loss: 0.0002\n","===> Epoch[100](62/122): Loss: 0.0002\n","===> Epoch[100](63/122): Loss: 0.0002\n","===> Epoch[100](64/122): Loss: 0.0002\n","===> Epoch[100](65/122): Loss: 0.0002\n","===> Epoch[100](66/122): Loss: 0.0002\n","===> Epoch[100](67/122): Loss: 0.0002\n","===> Epoch[100](68/122): Loss: 0.0002\n","===> Epoch[100](69/122): Loss: 0.0002\n","===> Epoch[100](70/122): Loss: 0.0002\n","===> Epoch[100](71/122): Loss: 0.0002\n","===> Epoch[100](72/122): Loss: 0.0002\n","===> Epoch[100](73/122): Loss: 0.0002\n","===> Epoch[100](74/122): Loss: 0.0002\n","===> Epoch[100](75/122): Loss: 0.0002\n","===> Epoch[100](76/122): Loss: 0.0002\n","===> Epoch[100](77/122): Loss: 0.0002\n","===> Epoch[100](78/122): Loss: 0.0002\n","===> Epoch[100](79/122): Loss: 0.0002\n","===> Epoch[100](80/122): Loss: 0.0001\n","===> Epoch[100](81/122): Loss: 0.0002\n","===> Epoch[100](82/122): Loss: 0.0002\n","===> Epoch[100](83/122): Loss: 0.0002\n","===> Epoch[100](84/122): Loss: 0.0002\n","===> Epoch[100](85/122): Loss: 0.0002\n","===> Epoch[100](86/122): Loss: 0.0002\n","===> Epoch[100](87/122): Loss: 0.0002\n","===> Epoch[100](88/122): Loss: 0.0002\n","===> Epoch[100](89/122): Loss: 0.0002\n","===> Epoch[100](90/122): Loss: 0.0002\n","===> Epoch[100](91/122): Loss: 0.0002\n","===> Epoch[100](92/122): Loss: 0.0002\n","===> Epoch[100](93/122): Loss: 0.0002\n","===> Epoch[100](94/122): Loss: 0.0002\n","===> Epoch[100](95/122): Loss: 0.0002\n","===> Epoch[100](96/122): Loss: 0.0002\n","===> Epoch[100](97/122): Loss: 0.0002\n","===> Epoch[100](98/122): Loss: 0.0002\n","===> Epoch[100](99/122): Loss: 0.0002\n","===> Epoch[100](100/122): Loss: 0.0002\n","===> Epoch[100](101/122): Loss: 0.0002\n","===> Epoch[100](102/122): Loss: 0.0002\n","===> Epoch[100](103/122): Loss: 0.0002\n","===> Epoch[100](104/122): Loss: 0.0002\n","===> Epoch[100](105/122): Loss: 0.0002\n","===> Epoch[100](106/122): Loss: 0.0002\n","===> Epoch[100](107/122): Loss: 0.0002\n","===> Epoch[100](108/122): Loss: 0.0002\n","===> Epoch[100](109/122): Loss: 0.0002\n","===> Epoch[100](110/122): Loss: 0.0002\n","===> Epoch[100](111/122): Loss: 0.0002\n","===> Epoch[100](112/122): Loss: 0.0002\n","===> Epoch[100](113/122): Loss: 0.0002\n","===> Epoch[100](114/122): Loss: 0.0002\n","===> Epoch[100](115/122): Loss: 0.0002\n","===> Epoch[100](116/122): Loss: 0.0002\n","===> Epoch[100](117/122): Loss: 0.0002\n","===> Epoch[100](118/122): Loss: 0.0002\n","===> Epoch[100](119/122): Loss: 0.0002\n","===> Epoch[100](120/122): Loss: 0.0002\n","===> Epoch[100](121/122): Loss: 0.0002\n","===> Epoch[100](122/122): Loss: 0.0001\n","psnr_indiv=  [37.08171005724788] global_psnr=  37.08171005724788\n","===> Epoch 100 Complete: Avg. Loss: 0.0002\n","test_per_mod {'flair': 40.92442801771395}\n","train_per_mod {'flair': 37.08171005724788}\n","Checkpoint saved to /content/Gdrive/My Drive/Colab Notebooks/PRE/Multi-Modalities/Trainings/flair->flair 32blocs/Pre_trained_model/flair->flair 32blocs\n","===> Display results\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gCNgugbty0m2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":531},"executionInfo":{"status":"ok","timestamp":1597596585266,"user_tz":-120,"elapsed":12850664,"user":{"displayName":"Alexandre Bodinier","photoUrl":"","userId":"11185344835719138768"}},"outputId":"c4855969-ece1-47d0-e988-72b85f053dab"},"source":["def plot(x, label, linestyle='solid', color='g'):\n","    t = np.arange(0, len(x), 1)\n","    plt.plot(t, x, label=label, linestyle=linestyle, color=color)\n","\n","plt.figure(figsize=(10, 8))\n","plot(stats['train_PSNR'], \"train PSNR\", linestyle=\"dashed\", color=\"g\")\n","plot(stats['test_PSNR'], \"test PSNR\", linestyle=\"solid\", color=\"g\")\n","plt.legend()\n","plt.grid()\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"PSNR (dB)\")\n","plt.title(\"Learning Curve\" + modelName)\n","plt.show()\n","plt.savefig(dir+'/'+modelName + \"lc.jpg\")"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x576 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}}]}]}